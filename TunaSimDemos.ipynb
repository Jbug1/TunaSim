{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import testUtils\n",
    "\n",
    "from TunaSims import tuna_sim\n",
    "from funcOb import func_ob\n",
    "import math_distance\n",
    "import tools\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that Sims are in funtion space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/train/10_ppm/chunk_1.pkl')\n",
    "demo_matches_test = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/train/10_ppm/chunk_2.pkl')\n",
    "demo_query = demo_matches.iloc[0]['query']\n",
    "demo_target = demo_matches.iloc[0]['target']\n",
    "demo_query_prec = demo_matches.iloc[0]['precquery']\n",
    "demo_target_prec = demo_matches.iloc[0]['prectarget']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_tuna = tuna_sim(demo_query,\n",
    "              demo_target,\n",
    "              demo_query_prec,\n",
    "              demo_target_prec,\n",
    "              dif_a = 1,\n",
    "              dif_b = 1,\n",
    "              unnormed = 1)\n",
    "\n",
    "dot_tuna = tuna_sim(demo_query,\n",
    "              demo_target,\n",
    "              demo_query_prec,\n",
    "              demo_target_prec,\n",
    "              mult_a = 1,\n",
    "              mult_b = 2,\n",
    "              collapsed = 1,\n",
    "              mult_norm_a= 1,\n",
    "              mult_norm_b= 2,\n",
    "              sim_flip=True)\n",
    "\n",
    "harmonic_tuna = tuna_sim(demo_query,\n",
    "              demo_target,\n",
    "              demo_query_prec,\n",
    "              demo_target_prec,\n",
    "              mult_a = 1,\n",
    "              mult_b = 1,\n",
    "              expanded = 2,\n",
    "              add_norm_a= 1,\n",
    "              add_norm_b= 1,\n",
    "              sim_flip=True)\n",
    "\n",
    "demo_query[:,1] /= sum(demo_query[:,1])\n",
    "demo_target[:,1] /= sum(demo_target[:,1])\n",
    "combined_old = tools.match_peaks_in_spectra(demo_query, demo_target, ms2_da=0.05)\n",
    "manhattan = 1 - tools.sigmoid(math_distance.manhattan_distance(combined_old[:,1], combined_old[:,2]))\n",
    "dot_product = tools.sigmoid(1 - math_distance.dot_product_nosqrt_distance(combined_old[:,1], combined_old[:,2]))\n",
    "harmonic_mean = tools.sigmoid(1 - math_distance.harmonic_mean_distance(combined_old[:,1], combined_old[:,2]))\n",
    "\n",
    "print(f'manhattan: {abs(manhattan - manhattan_tuna)}')\n",
    "print(f'dot_product: {abs(dot_product - dot_tuna)}')\n",
    "print(f'harmonic_mean: {abs(harmonic_mean - harmonic_tuna)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we recover similarity function from scores and input vectors alone, which training strategies are best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First We will do no funny biz with cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(funcOb)\n",
    "func_skeletons = [\n",
    "    funcOb.func_ob(\n",
    "    name = \"5k_iter\",\n",
    "    sim_func = partial(tuna_sim),\n",
    "    init_vals= 0.1,\n",
    "    params = None,\n",
    "    tol = 0,\n",
    "    lambdas= 1,\n",
    "    max_iter = 5000,\n",
    "    epsilon = 1e-5),\n",
    "    funcOb.func_ob(\n",
    "    name = \"5k_iter_1e3\",\n",
    "    sim_func = partial(tuna_sim),\n",
    "    init_vals= 0.1,\n",
    "    params = None,\n",
    "    tol = 0,\n",
    "    lambdas= 1,\n",
    "    max_iter = 5000,\n",
    "    epsilon = 1e-3),\n",
    "    funcOb.func_ob(\n",
    "    name = \"5k_iter\",\n",
    "    sim_func = partial(tuna_sim),\n",
    "    init_vals= 0.1,\n",
    "    params = None,\n",
    "    tol = 0,\n",
    "    lambdas= 1,\n",
    "    max_iter = 5000,\n",
    "    epsilon = 1e-5,\n",
    "    zero_grad_epsilon_boost = 1.2)\n",
    "]\n",
    "  \n",
    "\n",
    "params = {\n",
    "    \"None\":[],\n",
    "    \"dif_only\": ['unnormed','dif_a','dif_b'],\n",
    "    \"dif_and_mult\": ['unnormed','dif_a','dif_b','mult_a','mult_b'],\n",
    "    \"collapsed\": ['collapsed','dif_a','dif_b','mult_a','mult_b', 'mult_norm_a','mult_norm_b'],\n",
    "    \"expanded\": ['expanded','dif_a','dif_b','mult_a','mult_b', 'add_norm_a', 'add_norm_b'],\n",
    "    # \"collapsed_and_unnorm\": ['unnormed', 'collapsed','dif_a','dif_b','mult_a','mult_b','mult_norm_a','mult_norm_b', 'add_norm_a', 'add_norm_b'],\n",
    "    # \"expanded_and_unnorm\": ['unnormed', 'expanded','dif_a','dif_b','mult_a','mult_b', 'add_norm_a', 'add_norm_b']  \n",
    "}\n",
    "\n",
    "params_dict = dict()\n",
    "for key, value in params.items():\n",
    "    params_dict[f'{key}__clean'] = np.array(value+['query_max_mz_fix',\n",
    "                                               'target_max_mz_fix', \n",
    "                                               'query_fixed_noise', \n",
    "                                               'target_fixed_noise',\n",
    "                                                'query_var_noise',\n",
    "                                                'target_var_noise',\n",
    "                                                'query_fixed_power',\n",
    "                                                'target_fixed_power'])\n",
    "    \n",
    "    params_dict[f'{key}__reweight'] = np.array(value+['query_fixed_power',\n",
    "                                            'query_mz_power',\n",
    "                                            'query_ent_power',\n",
    "                                            'target_fixed_power',\n",
    "                                            'target_mz_power',\n",
    "                                            'target_ent_power',\n",
    "                                            'query_reweight_offset',\n",
    "                                            'target_reweight_offset'])\n",
    "    \n",
    "    params_dict[f'{key}__clean__reweight'] = np.array(value+['query_max_mz_fix',\n",
    "                                               'target_max_mz_fix', \n",
    "                                               'query_fixed_noise', \n",
    "                                               'target_fixed_noise',\n",
    "                                                'query_var_noise',\n",
    "                                                'target_var_noise',\n",
    "                                                'query_fixed_power',\n",
    "                                                'target_fixed_power',\n",
    "                                                'query_fixed_power',\n",
    "                                                'query_mz_power',\n",
    "                                                'query_ent_power',\n",
    "                                                'target_fixed_power',\n",
    "                                                'target_mz_power',\n",
    "                                                'target_ent_power',\n",
    "                                                'query_reweight_offset',\n",
    "                                                'target_reweight_offset'])\n",
    "    \n",
    "# params.update(new_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to turn this into an easy way to generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "manhattan_tuna = {'dif_a' : 1,\n",
    "              'dif_b' : 1,\n",
    "              'unnormed' : 1}\n",
    "\n",
    "dot_tuna = {'mult_a' : 1,\n",
    "              'mult_b' : 2,\n",
    "              'collapsed' : 1,\n",
    "              'mult_norm_a': 1,\n",
    "              'mult_norm_b': 2,\n",
    "              'sim_flip':True}\n",
    "\n",
    "harmonic_tuna = {'mult_a' :1,\n",
    "              'mult_b' : 1,\n",
    "              'expanded' : 2,\n",
    "              'add_norm_a' : 1,\n",
    "              'add_norm_b': 1,\n",
    "              'sim_flip' : True}\n",
    "\n",
    "clean1 = {'query_max_mz_fix' : 1.6,\n",
    "            'query_fixed_noise' : 4,\n",
    "            'query_da_thresh' : 0.05,\n",
    "            'target_max_mz_fix' : 1.6,\n",
    "            'target_fixed_noise' : 4,\n",
    "            'target_da_thresh' : 0.05}\n",
    "\n",
    "clean2 = {'query_max_mz_var' : 0.01,\n",
    "            'query_var_noise' : 0.01,\n",
    "            'query_da_thresh' : 0.05,\n",
    "              'target_max_mz_var': 0.01,\n",
    "              'target_var_noise' :0.01,\n",
    "              'target_da_thresh' : 0.05}\n",
    "\n",
    "clean3 = {'query_max_mz_fix' : 0.8,\n",
    "        'query_max_mz_var' : 0.005,\n",
    "        'query_fixed_noise' : 1,\n",
    "        'query_var_noise': 0.01,\n",
    "        'query_da_thresh' :0.05,\n",
    "        'target_max_mz_fix' : 0.8,\n",
    "        'target_max_mz_var' : 0.005,\n",
    "        'target_fixed_noise' : 1,\n",
    "        'target_var_noise' : 0.01,\n",
    "        'target_da_thresh' : 0.05}\n",
    "\n",
    "reweight1 = {'query_fixed_power' : 0.75,\n",
    "             'target_fixed_power' : 0.75}\n",
    "\n",
    "reweight2 = {'query_ent_power' : 0.5,\n",
    "             'target_ent_power' : 0.5}\n",
    "\n",
    "reweight3 = {'query_ent_power' : 0.1,\n",
    "              'target_mz_power' :  0.1,\n",
    "              'query_reweight_offset' : -2,\n",
    "              'target_reweight_offset' : -2}\n",
    "\n",
    "reweight4 = {'query_fixed_power' : 0.2,\n",
    "              'query_mz_power' :0.1,\n",
    "              'query_ent_power' : 0.25,\n",
    "              'target_fixed_power' : 0.2,\n",
    "              'target_mz_power' : 0.1,\n",
    "              'target_ent_power' : 0.25,\n",
    "              'query_reweight_offset' : -2,\n",
    "              'target_reweight_offset' : -2}\n",
    "\n",
    "sims = {\"manhattan\": manhattan_tuna,\n",
    "        \"dot_product\": dot_tuna,\n",
    "        \"harmoinc\": harmonic_tuna}\n",
    "\n",
    "cleans = {\"clean1\": clean1,\n",
    "          \"clean2\": clean2,\n",
    "          \"clean3\": clean3,\n",
    "          \"None\": {}}\n",
    "\n",
    "reweights = {\"reweight1\":reweight1,\n",
    "             \"reweight2\": reweight2,\n",
    "             \"reweight3\": reweight3,\n",
    "             \"reweight4\": reweight4,\n",
    "             \"None\": {}}\n",
    "\n",
    "funcs_to_find = dict()\n",
    "\n",
    "for sim, simval in sims.items():\n",
    "    for clean, cleanval in cleans.items():\n",
    "        for reweight, reweight_val in reweights.items():\n",
    "            \n",
    "            vals = {}\n",
    "            vals.update(simval)\n",
    "            vals.update(cleanval)\n",
    "            vals.update(reweight_val)\n",
    "    \n",
    "            funcs_to_find[f'{sim}_{clean}_{reweight}'] = partial(tuna_sim, **vals)\n",
    "\n",
    "params_to_test = {'manhattan':['dif_only'],\n",
    "                  'dot': ['collapsed'],\n",
    "                  'harmonic': ['expanded']\n",
    "                }\n",
    "\n",
    "datasets = dict()\n",
    "\n",
    "demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/train/10_ppm/chunk_1.pkl')\n",
    "demo_matches_test = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/test/10_ppm/chunk_1.pkl')\n",
    "\n",
    "demo_matches = demo_matches.sample(frac=1)[:100000]\n",
    "demo_matches_test = demo_matches_test.sample(frac=1)[:100000]\n",
    "\n",
    "for funcname, func in funcs_to_find.items():\n",
    "\n",
    "    datasets[funcname] = list()\n",
    "    datasets[funcname].append(testUtils.create_scores_from_tuna(demo_matches,func))\n",
    "    datasets[funcname].append(testUtils.create_scores_from_tuna(demo_matches_test,func))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train funcs on different scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '/Users/jonahpoczobutt/projects/TunaRes/res_new.csv'\n",
    "reload(testUtils)\n",
    "for dataset_name, sets in datasets.items():\n",
    "\n",
    "    sim_name = dataset_name.split('_')[0]\n",
    "    params_set = params_to_test[sim_name]\n",
    "\n",
    "    demo_matches['match'] = sets[0]\n",
    "    demo_matches_test['match'] = sets[1]\n",
    "\n",
    "    test_subset = dict()\n",
    "    for name in params_set:\n",
    "\n",
    "        for key, value in params_dict.items():\n",
    "\n",
    "            if name in key:\n",
    "\n",
    "                if 'clean' in dataset_name:\n",
    "                    if 'clean' not in key:\n",
    "                        continue\n",
    "\n",
    "                    if 'reweight' in dataset_name:\n",
    "                        if 'reweight' not in key:\n",
    "                            continue\n",
    "\n",
    "                    else:\n",
    "                        if 'reweight' in key:\n",
    "                            continue\n",
    "\n",
    "                    test_subset[key] = value\n",
    "\n",
    "                elif 'reweight' in dataset_name:\n",
    "                        if 'reweight' not in key:\n",
    "                            continue\n",
    "\n",
    "                        test_subset[key] = value\n",
    "\n",
    "                elif 'reweight' in dataset_name or 'clean' in dataset_name:\n",
    "                    continue\n",
    "\n",
    "                test_subset[key] = value\n",
    "\n",
    "    func_skeletons_ = [copy.deepcopy(skel) for skel in func_skeletons]\n",
    "         \n",
    "    #get train and test errors for \n",
    "    res = testUtils.func_err_tester(func_skeletons_, \n",
    "                                    test_subset, \n",
    "                                    {dataset_name: [demo_matches,demo_matches_test]},\n",
    "                                    test_len = 10000)\n",
    "    \n",
    "    res['sim_name'] = sim_name\n",
    "    print(sim_name)\n",
    "\n",
    "    res.to_csv(output_file, mode ='a', header = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '/Users/jonahpoczobutt/projects/TunaRes/res_new_with_sim.csv'\n",
    "for dataset_name, sets in datasets.items():\n",
    "\n",
    "    sim_name = dataset_name.split('_')[0]\n",
    "    params_set = params_to_test[sim_name]\n",
    "\n",
    "    demo_matches['match'] = sets[0]\n",
    "    demo_matches_test['match'] = sets[1]\n",
    "\n",
    "    #add the similarity measure to func skeleton\n",
    "    funk_skeletons_ = list()\n",
    "    for skeleton in func_skeletons:\n",
    "\n",
    "        copied_skeleton = copy.deepcopy(skeleton)\n",
    "        copied_skeleton.sim_func = partial(tuna_sim,**sims[dataset_name.split('_')[0]])\n",
    "        copied_skeleton.name = f'{dataset_name.split('_')[0]}_{copied_skeleton.name}'\n",
    "        funk_skeletons_.append(copied_skeleton)\n",
    "\n",
    "    if 'clean' and 'reweight' in dataset_name:\n",
    "        test_subset = {'params'} = params_dict['None_clean_reweight']\n",
    "    elif 'clean' in dataset_name:\n",
    "        test_subset = {'params'} = params_dict['None_clean_None']\n",
    "    elif 'reweight' in dataset_name:\n",
    "        test_subset = {'params'} = params_dict['None_None_reweight']\n",
    "    else:\n",
    "        continue\n",
    "                \n",
    "    #get train and test errors for \n",
    "    res = testUtils.func_err_tester(func_skeletons, \n",
    "                                    test_subset, \n",
    "                                    {dataset_name: [demo_matches,demo_matches_test]},\n",
    "                                    test_len = 10000,\n",
    "                                    verbose=1000)\n",
    "    \n",
    "    res['sim_name'] = sim_name\n",
    "\n",
    "    res.to_csv(output_file, mode ='a', header = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
