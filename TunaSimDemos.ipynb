{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import testUtils\n",
    "\n",
    "from TunaSims import tuna_sim\n",
    "from funcOb import func_ob\n",
    "import math_distance\n",
    "import tools\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that Sims are in funtion space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/train/10_ppm/chunk_1.pkl')\n",
    "demo_matches_test = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/train/10_ppm/chunk_2.pkl')\n",
    "demo_query = demo_matches.iloc[0]['query']\n",
    "demo_target = demo_matches.iloc[0]['target']\n",
    "demo_query_prec = demo_matches.iloc[0]['precquery']\n",
    "demo_target_prec = demo_matches.iloc[0]['prectarget']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manhattan: 1.9321462207955165e-09\n",
      "dot_product: 0.0\n",
      "harmonic_mean: 0.0\n"
     ]
    }
   ],
   "source": [
    "manhattan_tuna = tuna_sim(demo_query,\n",
    "              demo_target,\n",
    "              demo_query_prec,\n",
    "              demo_target_prec,\n",
    "              dif_a = 1,\n",
    "              dif_b = 1,\n",
    "              unnormed = 1)\n",
    "\n",
    "dot_tuna = tuna_sim(demo_query,\n",
    "              demo_target,\n",
    "              demo_query_prec,\n",
    "              demo_target_prec,\n",
    "              mult_a = 1,\n",
    "              mult_b = 2,\n",
    "              collapsed = 1,\n",
    "              mult_norm_a= 1,\n",
    "              mult_norm_b= 2,\n",
    "              sim_flip=True)\n",
    "\n",
    "harmonic_tuna = tuna_sim(demo_query,\n",
    "              demo_target,\n",
    "              demo_query_prec,\n",
    "              demo_target_prec,\n",
    "              mult_a = 1,\n",
    "              mult_b = 1,\n",
    "              expanded = 2,\n",
    "              add_norm_a= 1,\n",
    "              add_norm_b= 1,\n",
    "              sim_flip=True)\n",
    "\n",
    "demo_query[:,1] /= sum(demo_query[:,1])\n",
    "demo_target[:,1] /= sum(demo_target[:,1])\n",
    "combined_old = tools.match_peaks_in_spectra(demo_query, demo_target, ms2_da=0.05)\n",
    "manhattan = 1 - tools.sigmoid(math_distance.manhattan_distance(combined_old[:,1], combined_old[:,2]))\n",
    "dot_product = tools.sigmoid(1 - math_distance.dot_product_nosqrt_distance(combined_old[:,1], combined_old[:,2]))\n",
    "harmonic_mean = tools.sigmoid(1 - math_distance.harmonic_mean_distance(combined_old[:,1], combined_old[:,2]))\n",
    "\n",
    "print(f'manhattan: {abs(manhattan - manhattan_tuna)}')\n",
    "print(f'dot_product: {abs(dot_product - dot_tuna)}')\n",
    "print(f'harmonic_mean: {abs(harmonic_mean - harmonic_tuna)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we recover similarity function from scores and input vectors alone, which training strategies are best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First We will do no funny biz with cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_skeletons = [\n",
    "    func_ob(\n",
    "    name = \"5k_iter\",\n",
    "    sim_func = partial(tuna_sim),\n",
    "    init_vals= 0.1,\n",
    "    params = None,\n",
    "    tol = 0,\n",
    "    lambdas= 1,\n",
    "    max_iter = 10000,\n",
    "    epsilon = 1e-5),\n",
    "    func_ob(\n",
    "    name = \"5k_iter_1e3\",\n",
    "    sim_func = partial(tuna_sim),\n",
    "    init_vals= 0.1,\n",
    "    params = None,\n",
    "    tol = 0,\n",
    "    lambdas= 1,\n",
    "    max_iter = 10000,\n",
    "    epsilon = 1e-3),\n",
    "    func_ob(\n",
    "    name = \"5k_iter\",\n",
    "    sim_func = partial(tuna_sim),\n",
    "    init_vals= 0.1,\n",
    "    params = None,\n",
    "    tol = 0,\n",
    "    lambdas= 1,\n",
    "    max_iter = 10000,\n",
    "    epsilon = 1e-5,\n",
    "    zero_grad_epsilon_boost = 1.2)\n",
    "]\n",
    "  \n",
    "\n",
    "params = {\n",
    "    \"dif_only\": ['unnormed','dif_a','dif_b'],\n",
    "    \"dif_and_mult\": ['unnormed','dif_a','dif_b','mult_a','mult_b'],\n",
    "    \"collapsed\": ['collapsed','dif_a','dif_b','mult_a','mult_b', 'mult_norm_a','mult_norm_b'],\n",
    "    \"expanded\": ['expanded','dif_a','dif_b','mult_a','mult_b', 'add_norm_a', 'add_norm_b'],\n",
    "    \"collapsed_and_unnorm\": ['unnormed', 'collapsed','dif_a','dif_b','mult_a','mult_b','mult_norm_a','mult_norm_b', 'add_norm_a', 'add_norm_b'],\n",
    "    \"expanded_and_unnorm\": ['unnormed', 'expanded','dif_a','dif_b','mult_a','mult_b', 'add_norm_a', 'add_norm_b']  \n",
    "}\n",
    "\n",
    "params_dict = dict()\n",
    "for key, value in params.items():\n",
    "    params_dict[f'{key}__cleaning'] = value+['query_max_mz_fix',\n",
    "                                               'target_max_mz_fix', \n",
    "                                               'query_fixed_noise', \n",
    "                                               'target_fixed_noise',\n",
    "                                                'query_da_thresh',\n",
    "                                                'target_da_thresh',\n",
    "                                                'query_fixed_power',\n",
    "                                                'target_fixed_power']\n",
    "    \n",
    "    params_dict[f'{key}__reweight'] = value+['query_fixed_power',\n",
    "                                            'query_mz_power',\n",
    "                                            'query_ent_power',\n",
    "                                            'target_fixed_power',\n",
    "                                            'target_mz_power',\n",
    "                                            'target_ent_power',\n",
    "                                            'query_reweight_offset',\n",
    "                                            'target_reweight_offset']\n",
    "    \n",
    "    params_dict[f'{key}__cleaning__reweight'] = value+['query_max_mz_fix',\n",
    "                                               'target_max_mz_fix', \n",
    "                                               'query_fixed_noise', \n",
    "                                               'target_fixed_noise',\n",
    "                                                'query_da_thresh',\n",
    "                                                'target_da_thresh',\n",
    "                                                'query_fixed_power',\n",
    "                                                'target_fixed_power',\n",
    "                                                'query_fixed_power',\n",
    "                                                'query_mz_power',\n",
    "                                                'query_ent_power',\n",
    "                                                'target_fixed_power',\n",
    "                                                'target_mz_power',\n",
    "                                                'target_ent_power',\n",
    "                                                'query_reweight_offset',\n",
    "                                                'target_reweight_offset']\n",
    "    \n",
    "# params.update(new_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to turn this into an easy way to generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonahpoczobutt/projects/TunaSim/TunaSims.py:114: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  collapsed_term = collapsed * (collapsed_difs + collapsed_mults) / (np.sum(mult_norm) + np.sum(add_norm))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "manhattan_tuna = {'dif_a' : 1,\n",
    "              'dif_b' : 1,\n",
    "              'unnormed' : 1}\n",
    "\n",
    "dot_tuna = {'mult_a' : 1,\n",
    "              'mult_b' : 2,\n",
    "              'collapsed' : 1,\n",
    "              'mult_norm_a': 1,\n",
    "              'mult_norm_b': 2,\n",
    "              'sim_flip':True}\n",
    "\n",
    "harmonic_tuna = {'mult_a' :1,\n",
    "              'mult_b' : 1,\n",
    "              'expanded' : 2,\n",
    "              'add_norm_a' : 1,\n",
    "              'add_norm_b': 1,\n",
    "              'sim_flip' : True}\n",
    "\n",
    "clean1 = {'query_max_mz_fix' : 1.6,\n",
    "            'query_fixed_noise' : 4,\n",
    "            'query_da_thresh' : 0.05,\n",
    "            'target_max_mz_fix' : 1.6,\n",
    "            'target_fixed_noise' : 4,\n",
    "            'target_da_thresh' : 0.05}\n",
    "\n",
    "clean2 = {'query_max_mz_var' : 0.01,\n",
    "            'query_var_noise' : 0.01,\n",
    "            'query_da_thresh' : 0.05,\n",
    "              'target_max_mz_var': 0.01,\n",
    "              'target_var_noise' :0.01,\n",
    "              'target_da_thresh' : 0.05}\n",
    "\n",
    "clean3 = {'query_max_mz_fix' : 0.8,\n",
    "        'query_max_mz_var' : 0.005,\n",
    "        'query_fixed_noise' : 1,\n",
    "        'query_var_noise': 0.01,\n",
    "        'query_da_thresh' :0.05,\n",
    "        'target_max_mz_fix' : 0.8,\n",
    "        'target_max_mz_var' : 0.005,\n",
    "        'target_fixed_noise' : 1,\n",
    "        'target_var_noise' : 0.01,\n",
    "        'target_da_thresh' : 0.05}\n",
    "\n",
    "reweight1 = {'query_fixed_power' : 0.75,\n",
    "             'target_fixed_power' : 0.75}\n",
    "\n",
    "reweight2 = {'query_ent_power' : 0.5,\n",
    "             'target_ent_power' : 0.5}\n",
    "\n",
    "reweight3 = {'query_ent_power' : 0.1,\n",
    "              'target_mz_power' :  0.1,\n",
    "              'query_reweight_offset' : -2,\n",
    "              'target_reweight_offset' : -2}\n",
    "\n",
    "reweight4 = {'query_fixed_power' : 0.2,\n",
    "              'query_mz_power' :0.1,\n",
    "              'query_ent_power' : 0.25,\n",
    "              'target_fixed_power' : 0.2,\n",
    "              'target_mz_power' : 0.1,\n",
    "              'target_ent_power' : 0.25,\n",
    "              'query_reweight_offset' : -2,\n",
    "              'target_reweight_offset' : -2}\n",
    "\n",
    "sims = {\"manhattan\": manhattan_tuna,\n",
    "        \"dot_product\": dot_tuna,\n",
    "        \"harmoinc\": harmonic_tuna}\n",
    "\n",
    "cleans = {\"clean1\": clean1,\n",
    "          \"clean2\": clean2,\n",
    "          \"clean3\": clean3,\n",
    "          \"None\": {}}\n",
    "\n",
    "reweights = {\"reweight1\":reweight1,\n",
    "             \"reweight2\": reweight2,\n",
    "             \"reweight3\": reweight3,\n",
    "             \"reweight4\": reweight4,\n",
    "             \"None\": {}}\n",
    "\n",
    "funcs_to_find = dict()\n",
    "\n",
    "for sim, simval in sims.items():\n",
    "    for clean, cleanval in cleans.items():\n",
    "        for reweight, reweight_val in reweights.items():\n",
    "            \n",
    "            vals = {}\n",
    "            vals.update(simval)\n",
    "            vals.update(cleanval)\n",
    "            vals.update(reweight_val)\n",
    "    \n",
    "            funcs_to_find[f'{sim}_{clean}_{reweight}'] = partial(tuna_sim, **vals)\n",
    "\n",
    "params_to_test = {'manhattan':['dif_only'],\n",
    "                  'dot': ['collapsed'],\n",
    "                  'harmonic': ['expanded']\n",
    "                }\n",
    "\n",
    "datasets = dict()\n",
    "\n",
    "demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/train/10_ppm/chunk_1.pkl')\n",
    "demo_matches_test = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/test/10_ppm/chunk_1.pkl')\n",
    "\n",
    "demo_matches = demo_matches.sample(frac=1)[:100000]\n",
    "demo_matches_test = demo_matches_test.sample(frac=1)[:100000]\n",
    "\n",
    "for funcname, func in funcs_to_find.items():\n",
    "\n",
    "    datasets[funcname] = list()\n",
    "    datasets[funcname].append(testUtils.create_scores_from_tuna(demo_matches,func))\n",
    "    datasets[funcname].append(testUtils.create_scores_from_tuna(demo_matches_test,func))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train funcs on different scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 2000 updates\n",
      "completed 4000 updates\n",
      "completed 6000 updates\n",
      "completed 8000 updates\n",
      "completed 10000 updates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonahpoczobutt/projects/TunaSim/TunaSims.py:83: RuntimeWarning: invalid value encountered in divide\n",
      "  query[:,1] /= np.sum(query[:,1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 2000 updates\n",
      "completed 4000 updates\n",
      "completed 6000 updates\n",
      "completed 8000 updates\n",
      "completed 10000 updates\n",
      "completed 2000 updates\n",
      "completed 4000 updates\n",
      "completed 6000 updates\n",
      "completed 8000 updates\n",
      "completed 10000 updates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonahpoczobutt/projects/TunaSim/TunaSims.py:102: RuntimeWarning: divide by zero encountered in power\n",
      "  add_norm = add_norm_a * (np.power(query, add_norm_b) + np.power(target, add_norm_b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 2000 updates\n",
      "completed 4000 updates\n",
      "completed 6000 updates\n",
      "completed 8000 updates\n",
      "completed 10000 updates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonahpoczobutt/projects/TunaSim/TunaSims.py:119: RuntimeWarning: invalid value encountered in divide\n",
      "  expanded_term = expanded * np.sum((expanded_difs + expanded_mults) / add_norm)\n",
      "/Users/jonahpoczobutt/projects/TunaSim/TunaSims.py:102: RuntimeWarning: overflow encountered in power\n",
      "  add_norm = add_norm_a * (np.power(query, add_norm_b) + np.power(target, add_norm_b))\n",
      "/Users/jonahpoczobutt/projects/TunaSim/TunaSims.py:102: RuntimeWarning: overflow encountered in multiply\n",
      "  add_norm = add_norm_a * (np.power(query, add_norm_b) + np.power(target, add_norm_b))\n",
      "/Users/jonahpoczobutt/projects/TunaSim/TunaSims.py:98: RuntimeWarning: divide by zero encountered in power\n",
      "  expanded_mults = mult_a * query * target ** mult_b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 2000 updates\n",
      "completed 4000 updates\n",
      "completed 6000 updates\n",
      "completed 8000 updates\n",
      "completed 10000 updates\n",
      "completed 2000 updates\n",
      "completed 4000 updates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonahpoczobutt/projects/TunaSim/tools.py:469: RuntimeWarning: divide by zero encountered in scalar power\n",
      "  return np.power(intensities, offset + mz_power_array + fixed_exp + scipy.stats.entropy(intensities) ** entropy_exp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 6000 updates\n",
      "completed 8000 updates\n",
      "completed 10000 updates\n",
      "completed 2000 updates\n",
      "completed 4000 updates\n",
      "completed 6000 updates\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m key:\n\u001b[1;32m     18\u001b[0m             test_subset[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m---> 20\u001b[0m res \u001b[38;5;241m=\u001b[39m testUtils\u001b[38;5;241m.\u001b[39mfunc_err_tester(func_skeletons, test_subset, {dataset_name: [demo_matches,demo_matches_test]}, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m     21\u001b[0m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msim_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sim_name\n\u001b[1;32m     23\u001b[0m res\u001b[38;5;241m.\u001b[39mto_csv(output_file, mode \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/projects/TunaSim/testUtils.py:35\u001b[0m, in \u001b[0;36mfunc_err_tester\u001b[0;34m(base_objects, test_params, datasets, logpath, verbose)\u001b[0m\n\u001b[1;32m     33\u001b[0m train_func\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m params\n\u001b[1;32m     34\u001b[0m train_func\u001b[38;5;241m.\u001b[39minit_vals \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(params)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m---> 35\u001b[0m train_func\u001b[38;5;241m.\u001b[39mfit(train, verbose \u001b[38;5;241m=\u001b[39m verbose)\n\u001b[1;32m     37\u001b[0m fitted_func \u001b[38;5;241m=\u001b[39m train_func\u001b[38;5;241m.\u001b[39mtrained_func()\n\u001b[1;32m     39\u001b[0m train_estimates \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(train))\n",
      "File \u001b[0;32m~/projects/TunaSim/funcOb.py:98\u001b[0m, in \u001b[0;36mfunc_ob.fit\u001b[0;34m(self, train_data, warm_start, verbose)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambdas \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambdas)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstoch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstoch_descent(train_data, warm_start, verbose)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscipy_solver_estimate(train_data, warm_start)\n",
      "File \u001b[0;32m~/projects/TunaSim/funcOb.py:167\u001b[0m, in \u001b[0;36mfunc_ob.stoch_descent\u001b[0;34m(self, train_data, warm_start, verbose)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m#estimate gradient and update values\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m approx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_vals, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon, [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, train_data\u001b[38;5;241m.\u001b[39miloc[index:index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39misnan(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad)) \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39misinf(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad)):\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbad grad\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/optimize/_optimize.py:1087\u001b[0m, in \u001b[0;36mapprox_fprime\u001b[0;34m(xk, f, epsilon, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Finite difference approximation of the derivatives of a\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;124;03mscalar or vector-valued function.\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \n\u001b[1;32m   1085\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m xk \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(xk, \u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m-> 1087\u001b[0m f0 \u001b[38;5;241m=\u001b[39m f(xk, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m approx_derivative(f, xk, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2-point\u001b[39m\u001b[38;5;124m'\u001b[39m, abs_step\u001b[38;5;241m=\u001b[39mepsilon,\n\u001b[1;32m   1090\u001b[0m                          args\u001b[38;5;241m=\u001b[39margs, f0\u001b[38;5;241m=\u001b[39mf0)\n",
      "File \u001b[0;32m~/projects/TunaSim/funcOb.py:10\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(x, args, loss_func, reg_func, sim_func)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(x, args, loss_func, reg_func, sim_func):\n\u001b[1;32m      9\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k:v \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args[\u001b[38;5;241m0\u001b[39m],x)}\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(args[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m i: loss_func(i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m sim_func(i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     11\u001b[0m                                                                           i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     12\u001b[0m                                                                           i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecquery\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     13\u001b[0m                                                                           i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprectarget\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     14\u001b[0m                                                                           \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m reg_func(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:10034\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m  10022\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10024\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10025\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10026\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10032\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10033\u001b[0m )\n\u001b[0;32m> 10034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:837\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:968\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    965\u001b[0m results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[1;32m    967\u001b[0m \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[0;32m--> 968\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1005\u001b[0m, in \u001b[0;36mFrameApply.wrap_results\u001b[0;34m(self, results, res_index)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     result \u001b[38;5;241m=\u001b[39m constructor_sliced(results, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1005\u001b[0m     result \u001b[38;5;241m=\u001b[39m constructor_sliced(results)\n\u001b[1;32m   1006\u001b[0m result\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m res_index\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:475\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    473\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_dict_like(data):\n\u001b[0;32m--> 475\u001b[0m     data, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_dict(data, index, dtype)\n\u001b[1;32m    476\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:568\u001b[0m, in \u001b[0;36mSeries._init_dict\u001b[0;34m(self, data, index, dtype)\u001b[0m\n\u001b[1;32m    565\u001b[0m     keys, values \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;241m0\u001b[39m), []\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# Input is now list-like, so rely on \"standard\" construction:\u001b[39;00m\n\u001b[0;32m--> 568\u001b[0m s \u001b[38;5;241m=\u001b[39m Series(values, index\u001b[38;5;241m=\u001b[39mkeys, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    570\u001b[0m \u001b[38;5;66;03m# Now we just make sure the order is respected, if any\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:428\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    425\u001b[0m name \u001b[38;5;241m=\u001b[39m ibase\u001b[38;5;241m.\u001b[39mmaybe_extract_name(name, data, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 428\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dtype(dtype)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:7577\u001b[0m, in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7575\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy, tupleize_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   7576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 7577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:560\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[1;32m    557\u001b[0m         data \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39m_dtype_obj)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     arr \u001b[38;5;241m=\u001b[39m sanitize_array(data, \u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex must be specified when data is not list-like\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/construction.py:653\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    650\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m _try_cast(data, dtype, copy)\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 653\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m maybe_convert_platform(data)\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subarr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    655\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/cast.py:134\u001b[0m, in \u001b[0;36mmaybe_convert_platform\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m _dtype_obj:\n\u001b[1;32m    133\u001b[0m     arr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, arr)\n\u001b[0;32m--> 134\u001b[0m     arr \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(arr)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32mlib.pyx:2525\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/numeric.py:329\u001b[0m, in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, order, like)\u001b[0m\n\u001b[1;32m    327\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m asarray(fill_value)\n\u001b[1;32m    328\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m fill_value\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 329\u001b[0m a \u001b[38;5;241m=\u001b[39m empty(shape, dtype, order)\n\u001b[1;32m    330\u001b[0m multiarray\u001b[38;5;241m.\u001b[39mcopyto(a, fill_value, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_file = '/Users/jonahpoczobutt/projects/TunaRes/res_new.csv'\n",
    "reload(testUtils)\n",
    "for dataset_name, sets in datasets.items():\n",
    "\n",
    "    sim_name = dataset_name.split('_')[0]\n",
    "    params_set = params_to_test[sim_name]\n",
    "\n",
    "    demo_matches['match'] = sets[0]\n",
    "    demo_matches_test['match'] = sets[1]\n",
    "\n",
    "    test_subset = dict()\n",
    "    for name in params_set:\n",
    "\n",
    "        for key, value in params_dict.items():\n",
    "\n",
    "            if name in key:\n",
    "\n",
    "                test_subset[key] = value\n",
    "\n",
    "    #add the similarity measure to func skeleton\n",
    "    skeletons_with_sim = list()\n",
    "    for skeleton in func_skeletons:\n",
    "\n",
    "        copied_skeleton = copy.deepcopy(skeleton)\n",
    "        copied_skeleton.sim_func = partial(tuna_sim,**sims[dataset_name.split('_')[0]])\n",
    "        skeletons_with_sim.append(copied_skeleton)\n",
    "\n",
    "    func_skeletons_ = func_skeletons + skeletons_with_sim\n",
    "                \n",
    "    #get train and test errors for \n",
    "    res = testUtils.func_err_tester(func_skeletons_, \n",
    "                                    test_subset, \n",
    "                                    {dataset_name: [demo_matches,demo_matches_test]})\n",
    "    \n",
    "    res['sim_name'] = sim_name\n",
    "\n",
    "    res.to_csv(output_file, mode ='a', header = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_skeletons[0].lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
