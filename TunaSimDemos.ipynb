{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import testUtils\n",
    "\n",
    "from TunaSims import tuna_sim\n",
    "from funcOb import func_ob\n",
    "import math_distance\n",
    "import tools\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that Sims are in funtion space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/train/10_ppm/chunk_1.pkl')\n",
    "demo_matches_test = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/train/10_ppm/chunk_2.pkl')\n",
    "demo_query = demo_matches.iloc[0]['query']\n",
    "demo_target = demo_matches.iloc[0]['target']\n",
    "demo_query_prec = demo_matches.iloc[0]['precquery']\n",
    "demo_target_prec = demo_matches.iloc[0]['prectarget']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manhattan: 1.9321462207955165e-09\n",
      "dot_product: 0.0\n",
      "harmonic_mean: 0.0\n"
     ]
    }
   ],
   "source": [
    "manhattan_tuna = tuna_sim(demo_query,\n",
    "              demo_target,\n",
    "              demo_query_prec,\n",
    "              demo_target_prec,\n",
    "              dif_a = 1,\n",
    "              dif_b = 1,\n",
    "              unnormed = 1)\n",
    "\n",
    "dot_tuna = tuna_sim(demo_query,\n",
    "              demo_target,\n",
    "              demo_query_prec,\n",
    "              demo_target_prec,\n",
    "              mult_a = 1,\n",
    "              mult_b = 2,\n",
    "              collapsed = 1,\n",
    "              mult_norm_a= 1,\n",
    "              mult_norm_b= 2,\n",
    "              sim_flip=True)\n",
    "\n",
    "harmonic_tuna = tuna_sim(demo_query,\n",
    "              demo_target,\n",
    "              demo_query_prec,\n",
    "              demo_target_prec,\n",
    "              mult_a = 1,\n",
    "              mult_b = 1,\n",
    "              expanded = 2,\n",
    "              add_norm_a= 1,\n",
    "              add_norm_b= 1,\n",
    "              sim_flip=True)\n",
    "\n",
    "demo_query[:,1] /= sum(demo_query[:,1])\n",
    "demo_target[:,1] /= sum(demo_target[:,1])\n",
    "combined_old = tools.match_peaks_in_spectra(demo_query, demo_target, ms2_da=0.05)\n",
    "manhattan = 1 - tools.sigmoid(math_distance.manhattan_distance(combined_old[:,1], combined_old[:,2]))\n",
    "dot_product = tools.sigmoid(1 - math_distance.dot_product_nosqrt_distance(combined_old[:,1], combined_old[:,2]))\n",
    "harmonic_mean = tools.sigmoid(1 - math_distance.harmonic_mean_distance(combined_old[:,1], combined_old[:,2]))\n",
    "\n",
    "print(f'manhattan: {abs(manhattan - manhattan_tuna)}')\n",
    "print(f'dot_product: {abs(dot_product - dot_tuna)}')\n",
    "print(f'harmonic_mean: {abs(harmonic_mean - harmonic_tuna)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we recover similarity function from scores and input vectors alone, which training strategies are best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First We will do no funny biz with cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_skeletons = [\n",
    "    func_ob(\n",
    "    name = \"base_10k_iter\",\n",
    "    sim_func = partial(tuna_sim),\n",
    "    init_vals= 0.1,\n",
    "    params = None,\n",
    "    tol = 0,\n",
    "    lambdas= 1,\n",
    "    max_iter = 10000,\n",
    "    epsilon = 1e-5),\n",
    "    func_ob(\n",
    "    name = \"base_10k_iter_1e2\",\n",
    "    sim_func = partial(tuna_sim),\n",
    "    init_vals= 0.1,\n",
    "    params = None,\n",
    "    tol = 0,\n",
    "    lambdas= 1,\n",
    "    max_iter = 10000,\n",
    "    epsilon = 1e-2),\n",
    "    func_ob(\n",
    "    name = \"base_10k_iter_zero\",\n",
    "    sim_func = partial(tuna_sim),\n",
    "    init_vals= 0.1,\n",
    "    params = None,\n",
    "    tol = 0,\n",
    "    lambdas= 1,\n",
    "    max_iter = 10000,\n",
    "    epsilon = 1e-2,\n",
    "    zero_grad_lambda_boost = 2),\n",
    "    # funcOb.func_ob(\n",
    "    # name = \"base_10k_iter_flex\",\n",
    "    # sim_func = partial(tuna_sim),\n",
    "    # init_vals= 0.1,\n",
    "    # params = None,\n",
    "    # tol = 0,\n",
    "    # lambdas= 1,\n",
    "    # max_iter = 10000,\n",
    "    # epsilon = 1e-2,\n",
    "    # zero_grad_lambda_boost = 2,\n",
    "    #),\n",
    "]\n",
    "\n",
    "params = {\n",
    "    \"dif_only\": ['unnormed','dif_a','dif_b'],\n",
    "    \"dif_and_mult\": ['unnormed','dif_a','dif_b','mult_a','mult_b'],\n",
    "    \"collapsed\": ['collapsed','dif_a','dif_b','mult_a','mult_b', 'mult_norm_a','mult_norm_b'],\n",
    "    \"expanded\": ['expanded','dif_a','dif_b','mult_a','mult_b', 'add_norm_a', 'add_norm_b'],\n",
    "    \"collapsed_and_unnorm\": ['unnormed', 'collapsed','dif_a','dif_b','mult_a','mult_b','mult_norm_a','mult_norm_b', 'add_norm_a', 'add_norm_b'],\n",
    "    \"expanded_and_unnorm\": ['unnormed', 'expanded','dif_a','dif_b','mult_a','mult_b', 'add_norm_a', 'add_norm_b']  \n",
    "}\n",
    "\n",
    "params_dict = dict()\n",
    "for key, value in params.items():\n",
    "    params_dict[f'{key}__cleaning'] = value+['query_max_mz_fix',\n",
    "                                               'target_max_mz_fix', \n",
    "                                               'query_fixed_noise', \n",
    "                                               'target_fixed_noise',\n",
    "                                                'query_da_thresh',\n",
    "                                                'target_da_thresh',\n",
    "                                                'query_fixed_power',\n",
    "                                                'target_fixed_power']\n",
    "    \n",
    "    params_dict[f'{key}__reweight'] = value+['query_fixed_power',\n",
    "                                            'query_mz_power',\n",
    "                                            'query_ent_power',\n",
    "                                            'target_fixed_power',\n",
    "                                            'target_mz_power',\n",
    "                                            'target_ent_power',\n",
    "                                            'query_reweight_offset',\n",
    "                                            'target_reweight_offset']\n",
    "    \n",
    "    params_dict[f'{key}__cleaning__reweight'] = value+['query_max_mz_fix',\n",
    "                                               'target_max_mz_fix', \n",
    "                                               'query_fixed_noise', \n",
    "                                               'target_fixed_noise',\n",
    "                                                'query_da_thresh',\n",
    "                                                'target_da_thresh',\n",
    "                                                'query_fixed_power',\n",
    "                                                'target_fixed_power',\n",
    "                                                'query_fixed_power',\n",
    "                                                'query_mz_power',\n",
    "                                                'query_ent_power',\n",
    "                                                'target_fixed_power',\n",
    "                                                'target_mz_power',\n",
    "                                                'target_ent_power',\n",
    "                                                'query_reweight_offset',\n",
    "                                                'target_reweight_offset']\n",
    "    \n",
    "# params.update(new_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to turn this into an easy way to generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "manhattan_tuna = {'dif_a' : 1,\n",
    "              'dif_b' : 1,\n",
    "              'unnormed' : 1}\n",
    "\n",
    "dot_tuna = {'mult_a' : 1,\n",
    "              'mult_b' : 2,\n",
    "              'collapsed' : 1,\n",
    "              'mult_norm_a': 1,\n",
    "              'mult_norm_b': 2,\n",
    "              'sim_flip':True}\n",
    "\n",
    "harmonic_tuna = {'mult_a' :1,\n",
    "              'mult_b' : 1,\n",
    "              'expanded' : 2,\n",
    "              'add_norm_a' : 1,\n",
    "              'add_norm_b': 1,\n",
    "              'sim_flip' : True}\n",
    "\n",
    "clean1 = {'query_max_mz_fix' : 1.6,\n",
    "            'query_fixed_noise' : 0.01,\n",
    "            'query_da_thresh' : 0.05,\n",
    "            'target_max_mz_fix' : 1.6,\n",
    "            'target_fixed_noise' : 0.01,\n",
    "            'target_da_thresh' : 0.05}\n",
    "\n",
    "clean2 = {'query_max_mz_var' : 0.01,\n",
    "            'query_var_noise' : 0.01,\n",
    "            'query_da_thresh' : 0.05,\n",
    "              'target_max_mz_var': 0.01,\n",
    "              'target_var_noise' :0.01,\n",
    "              'target_da_thresh' : 0.05}\n",
    "\n",
    "clean3 = {'query_max_mz_fix' : 0.8,\n",
    "        'query_max_mz_var' : 0.005,\n",
    "        'query_fixed_noise' : 0.01,\n",
    "        'query_var_noise': 0.01,\n",
    "        'query_da_thresh' :0.05,\n",
    "        'target_max_mz_fix' : 0.8,\n",
    "        'target_max_mz_var' : 0.005,\n",
    "        'target_fixed_noise' : 0.01,\n",
    "        'target_var_noise' : 0.01,\n",
    "        'target_da_thresh' : 0.05}\n",
    "\n",
    "reweight1 = {'query_fixed_power' : 0.75,\n",
    "             'target_fixed_power' : 0.75}\n",
    "\n",
    "reweight2 = {'query_ent_power' : 0.5,\n",
    "             'target_ent_power' : 0.5}\n",
    "\n",
    "reweight3 = {'query_ent_power' : 0.1,\n",
    "              'target_mz_power' :  0.1,\n",
    "              'query_reweight_offset' : -2,\n",
    "              'target_reweight_offset' : -2}\n",
    "\n",
    "reweight4 = {'query_fixed_power' : 0.2,\n",
    "              'query_mz_power' :0.1,\n",
    "              'query_ent_power' : 0.25,\n",
    "              'target_fixed_power' : 0.2,\n",
    "              'target_mz_power' : 0.1,\n",
    "              'target_ent_power' : 0.25,\n",
    "              'query_reweight_offset' : -2,\n",
    "              'target_reweight_offset' : -2}\n",
    "\n",
    "sims = {\"manhattan\": manhattan_tuna,\n",
    "        \"dot_product\": dot_tuna,\n",
    "        \"harmoinc\": harmonic_tuna}\n",
    "\n",
    "cleans = {\"clean1\": clean1,\n",
    "          \"clean2\": clean2,\n",
    "          \"clean3\": clean3,\n",
    "          \"None\": {}}\n",
    "\n",
    "reweights = {\"reweight1\":reweight1,\n",
    "             \"reweight2\": reweight2,\n",
    "             \"reweight3\": reweight3,\n",
    "             \"reweight4\": reweight4,\n",
    "             \"None\": {}}\n",
    "\n",
    "funcs_to_find = dict()\n",
    "\n",
    "for sim, simval in sims.items():\n",
    "    for clean, cleanval in cleans.items():\n",
    "        for reweight, reweight_val in reweights.items():\n",
    "            \n",
    "            vals = {}\n",
    "            vals.update(simval)\n",
    "            vals.update(cleanval)\n",
    "            vals.update(reweight_val)\n",
    "    \n",
    "            funcs_to_find[f'{sim}_{clean}_{reweight}'] = partial(tuna_sim, **vals)\n",
    "\n",
    "params_to_test = {'manhattan':['dif_only','expanded','dif_and_mult'],\n",
    "                  'dot': ['collapsed','expanded','collapsed_and_unnorm'],\n",
    "                  'harmonic': ['collapsed','expanded','collapsed_and_unnorm']\n",
    "                }\n",
    "\n",
    "datasets = dict()\n",
    "\n",
    "demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/train/10_ppm/chunk_1.pkl')\n",
    "demo_matches_test = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/test/10_ppm/chunk_1.pkl')\n",
    "\n",
    "demo_matches = demo_matches.sample(frac=1)[:100000]\n",
    "demo_matches_test = demo_matches_test.sample(frac=1)[:100000]\n",
    "\n",
    "for funcname, func in funcs_to_find.items():\n",
    "\n",
    "    datasets[funcname] = list()\n",
    "    datasets[funcname].append(testUtils.create_scores_from_tuna(demo_matches,func))\n",
    "    datasets[funcname].append(testUtils.create_scores_from_tuna(demo_matches_test,func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_test = {'manhattan':['dif_only','expanded','dif_and_mult'],\n",
    "                  'dot': ['collapsed','expanded','collapsed_and_unnorm'],\n",
    "                  'harmonic': ['collapsed','expanded','collapsed_and_unnorm']\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train funcs on different scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '/Users/jonahpoczobutt/projects/TunaRes/res_new.csv'\n",
    "for dataset_name, sets in datasets.items():\n",
    "\n",
    "    sim_name = dataset_name.split('_')[0]\n",
    "    params_set = params_to_test[sim_name]\n",
    "\n",
    "    demo_matches['match'] = sets[0]\n",
    "    demo_matches_test['match'] = sets[1]\n",
    "\n",
    "    test_subset = dict()\n",
    "    for name in params_set:\n",
    "\n",
    "        for key, value in params_dict.items():\n",
    "\n",
    "            if name in key:\n",
    "\n",
    "                test_subset[key] = value\n",
    "                \n",
    "    res = testUtils.func_err_tester(func_skeletons, test_subset, {dataset_name: [demo_matches,demo_matches_test]})\n",
    "    res['sim_name'] = sim_name\n",
    "\n",
    "    res.to_csv(output_file, mode ='a', header = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
