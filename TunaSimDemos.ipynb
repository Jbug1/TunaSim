{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from TunaSims import tuna_sim\n",
    "import funcOb\n",
    "import math_distance\n",
    "import tools\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manhattan Similarity Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/train/10_ppm/chunk_1.pkl')\n",
    "demo_matches_test = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/train/10_ppm/chunk_2.pkl')\n",
    "demo_query = demo_matches.iloc[0]['query']\n",
    "demo_target = demo_matches.iloc[0]['target']\n",
    "demo_query_prec = demo_matches.iloc[0]['precquery']\n",
    "demo_target_prec = demo_matches.iloc[0]['prectarget']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_tuna = tuna_sim(demo_query,\n",
    "              demo_target,\n",
    "              demo_query_prec,\n",
    "              demo_target_prec,\n",
    "              dif_a = 1,\n",
    "              dif_b = 1,\n",
    "              unnormed = 1)\n",
    "\n",
    "dot_tuna = tuna_sim(demo_query,\n",
    "              demo_target,\n",
    "              demo_query_prec,\n",
    "              demo_target_prec,\n",
    "              mult_a = 1,\n",
    "              mult_b = 2,\n",
    "              collapsed = 1,\n",
    "              mult_norm_a= 1,\n",
    "              mult_norm_b= 2,\n",
    "              sim_flip=True)\n",
    "\n",
    "harmonic_tuna = tuna_sim(demo_query,\n",
    "              demo_target,\n",
    "              demo_query_prec,\n",
    "              demo_target_prec,\n",
    "              mult_a = 1,\n",
    "              mult_b = 1,\n",
    "              expanded = 2,\n",
    "              add_norm_a= 1,\n",
    "              add_norm_b= 1,\n",
    "              sim_flip=True)\n",
    "\n",
    "demo_query[:,1] /= sum(demo_query[:,1])\n",
    "demo_target[:,1] /= sum(demo_target[:,1])\n",
    "combined_old = tools.match_peaks_in_spectra(demo_query, demo_target, ms2_da=0.05)\n",
    "manhattan = 1 - tools.sigmoid(math_distance.manhattan_distance(combined_old[:,1], combined_old[:,2]))\n",
    "dot_product = tools.sigmoid(1 - math_distance.dot_product_nosqrt_distance(combined_old[:,1], combined_old[:,2]))\n",
    "harmonic_mean = tools.sigmoid(1 - math_distance.harmonic_mean_distance(combined_old[:,1], combined_old[:,2]))\n",
    "\n",
    "print(f'manhattan: {abs(manhattan - manhattan_tuna)}')\n",
    "print(f'dot_product: {abs(dot_product - dot_tuna)}')\n",
    "print(f'harmonic_mean: {abs(harmonic_mean - harmonic_tuna)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we recover similarity function from scores and input vectors alone, which training strategies are best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_err_tester(base_objects, test_params, datasets , logpath=None):\n",
    "    \"\"\" \n",
    "    base objects: func_obs pproperly named\n",
    "    test_params: dict with key: name value: params to be fit on\n",
    "    datasets: \"dict key: name value: tuple of train and test\n",
    "    \"\"\"\n",
    "\n",
    "    results = list()\n",
    "    for object in base_objects:\n",
    "\n",
    "        for name, params in test_params.items():\n",
    "\n",
    "            for dataset_name, (train, test) in datasets.items():\n",
    "\n",
    "                #don't train the original\n",
    "                train_func = copy.deepcopy(object)\n",
    "\n",
    "                #intiailize proper values and train\n",
    "                train_func.params = params\n",
    "                train_func.init_vals = np.zeros(len(params)) + 0.5\n",
    "                train_func.fit(train)\n",
    "\n",
    "                fitted_func = train_func.trained_func()\n",
    "\n",
    "                train_estimates = np.zeros(len(train))\n",
    "                for i in range(len(train)):\n",
    "\n",
    "                    train_estimates[i] = fitted_func(train.iloc[i]['query'],\n",
    "                                                    train.iloc[i]['target'],\n",
    "                                                    train.iloc[i]['precquery'],\n",
    "                                                    train.iloc[i]['prectarget'])\n",
    "                    \n",
    "                test_estimates = np.zeros(len(test))\n",
    "                for i in range(len(test)):\n",
    "\n",
    "                    test_estimates[i] = fitted_func(test.iloc[i]['query'],\n",
    "                                                    test.iloc[i]['target'],\n",
    "                                                    test.iloc[i]['precquery'],\n",
    "                                                    test.iloc[i]['prectarget'])\n",
    "                    \n",
    "                results.append([object.name, \n",
    "                               name,\n",
    "                               train_func.init_vals,\n",
    "                               dataset_name,\n",
    "                               np.mean(abs(train_estimates - train['match'].to_numpy())),\n",
    "                               np.mean(abs(test_estimates - test['match'].to_numpy())),\n",
    "                               np.mean(abs(test_estimates - train['match'].to_numpy()))])\n",
    "                \n",
    "                if logpath is not None:\n",
    "                    with open(logpath, 'a') as handle:\n",
    "                        handle.write(f'''{[object.name, \n",
    "                                        name, \n",
    "                                        train_func.init_vals,\n",
    "                                        dataset_name,\n",
    "                                        np.mean(abs(train_estimates - train['match'].to_numpy())),\n",
    "                                        np.mean(abs(test_estimates - test['match'].to_numpy())),\n",
    "                                        np.mean(abs(test_estimates - train['match'].to_numpy()))]} \\n''')\n",
    "\n",
    "    return pd.DataFrame(results, columns = ['name', 'params', 'trained_values', 'metric', 'train_err', 'test_err', 'range_control'])\n",
    "\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_skeletons = [\n",
    "    funcOb.func_ob(\n",
    "    name = \"base_1k_iter\",\n",
    "    sim_func = partial(tuna_sim),\n",
    "    init_vals= [1,1],\n",
    "    params = [1,1],\n",
    "    tol = 0,\n",
    "    lambdas= 1,\n",
    "    max_iter = 2000,\n",
    "    epsilon = 1e-5),\n",
    "    # funcOb.func_ob(\n",
    "    # name = \"base_10k_iter\",\n",
    "    # sim_func = partial(tuna_sim),\n",
    "    # init_vals= [1,1],\n",
    "    # params = [1,1],\n",
    "    # tol = 0,\n",
    "    # lambdas= 1,\n",
    "    # max_iter = 10000,\n",
    "    # epsilon = 1e-5),\n",
    "]\n",
    "\n",
    "params = {\n",
    "    \"dif_only\": ['unnormed','dif_a','dif_b','sim_flip'],\n",
    "    # \"dif_and_mult\": ['unnormed','dif_a','dif_b','mult_a','mult_b', 'sim_flip'],\n",
    "    \"collapsed\": ['collapsed','dif_a','dif_b','mult_a','mult_b', 'mult_norm_a','mult_norm_b', 'sim_flip'],\n",
    "    \"expanded\": ['expanded','dif_a','dif_b','mult_a','mult_b', 'add_norm_a', 'add_norm_b', 'sim_flip'],\n",
    "    # \"collapsed_and_unnorm\": ['unnormed', 'collapsed','dif_a','dif_b','mult_a','mult_b','mult_norm_a','mult_norm_b', 'add_norm_a', 'add_norm_b', 'sim_flip'],\n",
    "    # \"expanded_and_unnorm\": ['unnormed', 'expanded','dif_a','dif_b','mult_a','mult_b', 'add_norm_a', 'add_norm_b', 'sim_flip'],  \n",
    "}\n",
    "\n",
    "new_dict = dict()\n",
    "for key, value in params.items():\n",
    "    new_dict[f'{key}_cleaning'] = value+['query_max_mz_fix',\n",
    "                                               'target_max_mz_fix', \n",
    "                                               'query_fixed_noise', \n",
    "                                               'target_fixed_noise',\n",
    "                                                'query_da_thresh',\n",
    "                                                'target_da_thresh',\n",
    "                                                'query_fixed_power',\n",
    "                                                'target_fixed_power']\n",
    "    \n",
    "# params.update(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = dict()\n",
    "\n",
    "demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/train/10_ppm/chunk_1.pkl')\n",
    "demo_matches_test = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/test/10_ppm/chunk_1.pkl')\n",
    "\n",
    "demo_matches = demo_matches.sample(frac=1)[:10000]\n",
    "demo_matches_test = demo_matches_test.sample(frac=1)[:10000]\n",
    "\n",
    "sims = np.zeros(len(demo_matches))\n",
    "for i in range(len(demo_matches)):\n",
    "\n",
    "    q = demo_matches.iloc[i]['query']\n",
    "    t = demo_matches.iloc[i]['target']\n",
    "\n",
    "    q[:,1] /= sum(q[:,1])\n",
    "    t[:,1] /= sum(t[:,1])\n",
    "    combined = tools.match_peaks_in_spectra(q, t, ms2_da=0.05)\n",
    "    sims[i] = 1- tools.sigmoid(math_distance.manhattan_distance(combined[:,1], combined[:,2]))\n",
    "\n",
    "demo_matches['match'] = sims\n",
    "demo_matches['match'].fillna(0, inplace = True)\n",
    "\n",
    "sims_test = np.zeros(len(demo_matches_test))\n",
    "for i in range(len(demo_matches_test)):\n",
    "\n",
    "    q = demo_matches_test.iloc[i]['query']\n",
    "    t = demo_matches_test.iloc[i]['target']\n",
    "\n",
    "    q[:,1] /= sum(q[:,1])\n",
    "    t[:,1] /= sum(t[:,1])\n",
    "    combined = tools.match_peaks_in_spectra(q, t, ms2_da=0.05)\n",
    "    sims_test[i] = 1- tools.sigmoid(math_distance.manhattan_distance(combined[:,1], combined[:,2]))\n",
    "\n",
    "demo_matches_test['match'] = sims_test\n",
    "demo_matches_test['match'].fillna(0, inplace = True)\n",
    "\n",
    "datasets['manhattan'] = (demo_matches[['query','target','precquery','prectarget','match']], demo_matches_test[['query','target','precquery','prectarget','match']])\n",
    "\n",
    "sims = np.zeros(len(demo_matches))\n",
    "for i in range(len(demo_matches)):\n",
    "\n",
    "    q = demo_matches.iloc[i]['query']\n",
    "    t = demo_matches.iloc[i]['target']\n",
    "\n",
    "    q = tools.tuna_clean_spectrum(q,\n",
    "                                  max_mz=demo_matches.iloc[i]['precquery']-1.6,\n",
    "                                  ms2_da = 0.05,\n",
    "                                  noise_removal_fixed = 0.01,\n",
    "                                  noise_removal_var=0)\n",
    "    \n",
    "    t = tools.tuna_clean_spectrum(t,\n",
    "                                  max_mz=demo_matches.iloc[i]['prectarget']-1.6,\n",
    "                                  ms2_da = 0.05,\n",
    "                                  noise_removal_fixed = 0.01,\n",
    "                                  noise_removal_var=0)\n",
    "    \n",
    "    q[:,1] = tools.tuna_weight_intensity(q, fixed_exp = 0.75)\n",
    "    t[:,1] = tools.tuna_weight_intensity(t, fixed_exp = 0.75)\n",
    "\n",
    "    q[:,1] /= sum(q[:,1])\n",
    "    t[:,1] /= sum(t[:,1])\n",
    "    combined = tools.match_peaks_in_spectra(q, t, ms2_da=0.05)\n",
    "    sims[i] = 1- tools.sigmoid(math_distance.manhattan_distance(combined[:,1], combined[:,2]))\n",
    "\n",
    "demo_matches['match'] = sims\n",
    "demo_matches['match'].fillna(0, inplace=True)\n",
    "\n",
    "sims_test = np.zeros(len(demo_matches_test))\n",
    "for i in range(len(demo_matches_test)):\n",
    "\n",
    "    q = demo_matches_test.iloc[i]['query']\n",
    "    t = demo_matches_test.iloc[i]['target']\n",
    "\n",
    "    q = tools.tuna_clean_spectrum(q,\n",
    "                                  max_mz=demo_matches_test.iloc[i]['precquery']-1.6,\n",
    "                                  ms2_da = 0.05,\n",
    "                                  noise_removal_fixed = 0.01,\n",
    "                                  noise_removal_var=0)\n",
    "    \n",
    "    t = tools.tuna_clean_spectrum(t,\n",
    "                                  max_mz=demo_matches_test.iloc[i]['prectarget']-1.6,\n",
    "                                  ms2_da = 0.05,\n",
    "                                  noise_removal_fixed = 0.01,\n",
    "                                  noise_removal_var=0)\n",
    "    \n",
    "    q[:,1] = tools.tuna_weight_intensity(q, fixed_exp = 0.75)\n",
    "    t[:,1] = tools.tuna_weight_intensity(t, fixed_exp = 0.75)\n",
    "\n",
    "    q[:,1] /= sum(q[:,1])\n",
    "    t[:,1] /= sum(t[:,1])\n",
    "    combined = tools.match_peaks_in_spectra(q, t, ms2_da=0.05)\n",
    "    sims_test[i] = 1- tools.sigmoid(math_distance.manhattan_distance(combined[:,1], combined[:,2]))\n",
    "\n",
    "demo_matches_test['match'] = sims_test\n",
    "demo_matches_test['match'].fillna(0, inplace = True)\n",
    "\n",
    "datasets['manhattan_clean'] = (demo_matches[['query','target','precquery','prectarget','match']], demo_matches_test[['query','target','precquery','prectarget','match']])\n",
    "\n",
    "sims = np.zeros(len(demo_matches))\n",
    "for i in range(len(demo_matches)):\n",
    "\n",
    "    q = demo_matches.iloc[i]['query']\n",
    "    t = demo_matches.iloc[i]['target']\n",
    "\n",
    "    q[:,1] /= sum(q[:,1])\n",
    "    t[:,1] /= sum(t[:,1])\n",
    "    combined = tools.match_peaks_in_spectra(q, t, ms2_da=0.05)\n",
    "    sims[i] = 1- tools.sigmoid(math_distance.dot_product_distance(combined[:,1], combined[:,2]))\n",
    "\n",
    "demo_matches['match'] = sims\n",
    "demo_matches['match'].fillna(0, inplace=True)\n",
    "\n",
    "sims_test = np.zeros(len(demo_matches_test))\n",
    "for i in range(len(demo_matches_test)):\n",
    "\n",
    "    q = demo_matches_test.iloc[i]['query']\n",
    "    t = demo_matches_test.iloc[i]['target']\n",
    "\n",
    "    q[:,1] /= sum(q[:,1])\n",
    "    t[:,1] /= sum(t[:,1])\n",
    "    combined = tools.match_peaks_in_spectra(q, t, ms2_da=0.05)\n",
    "    sims_test[i] = 1- tools.sigmoid(math_distance.dot_product_distance(combined[:,1], combined[:,2]))\n",
    "\n",
    "demo_matches_test['match'] = sims_test\n",
    "demo_matches_test['match'].fillna(0, inplace = True)\n",
    "\n",
    "datasets['dot_product'] = (demo_matches[['query','target','precquery','prectarget','match']], demo_matches_test[['query','target','precquery','prectarget','match']])\n",
    "\n",
    "sims = np.zeros(len(demo_matches))\n",
    "for i in range(len(demo_matches)):\n",
    "\n",
    "    q = demo_matches.iloc[i]['query']\n",
    "    t = demo_matches.iloc[i]['target']\n",
    "\n",
    "    q = tools.tuna_clean_spectrum(q,\n",
    "                                  max_mz=demo_matches.iloc[i]['precquery']-1.6,\n",
    "                                  ms2_da = 0.05,\n",
    "                                  noise_removal_fixed = 0.01,\n",
    "                                  noise_removal_var=0)\n",
    "    \n",
    "    t = tools.tuna_clean_spectrum(t,\n",
    "                                  max_mz=demo_matches.iloc[i]['prectarget']-1.6,\n",
    "                                  ms2_da = 0.05,\n",
    "                                  noise_removal_fixed = 0.01,\n",
    "                                  noise_removal_var=0)\n",
    "    \n",
    "    q[:,1] = tools.tuna_weight_intensity(q, fixed_exp = 0.75)\n",
    "    t[:,1] = tools.tuna_weight_intensity(t, fixed_exp = 0.75)\n",
    "\n",
    "    q[:,1] /= sum(q[:,1])\n",
    "    t[:,1] /= sum(t[:,1])\n",
    "    combined = tools.match_peaks_in_spectra(q, t, ms2_da=0.05)\n",
    "    sims[i] = 1- tools.sigmoid(math_distance.dot_product_distance(combined[:,1], combined[:,2]))\n",
    "\n",
    "demo_matches['match'] = sims\n",
    "demo_matches['match'].fillna(0, inplace = True)\n",
    "\n",
    "sims_test = np.zeros(len(demo_matches_test))\n",
    "for i in range(len(demo_matches_test)):\n",
    "\n",
    "    q = demo_matches_test.iloc[i]['query']\n",
    "    t = demo_matches_test.iloc[i]['target']\n",
    "\n",
    "    q = tools.tuna_clean_spectrum(q,\n",
    "                                  max_mz=demo_matches_test.iloc[i]['precquery']-1.6,\n",
    "                                  ms2_da = 0.05,\n",
    "                                  noise_removal_fixed = 0.01,\n",
    "                                  noise_removal_var=0)\n",
    "    \n",
    "    t = tools.tuna_clean_spectrum(t,\n",
    "                                  max_mz=demo_matches_test.iloc[i]['prectarget']-1.6,\n",
    "                                  ms2_da = 0.05,\n",
    "                                  noise_removal_fixed = 0.01,\n",
    "                                  noise_removal_var=0)\n",
    "    \n",
    "    q[:,1] = tools.tuna_weight_intensity(q, fixed_exp = 0.75)\n",
    "    t[:,1] = tools.tuna_weight_intensity(t, fixed_exp = 0.75)\n",
    "\n",
    "    q[:,1] /= sum(q[:,1])\n",
    "    t[:,1] /= sum(t[:,1])\n",
    "    combined = tools.match_peaks_in_spectra(q, t, ms2_da=0.05)\n",
    "    sims_test[i] = 1- tools.sigmoid(math_distance.dot_product_distance(combined[:,1], combined[:,2]))\n",
    "\n",
    "demo_matches_test['match'] = sims_test\n",
    "demo_matches_test['match'].fillna(0, inplace = True)\n",
    "\n",
    "datasets['dotprod_clean'] = (demo_matches[['query','target','precquery','prectarget','match']], demo_matches_test[['query','target','precquery','prectarget','match']])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = func_err_tester(func_skeletons,\n",
    "                params,\n",
    "                datasets,\n",
    "                logpath = \"/Users/jonahpoczobutt/projects/TunaRes/log_2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ['unnormed', 'dif_a', 'dif_b', 'sim_flip']\n",
    "init_vals= [0.5 for i in range(len(params))]\n",
    "\n",
    "reload(funcOb)\n",
    "\n",
    "debug_func = funcOb.func_ob(\n",
    "    name = \"base_1k_iter\",\n",
    "    sim_func = partial(tuna_sim),\n",
    "    init_vals= init_vals,\n",
    "    params = params,\n",
    "    tol = 0,\n",
    "    lambdas= 1,\n",
    "    max_iter = 1000,\n",
    "    epsilon = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_func.fit(datasets['manhattan'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_func.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_func.init_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_func.trained_func()(datasets['manhattan'][0].iloc[0]['query'],\n",
    "                        datasets['manhattan'][0].iloc[0]['target'],\n",
    "                        datasets['manhattan'][0].iloc[0]['precquery'],\n",
    "                        datasets['manhattan'][0].iloc[0]['prectarget'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reload(funcOb)\n",
    "\n",
    "debug_func_ = funcOb.func_ob(\n",
    "    name = \"base_1k_iter\",\n",
    "    sim_func = partial(tuna_sim),\n",
    "    init_vals= debug_func.init_vals,\n",
    "    params = debug_func.params,\n",
    "    tol = 0,\n",
    "    lambdas= 1,\n",
    "    max_iter = 1,\n",
    "    epsilon = 1e-5)\n",
    "\n",
    "debug_func_.fit(datasets['manhattan'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
