{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TunaSims\n",
    "import numpy as np\n",
    "from funcTrainer import specSimTrainer\n",
    "import pandas as pd\n",
    "import datasetBuilder\n",
    "import tools_fast\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier as hgbc\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _weight_intensity_by_entropy(x):\n",
    "    WEIGHT_START = 0.25\n",
    "    ENTROPY_CUTOFF = 3\n",
    "    weight_slope = (1 - WEIGHT_START) / ENTROPY_CUTOFF\n",
    "\n",
    "    if np.sum(x) > 0:\n",
    "        entropy_x = scipy.stats.entropy(x)\n",
    "        if entropy_x < ENTROPY_CUTOFF:\n",
    "            weight = WEIGHT_START + weight_slope * entropy_x\n",
    "            x = np.power(x, weight)\n",
    "            x_sum = np.sum(x)\n",
    "            x = x / x_sum\n",
    "    return x\n",
    "\n",
    "def ppm(base, ppm):\n",
    "    \"\"\"\n",
    "    convert ppm threshold to dalton based on precursor exact mass (base)\n",
    "    \"\"\"\n",
    "\n",
    "    return base * (ppm / 1e6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_mean_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Harmonic mean distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        1-2\\sum(\\frac{P_{i}Q_{i}}{P_{i}+Q_{i}})\n",
    "    \"\"\"\n",
    "    p = _weight_intensity_by_entropy(p)\n",
    "    q = _weight_intensity_by_entropy(q)\n",
    "    return 2 * np.sum(p * q / (p + q))\n",
    "\n",
    "def lorentzian_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Lorentzian distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\sum{\\ln(1+|P_i-Q_i|)}\n",
    "    \"\"\"\n",
    "    p = _weight_intensity_by_entropy(p)\n",
    "    q = _weight_intensity_by_entropy(q)\n",
    "    return 1 - np.sum(np.log(1 + np.abs(p - q)))\n",
    "\n",
    "def matusita_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Matusita distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\sqrt{\\sum(\\sqrt{P_{i}}-\\sqrt{Q_{i}})^2}\n",
    "    \"\"\"\n",
    "    p = _weight_intensity_by_entropy(p)\n",
    "    q = _weight_intensity_by_entropy(q)\n",
    "    return 1- np.sum(np.power(np.sqrt(p) - np.sqrt(q), 2))\n",
    "\n",
    "def probabilistic_symmetric_chi_squared_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Probabilistic symmetric Ï‡2 distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\frac{1}{2} \\times \\sum\\frac{(P_{i}-Q_{i}\\ )^2}{P_{i}+Q_{i}\\ }\n",
    "    \"\"\"\n",
    "    p = _weight_intensity_by_entropy(p)\n",
    "    q = _weight_intensity_by_entropy(q)\n",
    "    return 1- (1 / 2 * np.sum(np.power(p - q, 2) / (p + q)))\n",
    "\n",
    "def entropy_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Unweighted entropy distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        -\\frac{2\\times S_{PQ}-S_P-S_Q} {ln(4)}, S_I=\\sum_{i} {I_i ln(I_i)}\n",
    "    \"\"\"\n",
    "    p = _weight_intensity_by_entropy(p)\n",
    "    q = _weight_intensity_by_entropy(q)\n",
    "    merged = p + q\n",
    "    entropy_increase = 2 * \\\n",
    "                       scipy.stats.entropy(merged) - scipy.stats.entropy(p) - \\\n",
    "                       scipy.stats.entropy(q)\n",
    "    \n",
    "    return 1 - entropy_increase\n",
    "\n",
    "def dot_product_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Dot product distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        1 - \\sqrt{\\frac{(\\sum{Q_iP_i})^2}{\\sum{Q_i^2\\sum P_i^2}}}\n",
    "    \"\"\"\n",
    "    p = _weight_intensity_by_entropy(p)\n",
    "    q = _weight_intensity_by_entropy(q)    \n",
    "    score = np.power(np.sum(q * p), 2) / (\n",
    "        np.sum(np.power(q, 2)) * np.sum(np.power(p, 2))\n",
    "    )\n",
    "    return np.sqrt(score)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \n",
    "        return 1/(1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Matches DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_dataset = False\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create old similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_output_dir = '/Users/jonahpoczobutt/projects/TunaRes/oldSimRes'\n",
    "if create_new_dataset:\n",
    "\n",
    "     demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/Nist20_inputs/train/chunk_1.pkl')\n",
    "     demo_matches_val = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/Nist20_inputs/val/chunk_1.pkl')\n",
    "\n",
    "     queries = list()\n",
    "     targets = list()\n",
    "     indices = list()\n",
    "     for i in range(len(demo_matches)):\n",
    "\n",
    "          query = demo_matches.iloc[i]['query'][demo_matches.iloc[i]['query'][:,0] < demo_matches.iloc[i]['precquery'] - ppm(demo_matches.iloc[i]['precquery'],3)]\n",
    "          target = demo_matches.iloc[i]['target'][demo_matches.iloc[i]['target'][:,0] < demo_matches.iloc[i]['prectarget'] - ppm(demo_matches.iloc[i]['prectarget'],3)]\n",
    "\n",
    "          if len(query) > 0 and len(target) > 0:\n",
    "               indices.append(i)\n",
    "               queries.append(query)\n",
    "               targets.append(target)\n",
    "\n",
    "     demo_matches = demo_matches.iloc[indices]\n",
    "     demo_matches['query'] = queries\n",
    "     demo_matches['target'] = targets\n",
    "\n",
    "     queries = list()\n",
    "     targets = list()\n",
    "     indices = list()\n",
    "     for i in range(len(demo_matches_val)):\n",
    "\n",
    "          query = demo_matches_val.iloc[i]['query'][demo_matches_val.iloc[i]['query'][:,0] < demo_matches_val.iloc[i]['precquery'] - ppm(demo_matches_val.iloc[i]['precquery'],3)]\n",
    "          target = demo_matches_val.iloc[i]['target'][demo_matches_val.iloc[i]['target'][:,0] < demo_matches_val.iloc[i]['prectarget'] - ppm(demo_matches_val.iloc[i]['prectarget'],3)]\n",
    "\n",
    "          if len(query) > 0 and len(target) > 0:\n",
    "               indices.append(i)\n",
    "               queries.append(query)\n",
    "               targets.append(target)\n",
    "\n",
    "     demo_matches_val = demo_matches_val.iloc[indices]\n",
    "     demo_matches_val['query'] = queries\n",
    "     demo_matches_val['target'] = targets\n",
    "\n",
    "     demo_matches.to_pickle('/Users/jonahpoczobutt/projects/TunaRes/inputs/demo_matches_no_prec.pkl')\n",
    "     demo_matches_val.to_pickle('/Users/jonahpoczobutt/projects/TunaRes/inputs/demo_matches_val_no_prec.pkl')\n",
    "\n",
    "     sim_names = ['prob','matusita','entropy','dot','lorentzian','harmonic']\n",
    "     distances = [probabilistic_symmetric_chi_squared_distance,\n",
    "               matusita_distance,\n",
    "               entropy_distance,\n",
    "               dot_product_distance,\n",
    "               lorentzian_distance,\n",
    "               harmonic_mean_distance]\n",
    "\n",
    "     for _ in range(len(sim_names)):\n",
    "\n",
    "          matched_scores_val = list()\n",
    "          for i in range(len(demo_matches_val)):\n",
    "          \n",
    "               matched = tools_fast.match_spectrum(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], ms2_da = 0.05)\n",
    "               matched_scores_val.append(sigmoid(distances[_](matched[:,1]/sum(matched[:,1]), matched[:,2]/sum(matched[:,2]))))\n",
    "\n",
    "          np.save(f'{sims_output_dir}/val_{sim_names[_]}.npy', np.array(matched_scores_val))\n",
    "\n",
    "          matched_scores = list()\n",
    "          for i in range(len(demo_matches)):\n",
    "\n",
    "               matched = tools_fast.match_spectrum(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'], ms2_da = 0.05)\n",
    "               matched_scores.append(sigmoid(distances[_](matched[:,1]/sum(matched[:,1]), matched[:,2]/sum(matched[:,2]))))\n",
    "\n",
    "          np.save(f'{sims_output_dir}/train_{sim_names[_]}.npy', np.array(matched_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "init_vals = {\n",
    "    'mult_a' : 0.001,\n",
    "    'mult_b': 1,\n",
    "    'dif_a': 0.001,\n",
    "    'dif_b':1,\n",
    "    'add_norm_b' : 1,\n",
    "    'target_intensity_a': 0.1,\n",
    "    'query_intensity_a': 0.1,\n",
    "    'target_intensity_b': 0.1,\n",
    "    'query_intensity_b': 0.1,\n",
    "    }\n",
    "\n",
    "init_vals_2 = {\n",
    "    'mult_a' : 0.001,\n",
    "    'mult_b': 1,\n",
    "    'dif_a': 0.001,\n",
    "    'dif_b':1,\n",
    "    'add_norm_a': 1,\n",
    "    'add_norm_b' : 1,\n",
    "    'target_intensity_a': 0.1,\n",
    "    'query_intensity_a': 0.1,\n",
    "    'target_intensity_b': 0.1,\n",
    "    'query_intensity_b': 0.1,\n",
    "    }\n",
    "\n",
    "init_vals_3 = {\n",
    "    'mult_a' : 0.001,\n",
    "    'mult_b': 1,\n",
    "    'dif_a': 0.001,\n",
    "    'dif_b':1,\n",
    "    'add_norm_int': 0,\n",
    "    'add_norm_a': 1,\n",
    "    'add_norm_b' : 1,\n",
    "    'target_intensity_a': 0.1,\n",
    "    'query_intensity_a': 0.1,\n",
    "    'target_intensity_b': 0.1,\n",
    "    'query_intensity_b': 0.1,\n",
    "    }\n",
    "\n",
    "regularization_grad = lambda x: 0.\n",
    "\n",
    "fixed_vals = {'sigmoid_score' : True, \n",
    "              'weight_combine': 'multiply'\n",
    "    }\n",
    "\n",
    "fixed_vals = {}\n",
    "\n",
    "bounds = {'add_norm_b': (0, 2),\n",
    "          'mult_add_norm_b': (0, 2),\n",
    "          'dif_add_norm_b': (0, 2),\n",
    "          'mult_b': (1e-10, 2),\n",
    "          'add_norm_a': (1e-10, 3),\n",
    "          'dif_b': (1e-10, 2),\n",
    "          'dif_a':(-3,3),\n",
    "          'mult_a': (-3,3),\n",
    "          'add_norm_int': (0, 3),\n",
    "          'target_normalized_intensity_int': (-0.2,1),\n",
    "          'query_normalized_intensity_int': (-0.2,1),\n",
    "          'target_normalized_intensity_a': (1e-10,2),\n",
    "          'query_normalized_intensity_a': (1e-10,2),\n",
    "          'target_normalized_intensity_b': (0,2),\n",
    "          'query_normalized_intensity_b': (0,2),\n",
    "          'target_normalized_intensity_c': (-2,2),\n",
    "          'query_normalized_intensity_c': (-2,2),\n",
    "          'target_mz_b': (-2,2),\n",
    "          'query_mz_b': (-2,2),\n",
    "          'target_mz_a': (-2,2),\n",
    "          'query_mz_a': (-2,2),\n",
    "          'target_mz_int': (-0.2,1),\n",
    "          'query_mz_int': (-0.2,1),\n",
    "          'target_mz_c': (-2,2),\n",
    "          'query_mz_c': (-2,2),\n",
    "           'target_intensity_int': (-0.2,1),\n",
    "           'query_intensity_int': (-0.2,1),\n",
    "          'target_intensity_a': (1e-10,2),\n",
    "          'query_intensity_a': (1e-10,2),\n",
    "          'target_intensity_b': (1e-10,2),\n",
    "          'query_intensity_b': (1e-10,2),\n",
    "          'target_intensity_c': (1e-10,2),\n",
    "          'query_intensity_c': (1e-10,2),\n",
    "          }\n",
    "\n",
    "\n",
    "init_names = ['b', 'ab', 'abint']\n",
    "inits = [init_vals, init_vals_2, init_vals_3]\n",
    "ad_params = [(0.98,0.025)]\n",
    "func_obs = list()\n",
    "\n",
    "for i in range(1):\n",
    "    for momentum in [None]:\n",
    "        for sched in [None]:\n",
    "            for i in range(len(inits)):\n",
    "                for ad_param in ad_params:\n",
    "                \n",
    "                    # func_obs.append(func_ob(f'{momentum}_{sched}_{init_names[i]}_{ad_param}',\n",
    "                    #             sim_func = TunaSims.ExpandedTuna,\n",
    "                    #             init_vals = inits[i].copy(),\n",
    "                    #             fixed_vals = fixed_vals,\n",
    "                    #             regularization_grad = regularization_grad,\n",
    "                    #             bounds = bounds,\n",
    "                    #             max_iter = 1e6,\n",
    "                    #             learning_rates = 0.001,\n",
    "                    #             momentum_type = momentum,\n",
    "                    #             learning_rate_scheduler = sched,\n",
    "                    #             learning_beta = 0.5,\n",
    "                    #             momentum_beta = 0.3,\n",
    "                    #             tol = 0,\n",
    "                    #             balance_classes = True,\n",
    "                    #             groupby_column = 'queryID_target_base',\n",
    "                    #             ad_int = ad_param[0],\n",
    "                    #             ad_slope= ad_param[1]))\n",
    "                    \n",
    "                    func_obs.append(specSimTrainer(f'{momentum}_{sched}_{init_names[i]}_{ad_param}',\n",
    "                                init_vals = inits[i].copy(),\n",
    "                                fixed_vals = fixed_vals,\n",
    "                                bounds = bounds,\n",
    "                                max_iter = 1e6,\n",
    "                                learning_rates = 0.001,\n",
    "                                learning_rate_scheduler = sched,\n",
    "                                learning_beta = 0.5,\n",
    "                                balance_column= 'score',\n",
    "                                groupby_column = 'queryID_target_base',\n",
    "                                ad_int = ad_param[0],\n",
    "                                ad_slope= ad_param[1]))\n",
    "                \n",
    "print(len(func_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m model_\u001b[38;5;241m.\u001b[39mmax_iter \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m     44\u001b[0m keys \u001b[38;5;241m=\u001b[39m model_\u001b[38;5;241m.\u001b[39msim_func\u001b[38;5;241m.\u001b[39mgrad_names\n\u001b[0;32m---> 45\u001b[0m inits \u001b[38;5;241m=\u001b[39m {keys[i]: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(model_\u001b[38;5;241m.\u001b[39mbounds[i][\u001b[38;5;241m0\u001b[39m], model_\u001b[38;5;241m.\u001b[39mbounds[i][\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m keys}\n\u001b[1;32m     46\u001b[0m model_\u001b[38;5;241m.\u001b[39msim_func \u001b[38;5;241m=\u001b[39m model_\u001b[38;5;241m.\u001b[39msim_func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minits)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 45\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m model_\u001b[38;5;241m.\u001b[39mmax_iter \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m     44\u001b[0m keys \u001b[38;5;241m=\u001b[39m model_\u001b[38;5;241m.\u001b[39msim_func\u001b[38;5;241m.\u001b[39mgrad_names\n\u001b[0;32m---> 45\u001b[0m inits \u001b[38;5;241m=\u001b[39m {keys[i]: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(model_\u001b[38;5;241m.\u001b[39mbounds[i][\u001b[38;5;241m0\u001b[39m], model_\u001b[38;5;241m.\u001b[39mbounds[i][\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m keys}\n\u001b[1;32m     46\u001b[0m model_\u001b[38;5;241m.\u001b[39msim_func \u001b[38;5;241m=\u001b[39m model_\u001b[38;5;241m.\u001b[39msim_func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minits)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/inputs/demo_matches_no_prec.pkl')\n",
    "demo_matches_val = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/inputs/demo_matches_val_no_prec.pkl')\n",
    "#demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/inputs/demo_lite.pkl')\n",
    "\n",
    "demo_matches['score'] = 1 * demo_matches['InchiCoreMatch']\n",
    "demo_matches['queryID_target_base'] = demo_matches['queryID'].astype(str) + '_' + demo_matches['target_base'].astype(str)\n",
    "demo_matches_val['queryID_target_base'] = demo_matches_val['queryID'].astype(str) + '_' + demo_matches_val['target_base'].astype(str)\n",
    "\n",
    "demo_matches_val['score'] = 1 * demo_matches_val['InchiCoreMatch']\n",
    "\n",
    "train_auc_top = {i.name: list() for i in func_obs}\n",
    "val_auc_top = {i.name: list() for i in func_obs}\n",
    "\n",
    "train_auc_all = {i.name: list() for i in func_obs}\n",
    "val_auc_all = {i.name: list() for i in func_obs}\n",
    "\n",
    "train_times = {i.name: list() for i in func_obs}\n",
    "\n",
    "absolutes = [0, 3e5]\n",
    "offsets = [absolutes[i+1] - absolutes[i] for i in range(len(absolutes)-1)]\n",
    "\n",
    "reps = 5\n",
    "\n",
    "trained_obs = []\n",
    "\n",
    "for model in func_obs:\n",
    "\n",
    "    for _ in range(reps):\n",
    "\n",
    "        model_ = copy.deepcopy(model)\n",
    "\n",
    "        accumulated = 0\n",
    "        accumulated_time = 0\n",
    "        train_aucs_top = list()\n",
    "        val_aucs_top = list()\n",
    "        train_aucs_all = list()\n",
    "        val_aucs_all = list()\n",
    "        trained_obs_sub = list()\n",
    "\n",
    "        for i in offsets:\n",
    "            \n",
    "            model_.max_iter = i\n",
    "\n",
    "            keys = model_.sim_func.grad_names\n",
    "            inits = {keys[i]: np.random.uniform(model_.bounds[i][0], model_.bounds[i][1]) for i in range(len(keys))}\n",
    "            model_.sim_func = model_.sim_func(**inits)\n",
    "            \n",
    "            print('start')\n",
    "            start = time.time()\n",
    "            model_.fit(demo_matches)\n",
    "            accumulated_time += time.time() - start\n",
    "\n",
    "            print(f'done training: {round((accumulated_time)/60, 4)}')\n",
    "\n",
    "            demo_matches['preds'] = model_.sim_func.predict_for_dataset(demo_matches)\n",
    "            demo_matches_val['preds'] = model_.sim_func.predict_for_dataset(demo_matches_val)\n",
    "\n",
    "            train_aucs_all.append(round(roc_auc_score(demo_matches['score'] , demo_matches['preds']), 4)) \n",
    "            val_aucs_all.append(round(roc_auc_score(demo_matches_val['score'] , demo_matches_val['preds']),4))\n",
    "\n",
    "\n",
    "            temp = demo_matches[['queryID_target_base','preds','score']].groupby(by=['queryID_target_base']).max()\n",
    "            temp_val = demo_matches_val[['queryID_target_base','preds','score']].groupby(by=['queryID_target_base']).max()\n",
    "\n",
    "            train_aucs_top.append(round(roc_auc_score(temp['score'] , temp['preds']), 4)) \n",
    "            val_aucs_top.append(round(roc_auc_score(temp_val['score'] , temp_val['preds']),4))\n",
    "\n",
    "            accumulated += model_.max_iter\n",
    "\n",
    "        trained_obs_sub.append(copy.deepcopy(model_))\n",
    "        \n",
    "    trained_obs.append(trained_obs_sub)\n",
    "    train_times[model.name].append(round(accumulated_time/60, 4))\n",
    "    train_auc_all[model.name].append(train_aucs_all)\n",
    "    train_auc_top[model.name].append(train_aucs_top)\n",
    "    val_auc_all[model.name].append(val_aucs_all)\n",
    "    val_auc_top[model.name].append(val_aucs_top)\n",
    "\n",
    "    print(model.name)\n",
    "\n",
    "    model_1 = model_\n",
    "\n",
    "train_auc_top, val_auc_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dif_a',\n",
       " 'dif_b',\n",
       " 'mult_a',\n",
       " 'mult_b',\n",
       " 'add_norm_int',\n",
       " 'add_norm_a',\n",
       " 'add_norm_b',\n",
       " 'query_intensity_a',\n",
       " 'query_intensity_b',\n",
       " 'target_intensity_a',\n",
       " 'target_intensity_b']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_.sim_func.grad_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/jonahpoczobutt/projects/TunaRes/pickled_models/model_1.pickle', 'rb') as handle:\n",
    "    model = pickle.load(handle)\n",
    "\n",
    "\n",
    "for i in model_1.init_vals:\n",
    "\n",
    "    print(i, getattr(model_1.sim_func,i), getattr(model,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/jonahpoczobutt/projects/TunaRes/pickled_models/model_1.pickle', 'rb') as handle:\n",
    "    model_ = pickle.load(handle)\n",
    "\n",
    "model_.query_mz = False\n",
    "model_.target_mz = False\n",
    "model_.query_mz_offset = False\n",
    "model_.target_mz_offset = False\n",
    "model_.query_intensity = False\n",
    "model_.target_intensity = False\n",
    "model_.query_normalized_intensity = False\n",
    "model_.target_normalized_intensity = False\n",
    "\n",
    "demo_matches['preds'] = [model_.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'], demo_matches.iloc[i]['precquery'], demo_matches.iloc[i]['prectarget'], grads = False) for i in range(len(demo_matches))]\n",
    "#demo_matches_val['preds'] = [model_.predict(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], demo_matches_val.iloc[i]['precquery'], demo_matches_val.iloc[i]['prectarget'],grads = False) for i in range(len(demo_matches_val))]\n",
    "\n",
    "# train_aucs_all.append(round(roc_auc_score(demo_matches['score'] , demo_matches['preds']), 4)) \n",
    "# val_aucs_all.append(round(roc_auc_score(demo_matches_val['score'] , demo_matches_val['preds']),4))\n",
    "\n",
    "temp = demo_matches.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "#temp_val = demo_matches_val.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "\n",
    "train_aucs_top.append(round(roc_auc_score(temp['score'] , temp['preds']), 4)) \n",
    "#val_aucs_top.append(round(roc_auc_score(temp_val['score'] , temp_val['preds']),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TunaSims\n",
    "import TunaSimsOld\n",
    "import pandas as pd\n",
    "\n",
    "demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/inputs/demo_matches_no_prec.pkl')\n",
    "\n",
    "new = TunaSims.speedyTuna(query_intensity_a = 1,\n",
    "                          query_intensity_b = 1,\n",
    "                          target_intensity_a = 1,\n",
    "                          target_intensity_b = 1,\n",
    "                          mult_a = 0.001,\n",
    "                          mult_b = 1,\n",
    "                          dif_a= 0.001,\n",
    "                          dif_b = 1,\n",
    "                          add_norm_a= 1,\n",
    "                          add_norm_b= 1)\n",
    "\n",
    "old = TunaSimsOld.ExpandedTuna(query_normalized_intensity_a = 1,\n",
    "                          query_normalized_intensity_b = 1,\n",
    "                          target_normalized_intensity_a = 1,\n",
    "                          target_normalized_intensity_b = 1,\n",
    "                          mult_a = 0.001,\n",
    "                          mult_b = 1,\n",
    "                          dif_a= 0.001,\n",
    "                          dif_b = 1,\n",
    "                          add_norm_a= 1,\n",
    "                          add_norm_b= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(999,1001):\n",
    "    new.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'], grads = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    new.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'], grads = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(999,1001):\n",
    "    old.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'], demo_matches.iloc[i]['precquery'], demo_matches.iloc[i]['prectarget'], grads = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    old.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'], demo_matches.iloc[i]['precquery'], demo_matches.iloc[i]['prectarget'], grads = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = pd.DataFrame([1 for i in range(1000)] + [5 for i in range(100)] + [3 for i in range(50000)], columns = ['yoop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy.sort_values(by='yoop', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [i for i in range(1000)]\n",
    "for i in range(10000):\n",
    "    \n",
    "    np.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model_.init_vals:\n",
    "\n",
    "    print(i, getattr(model_.sim_func, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/jonahpoczobutt/projects/TunaRes/pickled_models/model_1.pickle', 'rb') as handle:\n",
    "\n",
    "    model_ = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model_.init_vals:\n",
    "\n",
    "    print(i, getattr(model, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_.sim_func.nonzero_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_.n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_.sim_func.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model_.sim_func.grads1.keys():\n",
    "    print(i, getattr(model_.sim_func,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TunaSims\n",
    "\n",
    "mer = TunaSims.ScoreByQuery(raw_scores_int = 0,\n",
    "                            raw_scores_a = 1,\n",
    "                            raw_scores_b = 1,\n",
    "                            dif_from_top_int = 0,\n",
    "                            dif_from_top_a = -1,\n",
    "                            dif_from_top_b = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funcOb\n",
    "import TunaSims\n",
    "\n",
    "inits = {'raw_scores_int' :0,\n",
    "        'raw_scores_a' : 1,\n",
    "        'raw_scores_b' : 1,\n",
    "        'dif_from_top_int' : 0,\n",
    "        'dif_from_top_a' : -1,\n",
    "        'dif_from_top_b' : 1}\n",
    "\n",
    "fixed_vals = {}\n",
    "\n",
    "a = funcOb.scoreByQueryFunc(name = 'testy',\n",
    "                            init_vals = inits,\n",
    "                            fixed_vals = fixed_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.dot(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([None, 1])\n",
    "(a == None).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot([1,2], [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mer.predict(scores = [0.9, 0.8, 0.5], match_names = ['a', 'b', 'c'], grads = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to add the object attributes back so that we ca properly adjust gradients...for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.sum([[1,2,3], [1,2,3]], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0.6,1,0,1])\n",
    "sort_order = np.argsort(-a)\n",
    "mask = (sort_order == 0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import prod\n",
    "\n",
    "np.sum([np.array([1,2,3]),np.array([4,5,6])], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[sort_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "np.prod(np.vstack((a[sort_order], mask,a)),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[a==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(temp['preds'], bins = 100)\n",
    "plt.title('Preds Train')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(temp_val['preds'], bins = 100)\n",
    "plt.title('Preds Val')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues = np.array([0,1,0,0,0,0])\n",
    "\n",
    "\n",
    "#np.sum(np.trues - preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('function of remaining scores')\n",
    "print(np.concatenate((preds,[0])))\n",
    "print(np.max(preds) - np.concatenate((preds,[0])))\n",
    "\n",
    "print('then move through in reverse for scores above')\n",
    "print(np.concatenate((preds[::-1],[0])))\n",
    "print(np.concatenate((preds[::-1],[0])) - np.min(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array([0.45,0.4,0.3])\n",
    "padded = np.concatenate(([1-preds[0]], preds))\n",
    "max_dif = (np.max(padded) - padded)\n",
    "prob_above = np.array([sum(padded[:i]) for i in range(len(padded))])\n",
    "print(padded)\n",
    "print(max_dif)\n",
    "print(prob_above)\n",
    "\n",
    "\n",
    "#should also have function of other scores summed after a non-linear transformation\n",
    "#array[not]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(['a',None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_none_prob(max_prob):\n",
    "    \"\"\" \n",
    "    probs must already be sorted from max to min and have\n",
    "    candidate names in corresponding order\n",
    "    \"\"\"\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab only inchicores where performance was bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['residual'] = np.abs(temp['score'] - temp['preds'])\n",
    "median_residual = np.median(temp['residual'])\n",
    "print(len(temp[temp['score'] == 1])/len(temp))\n",
    "\n",
    "pos = 0\n",
    "neg = 0\n",
    "bad_ids = list()\n",
    "for i in range(len(temp)):\n",
    "\n",
    "    if temp.iloc[i]['residual'] >= median_residual:\n",
    "        bad_ids.append(temp.iloc[i]['queryID_target_base'])\n",
    "\n",
    "        if temp.iloc[i]['score'] == 1:\n",
    "            pos+=1\n",
    "\n",
    "        else:\n",
    "            neg+=1\n",
    "\n",
    "bad_ids = set(bad_ids)\n",
    "\n",
    "residual_inds = list()\n",
    "\n",
    "for i in range(len(demo_matches)):\n",
    "\n",
    "    if demo_matches.iloc[i]['queryID_target_base'] in bad_ids:\n",
    "\n",
    "        residual_inds.append(i)\n",
    "\n",
    "demo_matches = demo_matches.iloc[residual_inds]\n",
    "print(len(demo_matches))\n",
    "print(len(bad_ids))\n",
    "print(pos / (pos + neg))\n",
    "\n",
    "train_auc_top = {i.name: list() for i in func_obs}\n",
    "val_auc_top = {i.name: list() for i in func_obs}\n",
    "\n",
    "train_auc_all = {i.name: list() for i in func_obs}\n",
    "val_auc_all = {i.name: list() for i in func_obs}\n",
    "\n",
    "train_times = {i.name: list() for i in func_obs}\n",
    "\n",
    "absolutes = [0,1e5]\n",
    "offsets = [absolutes[i+1] - absolutes[i] for i in range(len(absolutes)-1)]\n",
    "\n",
    "reps = 1\n",
    "\n",
    "trained_obs = []\n",
    "\n",
    "for model in func_obs:\n",
    "\n",
    "    for _ in range(reps):\n",
    "\n",
    "        model_ = copy.deepcopy(model)\n",
    "\n",
    "        accumulated = 0\n",
    "        accumulated_time = 0\n",
    "        train_aucs_top = list()\n",
    "        val_aucs_top = list()\n",
    "        train_aucs_all = list()\n",
    "        val_aucs_all = list()\n",
    "        trained_obs_sub = list()\n",
    "\n",
    "        for i in offsets:\n",
    "            \n",
    "            model_.max_iter = i\n",
    "            \n",
    "            start = time.time()\n",
    "            model_.fit(demo_matches)\n",
    "            accumulated_time += time.time() - start\n",
    "\n",
    "            demo_matches['preds'] = [model_.sim_func.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'],demo_matches.iloc[i]['precquery'], demo_matches.iloc[i]['prectarget'], grads = False) for i in range(len(demo_matches))]\n",
    "            demo_matches_val['preds'] = [model_.sim_func.predict(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], demo_matches_val.iloc[i]['precquery'], demo_matches_val.iloc[i]['prectarget'], grads = False) for i in range(len(demo_matches_val))]\n",
    "\n",
    "            train_aucs_all.append(round(roc_auc_score(demo_matches['score'] , demo_matches['preds']), 4)) \n",
    "            val_aucs_all.append(round(roc_auc_score(demo_matches_val['score'] , demo_matches_val['preds']),4))\n",
    "\n",
    "            temp = demo_matches.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "            temp_val = demo_matches_val.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "\n",
    "            train_aucs_top.append(round(roc_auc_score(temp['score'] , temp['preds']), 4)) \n",
    "            val_aucs_top.append(round(roc_auc_score(temp_val['score'] , temp_val['preds']),4))\n",
    "\n",
    "            accumulated += model_.max_iter\n",
    "\n",
    "        trained_obs_sub.append(copy.deepcopy(model_))\n",
    "        \n",
    "    trained_obs.append(trained_obs_sub)\n",
    "    train_times[model.name].append(round(accumulated_time/60, 4))\n",
    "    train_auc_all[model.name].append(train_aucs_all)\n",
    "    train_auc_top[model.name].append(train_aucs_top)\n",
    "    val_auc_all[model.name].append(val_aucs_all)\n",
    "    val_auc_top[model.name].append(val_aucs_top)\n",
    "\n",
    "    print(model.name)\n",
    "\n",
    "    model_2 = model_\n",
    "\n",
    "train_auc_top, val_auc_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['residual'] = np.abs(temp['score'] - temp['preds'])\n",
    "median_residual = np.median(temp['residual'])\n",
    "print(len(temp[temp['score'] == 1])/len(temp))\n",
    "print(median_residual)\n",
    "\n",
    "pos = 0\n",
    "neg = 0\n",
    "bad_ids = list()\n",
    "for i in range(len(temp)):\n",
    "\n",
    "    if temp.iloc[i]['residual'] >= median_residual:\n",
    "        bad_ids.append(temp.iloc[i]['queryID_target_base'])\n",
    "\n",
    "        if temp.iloc[i]['score'] == 1:\n",
    "            pos+=1\n",
    "\n",
    "        else:\n",
    "            neg+=1\n",
    "\n",
    "bad_ids = set(bad_ids)\n",
    "\n",
    "residual_inds = list()\n",
    "\n",
    "for i in range(len(demo_matches)):\n",
    "\n",
    "    if demo_matches.iloc[i]['queryID_target_base'] in bad_ids:\n",
    "\n",
    "        residual_inds.append(i)\n",
    "\n",
    "demo_matches = demo_matches.iloc[residual_inds]\n",
    "print(len(demo_matches))\n",
    "print(len(bad_ids))\n",
    "print(pos / (pos + neg))\n",
    "\n",
    "train_auc_top = {i.name: list() for i in func_obs}\n",
    "val_auc_top = {i.name: list() for i in func_obs}\n",
    "\n",
    "train_auc_all = {i.name: list() for i in func_obs}\n",
    "val_auc_all = {i.name: list() for i in func_obs}\n",
    "\n",
    "train_times = {i.name: list() for i in func_obs}\n",
    "\n",
    "absolutes = [0,1e5]\n",
    "offsets = [absolutes[i+1] - absolutes[i] for i in range(len(absolutes)-1)]\n",
    "\n",
    "reps = 1\n",
    "\n",
    "trained_obs = []\n",
    "\n",
    "for model in func_obs:\n",
    "\n",
    "    for _ in range(reps):\n",
    "\n",
    "        model_ = copy.deepcopy(model)\n",
    "\n",
    "        accumulated = 0\n",
    "        accumulated_time = 0\n",
    "        train_aucs_top = list()\n",
    "        val_aucs_top = list()\n",
    "        train_aucs_all = list()\n",
    "        val_aucs_all = list()\n",
    "        trained_obs_sub = list()\n",
    "\n",
    "        for i in offsets:\n",
    "            \n",
    "            model_.max_iter = i\n",
    "            \n",
    "            start = time.time()\n",
    "            model_.fit(demo_matches)\n",
    "            accumulated_time += time.time() - start\n",
    "\n",
    "            demo_matches['preds'] = [model_.sim_func.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'],demo_matches.iloc[i]['precquery'], demo_matches.iloc[i]['prectarget'], grads = False) for i in range(len(demo_matches))]\n",
    "            demo_matches_val['preds'] = [model_.sim_func.predict(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], demo_matches_val.iloc[i]['precquery'], demo_matches_val.iloc[i]['prectarget'], grads = False) for i in range(len(demo_matches_val))]\n",
    "\n",
    "            train_aucs_all.append(round(roc_auc_score(demo_matches['score'] , demo_matches['preds']), 4)) \n",
    "            val_aucs_all.append(round(roc_auc_score(demo_matches_val['score'] , demo_matches_val['preds']),4))\n",
    "\n",
    "            temp = demo_matches.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "            temp_val = demo_matches_val.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "\n",
    "            train_aucs_top.append(round(roc_auc_score(temp['score'] , temp['preds']), 4)) \n",
    "            val_aucs_top.append(round(roc_auc_score(temp_val['score'] , temp_val['preds']),4))\n",
    "\n",
    "            accumulated += model_.max_iter\n",
    "\n",
    "        trained_obs_sub.append(copy.deepcopy(model_))\n",
    "        \n",
    "    trained_obs.append(trained_obs_sub)\n",
    "    train_times[model.name].append(round(accumulated_time/60, 4))\n",
    "    train_auc_all[model.name].append(train_aucs_all)\n",
    "    train_auc_top[model.name].append(train_aucs_top)\n",
    "    val_auc_all[model.name].append(val_aucs_all)\n",
    "    val_auc_top[model.name].append(val_aucs_top)\n",
    "\n",
    "    print(model.name)\n",
    "\n",
    "    model_3 = model_\n",
    "\n",
    "train_auc_top, val_auc_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['residual'] = np.abs(temp['score'] - temp['preds'])\n",
    "median_residual = np.median(temp['residual'])\n",
    "print(len(temp[temp['score'] == 1])/len(temp))\n",
    "print(median_residual)\n",
    "\n",
    "pos = 0\n",
    "neg = 0\n",
    "bad_ids = list()\n",
    "for i in range(len(temp)):\n",
    "\n",
    "    if temp.iloc[i]['residual'] >= median_residual:\n",
    "        bad_ids.append(temp.iloc[i]['queryID_target_base'])\n",
    "\n",
    "        if temp.iloc[i]['score'] == 1:\n",
    "            pos+=1\n",
    "\n",
    "        else:\n",
    "            neg+=1\n",
    "\n",
    "bad_ids = set(bad_ids)\n",
    "\n",
    "residual_inds = list()\n",
    "\n",
    "for i in range(len(demo_matches)):\n",
    "\n",
    "    if demo_matches.iloc[i]['queryID_target_base'] in bad_ids:\n",
    "\n",
    "        residual_inds.append(i)\n",
    "\n",
    "demo_matches = demo_matches.iloc[residual_inds]\n",
    "print(len(demo_matches))\n",
    "print(len(bad_ids))\n",
    "print(pos / (pos + neg))\n",
    "\n",
    "train_auc_top = {i.name: list() for i in func_obs}\n",
    "val_auc_top = {i.name: list() for i in func_obs}\n",
    "\n",
    "train_auc_all = {i.name: list() for i in func_obs}\n",
    "val_auc_all = {i.name: list() for i in func_obs}\n",
    "\n",
    "train_times = {i.name: list() for i in func_obs}\n",
    "\n",
    "absolutes = [0,1e5]\n",
    "offsets = [absolutes[i+1] - absolutes[i] for i in range(len(absolutes)-1)]\n",
    "\n",
    "reps = 1\n",
    "\n",
    "trained_obs = []\n",
    "\n",
    "for model in func_obs:\n",
    "\n",
    "    for _ in range(reps):\n",
    "\n",
    "        model_ = copy.deepcopy(model)\n",
    "\n",
    "        accumulated = 0\n",
    "        accumulated_time = 0\n",
    "        train_aucs_top = list()\n",
    "        val_aucs_top = list()\n",
    "        train_aucs_all = list()\n",
    "        val_aucs_all = list()\n",
    "        trained_obs_sub = list()\n",
    "\n",
    "        for i in offsets:\n",
    "            \n",
    "            model_.max_iter = i\n",
    "            \n",
    "            start = time.time()\n",
    "            model_.fit(demo_matches)\n",
    "            accumulated_time += time.time() - start\n",
    "\n",
    "            demo_matches['preds'] = [model_.sim_func.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'],demo_matches.iloc[i]['precquery'], demo_matches.iloc[i]['prectarget'], grads = False) for i in range(len(demo_matches))]\n",
    "            demo_matches_val['preds'] = [model_.sim_func.predict(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], demo_matches_val.iloc[i]['precquery'], demo_matches_val.iloc[i]['prectarget'], grads = False) for i in range(len(demo_matches_val))]\n",
    "\n",
    "            train_aucs_all.append(round(roc_auc_score(demo_matches['score'] , demo_matches['preds']), 4)) \n",
    "            val_aucs_all.append(round(roc_auc_score(demo_matches_val['score'] , demo_matches_val['preds']),4))\n",
    "\n",
    "            temp = demo_matches.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "            temp_val = demo_matches_val.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "\n",
    "            train_aucs_top.append(round(roc_auc_score(temp['score'] , temp['preds']), 4)) \n",
    "            val_aucs_top.append(round(roc_auc_score(temp_val['score'] , temp_val['preds']),4))\n",
    "\n",
    "            accumulated += model_.max_iter\n",
    "\n",
    "        trained_obs_sub.append(copy.deepcopy(model_))\n",
    "        \n",
    "    trained_obs.append(trained_obs_sub)\n",
    "    train_times[model.name].append(round(accumulated_time/60, 4))\n",
    "    train_auc_all[model.name].append(train_aucs_all)\n",
    "    train_auc_top[model.name].append(train_aucs_top)\n",
    "    val_auc_all[model.name].append(val_aucs_all)\n",
    "    val_auc_top[model.name].append(val_aucs_top)\n",
    "\n",
    "    print(model.name)\n",
    "\n",
    "    model_4 = model_\n",
    "\n",
    "train_auc_top, val_auc_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [model_1, model_2, model_3, model_4]:\n",
    "\n",
    "    print('\\n')\n",
    "    for i in init_vals:\n",
    "        print(i, round(getattr(model.sim_func, i),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/inputs/demo_matches_no_prec.pkl')\n",
    "\n",
    "all_scores_train = dict()\n",
    "all_scores_val = dict()\n",
    "\n",
    "models = [model_1, model_2, model_3, model_4]\n",
    "mod_names = ['model_1', 'model_2', 'model_3', 'model_4']\n",
    "\n",
    "for _ in range(len(models)):\n",
    "\n",
    "    print(mod_names[_])\n",
    "\n",
    "    all_scores_train[mod_names[_]] = [models[_].sim_func.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'],demo_matches.iloc[i]['precquery'], demo_matches.iloc[i]['prectarget'], grads = False) for i in range(len(demo_matches))]\n",
    "    \n",
    "all_scores_train['queryID'] = demo_matches['queryID'].tolist()\n",
    "all_scores_train['target_base'] = demo_matches['target_base'].tolist()\n",
    "all_scores_train['score'] = demo_matches['score'].tolist()\n",
    "\n",
    "del(demo_matches)\n",
    "\n",
    "demo_matches_val = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/inputs/demo_matches_val_no_prec.pkl')\n",
    "\n",
    "for _ in range(len(models)):\n",
    "\n",
    "    all_scores_val[mod_names[_]] = [models[_].sim_func.predict(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], demo_matches_val.iloc[i]['precquery'], demo_matches_val.iloc[i]['prectarget'], grads = False) for i in range(len(demo_matches_val))]\n",
    "    print(mod_names[_])\n",
    "\n",
    "all_scores_val['queryID'] = demo_matches_val['queryID'].tolist()\n",
    "all_scores_val['target_base'] = demo_matches_val['target_base'].tolist()\n",
    "all_scores_val['score'] = demo_matches_val['score'].tolist()\n",
    "del(demo_matches_val)\n",
    "\n",
    "for sim in sim_names:\n",
    "\n",
    "    print(sim)\n",
    "\n",
    "    all_scores_train[sim] = np.load(f'{sims_output_dir}/train_{sim}.npy')\n",
    "    all_scores_val[sim] = np.load(f'{sims_output_dir}/val_{sim}.npy')\n",
    "\n",
    "all_scores_train = pd.DataFrame(all_scores_train)\n",
    "all_scores_val = pd.DataFrame(all_scores_val)\n",
    "\n",
    "all_scores_train.to_pickle('/Users/jonahpoczobutt/projects/TunaRes/sim_scores/train.pickle')\n",
    "all_scores_val.to_pickle('/Users/jonahpoczobutt/projects/TunaRes/sim_scores/val.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/sim_scores/train.pickle')\n",
    "val_data = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/sim_scores/val.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_scores_train = train_data.groupby(['queryID', 'target_base']).max()\n",
    "max_scores_val = val_data.groupby(['queryID', 'target_base']).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in max_scores_train.columns[:-1]:\n",
    "\n",
    "    print(f\"{col}: train: {round(roc_auc_score(max_scores_train['score'], max_scores_train[col]),4)} val: {round(roc_auc_score(max_scores_val['score'], max_scores_val[col]),4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.iloc[:,:-3].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Train Models with Each Pair/Triplet of Sim Scores Old and New"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create column groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_names = ['prob', 'matusita', 'entropy', 'dot', 'lorentzian', 'harmonic']\n",
    "\n",
    "old_sim_combos = list()\n",
    "for n in range(1,7):\n",
    "\n",
    "    for comb in combinations(sim_names, n):\n",
    "        old_sim_combos.append(list(comb))\n",
    "\n",
    "\n",
    "new_sim_combos = list()\n",
    "new_sims = ['model_1', 'model_2', 'model_3', 'model_4']\n",
    "for n in range(1,5):\n",
    "\n",
    "    for comb in combinations(new_sims, n):\n",
    "        new_sim_combos.append(list(comb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Models for each Column Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_performance_old = dict()\n",
    "\n",
    "trained = 0\n",
    "consolidated = True\n",
    "for combo in old_sim_combos:\n",
    "\n",
    "    model = hgbc()\n",
    "    if consolidated:\n",
    "        train = max_scores_train\n",
    "        val = max_scores_val\n",
    "\n",
    "    else:\n",
    "        train = train_data.copy()\n",
    "        val = val_data.copy()\n",
    "\n",
    "    model.fit(train[combo], train['score'])\n",
    "\n",
    "    if consolidated:\n",
    "\n",
    "        preds = model.predict_proba(train[combo])[:,1]\n",
    "        preds_val = model.predict_proba(val[combo])[:,1]\n",
    "\n",
    "    else:\n",
    "\n",
    "        train['preds'] = model.predict_proba(train[combo])[:,1]\n",
    "        val['preds'] = model.predict_proba(val[combo])[:,1]\n",
    "\n",
    "        train = train.groupby(['queryID', 'target_base']).max()\n",
    "        val = val.groupby(['queryID', 'target_base']).max()\n",
    "\n",
    "        preds = train['preds']\n",
    "        preds_val = val['preds']\n",
    "    \n",
    "    train_auc = roc_auc_score(train['score'], preds)\n",
    "    val_auc = roc_auc_score(val['score'], preds_val)\n",
    "\n",
    "    sim_performance_old['-'.join(combo)] = (train_auc, val_auc)\n",
    "\n",
    "    trained +=1\n",
    "    if trained % 10 == 0:\n",
    "        print(trained)\n",
    "\n",
    "sim_performance_new = dict()\n",
    "trained = 0\n",
    "for combo in new_sim_combos:\n",
    "\n",
    "    model = hgbc()\n",
    "    if consolidated:\n",
    "        train = max_scores_train\n",
    "        val = max_scores_val\n",
    "\n",
    "    else:\n",
    "        train = train_data.copy()\n",
    "        val = val_data.copy()\n",
    "\n",
    "    model.fit(train[combo], train['score'])\n",
    "\n",
    "    if consolidated:\n",
    "        \n",
    "        preds = model.predict_proba(train[combo])[:,1]\n",
    "        preds_val = model.predict_proba(val[combo])[:,1]\n",
    "\n",
    "    else:\n",
    "\n",
    "        train['preds'] = model.predict_proba(train[combo])[:,1]\n",
    "        val['preds'] = model.predict_proba(val[combo])[:,1]\n",
    "\n",
    "        train = train.groupby(['queryID', 'target_base']).max()\n",
    "        val = val.groupby(['queryID', 'target_base']).max()\n",
    "\n",
    "        preds = train['preds']\n",
    "        preds_val = val['preds']\n",
    "    \n",
    "    train_auc = roc_auc_score(train['score'], preds)\n",
    "    val_auc = roc_auc_score(val['score'], preds_val)\n",
    "\n",
    "    sim_performance_new['-'.join(combo)] = (train_auc, val_auc)\n",
    "\n",
    "    trained +=1\n",
    "    if trained % 10 == 0:\n",
    "        print(trained)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([val[0] for key, val in sim_performance_old.items() if len(key.split('-'))==1]), np.max([val[0] for key, val in sim_performance_old.items() if len(key.split('-'))==1]))\n",
    "print(np.mean([val[0] for key, val in sim_performance_old.items() if len(key.split('-'))==2]), np.max([val[0] for key, val in sim_performance_old.items() if len(key.split('-'))==2]))\n",
    "print(np.mean([val[0] for key, val in sim_performance_old.items() if len(key.split('-'))==3]), np.max([val[0] for key, val in sim_performance_old.items() if len(key.split('-'))==3]))\n",
    "print(np.mean([val[0] for key, val in sim_performance_old.items() if len(key.split('-'))==4]), np.max([val[0] for key, val in sim_performance_old.items() if len(key.split('-'))==4]))\n",
    "print(np.mean([val[0] for key, val in sim_performance_old.items() if len(key.split('-'))==5]), np.max([val[0] for key, val in sim_performance_old.items() if len(key.split('-'))==5]))\n",
    "print(np.mean([val[0] for key, val in sim_performance_old.items() if len(key.split('-'))==6]), np.max([val[0] for key, val in sim_performance_old.items() if len(key.split('-'))==6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([val[0] for key, val in sim_performance_new.items() if len(key.split('-'))==1]), np.max([val[0] for key, val in sim_performance_new.items() if len(key.split('-'))==1]))\n",
    "print(np.mean([val[0] for key, val in sim_performance_new.items() if len(key.split('-'))==2]), np.max([val[0] for key, val in sim_performance_new.items() if len(key.split('-'))==2]))\n",
    "print(np.mean([val[0] for key, val in sim_performance_new.items() if len(key.split('-'))==3]), np.max([val[0] for key, val in sim_performance_new.items() if len(key.split('-'))==3]))\n",
    "print(np.mean([val[0] for key, val in sim_performance_new.items() if len(key.split('-'))==4]), np.max([val[0] for key, val in sim_performance_new.items() if len(key.split('-'))==4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Val Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([val[1] for key, val in sim_performance_old.items() if len(key.split('-'))==1]), np.max([val[1] for key, val in sim_performance_old.items() if len(key.split('-'))==1]))\n",
    "print(np.mean([val[1] for key, val in sim_performance_old.items() if len(key.split('-'))==2]), np.max([val[1] for key, val in sim_performance_old.items() if len(key.split('-'))==2]))\n",
    "print(np.mean([val[1] for key, val in sim_performance_old.items() if len(key.split('-'))==3]), np.max([val[1] for key, val in sim_performance_old.items() if len(key.split('-'))==3]))\n",
    "print(np.mean([val[1] for key, val in sim_performance_old.items() if len(key.split('-'))==4]), np.max([val[1] for key, val in sim_performance_old.items() if len(key.split('-'))==4]))\n",
    "print(np.mean([val[1] for key, val in sim_performance_old.items() if len(key.split('-'))==5]), np.max([val[1] for key, val in sim_performance_old.items() if len(key.split('-'))==5]))\n",
    "print(np.mean([val[1] for key, val in sim_performance_old.items() if len(key.split('-'))==6]), np.max([val[1] for key, val in sim_performance_old.items() if len(key.split('-'))==6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([val[1] for key, val in sim_performance_new.items() if len(key.split('-'))==1]), np.max([val[1] for key, val in sim_performance_new.items() if len(key.split('-'))==1]))\n",
    "print(np.mean([val[1] for key, val in sim_performance_new.items() if len(key.split('-'))==2]), np.max([val[1] for key, val in sim_performance_new.items() if len(key.split('-'))==2]))\n",
    "print(np.mean([val[1] for key, val in sim_performance_new.items() if len(key.split('-'))==3]), np.max([val[1] for key, val in sim_performance_new.items() if len(key.split('-'))==3]))\n",
    "print(np.mean([val[1] for key, val in sim_performance_new.items() if len(key.split('-'))==4]), np.max([val[1] for key, val in sim_performance_new.items() if len(key.split('-'))==4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,2,3,4,5]:\n",
    "\n",
    "    performances = [val[1] for key, val in sim_performance_old.items() if len(key.split('-')) == i]\n",
    "    keys = [key for key, val in sim_performance_old.items() if len(key.split('-')) == i]\n",
    "\n",
    "    max_key = keys[np.argmax(performances)]\n",
    "    print(i, max_key, round(sim_performance_old[max_key][0],4), round(sim_performance_old[max_key][1],4))\n",
    "\n",
    "print('\\n')\n",
    "for i in [1,2,3,4]:\n",
    "\n",
    "    performances = [val[1] for key, val in sim_performance_new.items() if len(key.split('-')) == i]\n",
    "    keys = [key for key, val in sim_performance_new.items() if len(key.split('-')) == i]\n",
    "\n",
    "    max_key = keys[np.argmax(performances)]\n",
    "    print(i, max_key, round(sim_performance_new[max_key][0],4), round(sim_performance_new[max_key][1],4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
