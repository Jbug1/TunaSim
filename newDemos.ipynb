{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TunaSims\n",
    "import numpy as np\n",
    "from funcOb import func_ob\n",
    "import pandas as pd\n",
    "import tools_fast\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier as hgbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _weight_intensity_by_entropy(x):\n",
    "    WEIGHT_START = 0.25\n",
    "    ENTROPY_CUTOFF = 3\n",
    "    weight_slope = (1 - WEIGHT_START) / ENTROPY_CUTOFF\n",
    "\n",
    "    if np.sum(x) > 0:\n",
    "        entropy_x = scipy.stats.entropy(x)\n",
    "        if entropy_x < ENTROPY_CUTOFF:\n",
    "            weight = WEIGHT_START + weight_slope * entropy_x\n",
    "            x = np.power(x, weight)\n",
    "            x_sum = np.sum(x)\n",
    "            x = x / x_sum\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_mean_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Harmonic mean distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        1-2\\sum(\\frac{P_{i}Q_{i}}{P_{i}+Q_{i}})\n",
    "    \"\"\"\n",
    "\n",
    "    return 2 * np.sum(p * q / (p + q))\n",
    "\n",
    "def lorentzian_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Lorentzian distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\sum{\\ln(1+|P_i-Q_i|)}\n",
    "    \"\"\"\n",
    "\n",
    "    return 1 - np.sum(np.log(1 + np.abs(p - q)))\n",
    "\n",
    "def matusita_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Matusita distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\sqrt{\\sum(\\sqrt{P_{i}}-\\sqrt{Q_{i}})^2}\n",
    "    \"\"\"\n",
    "\n",
    "    return 1- np.sum(np.power(np.sqrt(p) - np.sqrt(q), 2))\n",
    "\n",
    "def probabilistic_symmetric_chi_squared_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Probabilistic symmetric Ï‡2 distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\frac{1}{2} \\times \\sum\\frac{(P_{i}-Q_{i}\\ )^2}{P_{i}+Q_{i}\\ }\n",
    "    \"\"\"\n",
    "\n",
    "    return 1- (1 / 2 * np.sum(np.power(p - q, 2) / (p + q)))\n",
    "\n",
    "def entropy_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Unweighted entropy distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        -\\frac{2\\times S_{PQ}-S_P-S_Q} {ln(4)}, S_I=\\sum_{i} {I_i ln(I_i)}\n",
    "    \"\"\"\n",
    "\n",
    "    merged = p + q\n",
    "    entropy_increase = 2 * \\\n",
    "                       scipy.stats.entropy(merged) - scipy.stats.entropy(p) - \\\n",
    "                       scipy.stats.entropy(q)\n",
    "    \n",
    "    return 1 - entropy_increase\n",
    "\n",
    "def dot_product_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Dot product distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        1 - \\sqrt{\\frac{(\\sum{Q_iP_i})^2}{\\sum{Q_i^2\\sum P_i^2}}}\n",
    "    \"\"\"\n",
    "    \n",
    "    score = np.power(np.sum(q * p), 2) / (\n",
    "        np.sum(np.power(q, 2)) * np.sum(np.power(p, 2))\n",
    "    )\n",
    "    return np.sqrt(score)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \n",
    "        return 1/(1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create old similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_output_dir = '/Users/jonahpoczobutt/projects/TunaRes/oldSimResUW'\n",
    "\n",
    "demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/train/10_ppm/chunk_1.pkl')\n",
    "demo_matches_val = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/train/10_ppm/chunk_2.pkl')\n",
    "demo_matches['score'] = 1 * demo_matches['InchiCoreMatch']\n",
    "demo_matches_val['score'] = 1 * demo_matches_val['InchiCoreMatch']\n",
    "\n",
    "sim_names = ['prob','matusita','entropy','dot','lorentzian','harmonic']\n",
    "distances = [probabilistic_symmetric_chi_squared_distance,\n",
    "             matusita_distance,\n",
    "             entropy_distance,\n",
    "             dot_product_distance,\n",
    "             lorentzian_distance,\n",
    "             harmonic_mean_distance]\n",
    "\n",
    "# for _ in range(len(sim_names)):\n",
    "\n",
    "#      matched_scores_val = list()\n",
    "#      for i in range(len(demo_matches_val)):\n",
    "     \n",
    "#           matched = tools_fast.match_spectrum(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], ms2_da = 0.05)\n",
    "#           matched_scores_val.append(sigmoid(distances[_](matched[:,1]/sum(matched[:,1]), matched[:,2]/sum(matched[:,2]))))\n",
    "\n",
    "#      np.save(f'{sims_output_dir}/val_{sim_names[_]}.npy', np.array(matched_scores_val))\n",
    "\n",
    "#      matched_scores = list()\n",
    "#      for i in range(len(demo_matches)):\n",
    "\n",
    "#           matched = tools_fast.match_spectrum(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'], ms2_da = 0.05)\n",
    "#           matched_scores.append(sigmoid(distances[_](matched[:,1]/sum(matched[:,1]), matched[:,2]/sum(matched[:,2]))))\n",
    "\n",
    "#      np.save(f'{sims_output_dir}/train_{sim_names[_]}.npy', np.array(matched_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_vals = {\n",
    "    'mult_a' : 0.001,\n",
    "    'mult_b': 1,\n",
    "    'dif_a': 0.001,\n",
    "    'dif_b':1,\n",
    "    'add_norm_b' : 1,\n",
    "    # 'target_normalized_intensity_int': 0,\n",
    "    # 'query_normalized_intensity_int': 0,\n",
    "    'target_normalized_intensity_a': 0.1,\n",
    "    'query_normalized_intensity_a': 0.1,\n",
    "    'target_normalized_intensity_b': 0.1,\n",
    "    'query_normalized_intensity_b': 0.1,\n",
    "    # 'target_normalized_intensity_c': 0.1,\n",
    "    # 'query_normalized_intensity_c': 0.1,\n",
    "    # 'target_mz_int': 1e-10,\n",
    "    # 'query_mz_int': 1e-10,\n",
    "    # 'target_mz_a': 0.001,\n",
    "    # 'query_mz_a': 0.001,\n",
    "    # 'target_mz_b': 0.001,\n",
    "    # 'query_mz_b': 0.001,\n",
    "    # 'target_mz_c': 0.001,\n",
    "    # 'query_mz_c': 0.001,\n",
    "    # 'target_intensity_int': 0,\n",
    "    # 'query_intensity_int': 0,\n",
    "    # 'target_intensity_a': 1,\n",
    "    # 'query_intensity_a': 1,\n",
    "    # 'target_intensity_b': 1,\n",
    "    # 'query_intensity_b': 1,\n",
    "    # 'target_intensity_c': 1,\n",
    "    # 'query_intensity_c': 1,\n",
    "    }\n",
    "\n",
    "regularization_grad = lambda x: 0.\n",
    "\n",
    "fixed_vals = {'sigmoid_score' : True, \n",
    "              'weight_combine': 'multiply'\n",
    "    }\n",
    "\n",
    "bounds = {'add_norm_b': (0, 2),\n",
    "          'mult_b': (1e-10, 2),\n",
    "          'add_norm_a': (1e-10, 3),\n",
    "          'dif_b': (1e-10, 2),\n",
    "          'dif_a':(-1.5,1.5),\n",
    "          'mult_a': (-1.5,1.5),\n",
    "          'target_normalized_intensity_int': (-0.2,1),\n",
    "          'query_normalized_intensity_int': (-0.2,1),\n",
    "          'target_normalized_intensity_a': (-2,2),\n",
    "          'query_normalized_intensity_a': (-2,2),\n",
    "          'target_normalized_intensity_b': (-2,2),\n",
    "          'query_normalized_intensity_b': (-2,2),\n",
    "          'target_normalized_intensity_c': (-2,2),\n",
    "          'query_normalized_intensity_c': (-2,2),\n",
    "          'target_normalized_intensity_a': (-2,2),\n",
    "          'query_normalized_intensity_a': (-2,2),\n",
    "          'target_mz_b': (-2,2),\n",
    "          'query_mz_b': (-2,2),\n",
    "          'target_mz_a': (-2,2),\n",
    "          'query_mz_a': (-2,2),\n",
    "          'target_mz_int': (-0.2,1),\n",
    "          'query_mz_int': (-0.2,1),\n",
    "          'target_mz_c': (-2,2),\n",
    "          'query_mz_c': (-2,2),\n",
    "           'target_intensity_int': (-0.2,1),\n",
    "           'query_intensity_int': (-0.2,1),\n",
    "          'target_intensity_a': (1e-10,2),\n",
    "          'query_intensity_a': (1e-10,2),\n",
    "          'target_intensity_b': (1e-10,2),\n",
    "          'query_intensity_b': (1e-10,2),\n",
    "          'target_intensity_c': (1e-10,2),\n",
    "          'query_intensity_c': (1e-10,2),\n",
    "          }\n",
    "\n",
    "testerooni = func_ob('teesterooni',\n",
    "                     sim_func = TunaSims.ExpandedTuna,\n",
    "                     init_vals = init_vals,\n",
    "                     fixed_vals = fixed_vals,\n",
    "                     regularization_grad = regularization_grad,\n",
    "                     bounds = bounds,\n",
    "                     max_iter = 1e6,\n",
    "                     learning_rates = 0.001,\n",
    "                     momentum_type = None,\n",
    "                     learning_rate_scheduler = None,\n",
    "                     tol = 0,\n",
    "                     balance_classes = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained values\n",
      "mult_a 0.001\n",
      "mult_b 1\n",
      "dif_a 0.001\n",
      "dif_b 1\n",
      "add_norm_b 1\n",
      "target_normalized_intensity_a 0.1\n",
      "query_normalized_intensity_a 0.1\n",
      "target_normalized_intensity_b 0.1\n",
      "query_normalized_intensity_b 0.1\n"
     ]
    }
   ],
   "source": [
    "print('trained values')\n",
    "for i in testerooni.init_vals.keys():\n",
    "    print(i, getattr(testerooni.sim_func,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n",
      "completed 10000 iterations\n",
      "completed 20000 iterations\n",
      "completed 30000 iterations\n",
      "completed 40000 iterations\n",
      "completed 50000 iterations\n",
      "completed 60000 iterations\n",
      "completed 70000 iterations\n",
      "completed 80000 iterations\n",
      "completed 90000 iterations\n",
      "completed 100000 iterations\n",
      "completed 110000 iterations\n",
      "completed 120000 iterations\n",
      "completed 130000 iterations\n",
      "completed 140000 iterations\n",
      "completed 150000 iterations\n",
      "completed 160000 iterations\n",
      "completed 170000 iterations\n",
      "completed 180000 iterations\n",
      "completed 190000 iterations\n",
      "completed 200000 iterations\n",
      "completed 210000 iterations\n",
      "completed 220000 iterations\n",
      "completed 230000 iterations\n",
      "completed 240000 iterations\n",
      "completed 250000 iterations\n",
      "completed 260000 iterations\n",
      "completed 270000 iterations\n",
      "completed 280000 iterations\n",
      "completed 290000 iterations\n",
      "completed 300000 iterations\n",
      "completed 310000 iterations\n",
      "completed 320000 iterations\n",
      "completed 330000 iterations\n",
      "completed 340000 iterations\n",
      "completed 350000 iterations\n",
      "completed 360000 iterations\n",
      "completed 370000 iterations\n",
      "completed 380000 iterations\n",
      "completed 390000 iterations\n",
      "completed 400000 iterations\n",
      "completed 410000 iterations\n",
      "completed 420000 iterations\n",
      "completed 430000 iterations\n",
      "completed 440000 iterations\n",
      "completed 450000 iterations\n",
      "completed 460000 iterations\n",
      "completed 470000 iterations\n",
      "completed 480000 iterations\n",
      "completed 490000 iterations\n",
      "completed 500000 iterations\n",
      "completed 510000 iterations\n",
      "completed 520000 iterations\n",
      "completed 530000 iterations\n",
      "completed 540000 iterations\n",
      "completed 550000 iterations\n",
      "completed 560000 iterations\n",
      "completed 570000 iterations\n",
      "completed 580000 iterations\n",
      "completed 590000 iterations\n",
      "completed 600000 iterations\n",
      "completed 610000 iterations\n",
      "completed 620000 iterations\n",
      "completed 630000 iterations\n",
      "completed 640000 iterations\n",
      "completed 650000 iterations\n",
      "completed 660000 iterations\n",
      "completed 670000 iterations\n",
      "completed 680000 iterations\n",
      "completed 690000 iterations\n",
      "completed 700000 iterations\n",
      "completed 710000 iterations\n",
      "completed 720000 iterations\n",
      "completed 730000 iterations\n",
      "completed 740000 iterations\n",
      "completed 750000 iterations\n",
      "completed 760000 iterations\n",
      "completed 770000 iterations\n",
      "completed 780000 iterations\n",
      "completed 790000 iterations\n",
      "completed 800000 iterations\n",
      "completed 810000 iterations\n",
      "completed 820000 iterations\n",
      "completed 830000 iterations\n",
      "completed 840000 iterations\n",
      "completed 850000 iterations\n",
      "completed 860000 iterations\n",
      "completed 870000 iterations\n",
      "completed 880000 iterations\n",
      "completed 890000 iterations\n",
      "completed 900000 iterations\n",
      "completed 910000 iterations\n",
      "completed 920000 iterations\n",
      "completed 930000 iterations\n",
      "completed 940000 iterations\n",
      "completed 950000 iterations\n",
      "completed 960000 iterations\n",
      "completed 970000 iterations\n",
      "completed 980000 iterations\n",
      "completed 990000 iterations\n",
      "completed 1000000 iterations\n",
      "trained values\n",
      "mult_a 0.9294607082177285\n",
      "mult_b 0.7746077937660887\n",
      "dif_a -1.0879634056813028\n",
      "dif_b 1.6148769008821175\n",
      "add_norm_b 0.8601491955420384\n",
      "target_normalized_intensity_a 0.645991442660267\n",
      "query_normalized_intensity_a 0.7967488746178535\n",
      "target_normalized_intensity_b -0.4550452174313565\n",
      "query_normalized_intensity_b 0.04037588335825819\n",
      "\n",
      "\n",
      "aucs\n",
      "Custom  0.8519 0.8133\n",
      "0.8141 0.7987\n",
      "0.8212 0.803\n",
      "0.8176 0.801\n",
      "0.7616 0.7501\n",
      "0.8005 0.7868\n",
      "0.8141 0.7987\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "\n",
    "    print(f'round {i+1}')\n",
    "    testerooni.fit(demo_matches.copy(), verbose = 1e4)\n",
    "    demo_matches['preds'] = [testerooni.sim_func.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'],demo_matches.iloc[i]['precquery'], demo_matches.iloc[i]['prectarget'], grads = False) for i in range(len(demo_matches))]\n",
    "    demo_matches_val['preds'] = [testerooni.sim_func.predict(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], demo_matches_val.iloc[i]['precquery'], demo_matches_val.iloc[i]['prectarget']) for i in range(len(demo_matches_val))]\n",
    "\n",
    "print('trained values')\n",
    "for i in testerooni.init_vals.keys():\n",
    "    print(i, getattr(testerooni.sim_func,i))\n",
    "\n",
    "print('\\n')\n",
    "print('aucs')\n",
    "temp = demo_matches.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "temp_val = demo_matches_val.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "print('Custom ',round(roc_auc_score(temp['score'] , temp['preds']), 4), round(roc_auc_score(temp_val['score'] , temp_val['preds']), 4))\n",
    "\n",
    "for sim in sim_names:\n",
    "    demo_matches['preds'] = np.load(f'{sims_output_dir}/train_{sim}.npy')\n",
    "    demo_matches_val['preds'] = np.load(f'{sims_output_dir}/val_{sim}.npy')\n",
    "\n",
    "    temp = demo_matches.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "    temp_val = demo_matches_val.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "\n",
    "    print(round(roc_auc_score(temp['score'] , temp['preds']), 4), round(roc_auc_score(temp_val['score'] , temp_val['preds']), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(preds, bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreds Train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(preds, bins = 100)\n",
    "plt.title('Preds Train')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(preds_val, bins = 100)\n",
    "plt.title('Preds Val')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_matches['residual'] = np.abs(demo_matches['score'] - preds)\n",
    "residual_threshold = 0.2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels_train = demo_matches['score']\n",
    "original_labels_val = demo_matches_val['score']\n",
    "#maintain mapping to 0 1 interval\n",
    "demo_matches['score'] = (demo_matches['score'] - preds + 1) / 2\n",
    "plt.hist(demo_matches['score'], bins = 100)\n",
    "plt.title('train_residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testerooni = func_ob('teesterooni',\n",
    "                     sim_func = TunaSims.ExpandedTuna,\n",
    "                     init_vals = init_vals,\n",
    "                     fixed_vals = fixed_vals,\n",
    "                     regularization_grad = regularization_grad,\n",
    "                     bounds = bounds,\n",
    "                     max_iter = 1000000,\n",
    "                     lambdas = 0.001,\n",
    "                     tol = 0,\n",
    "                     balance_classes = False)\n",
    "\n",
    "testerooni.fit(demo_matches, verbose = 1e7)\n",
    "print(testerooni.converged)\n",
    "\n",
    "\n",
    "preds_2 = np.array([testerooni.sim_func.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'],demo_matches.iloc[i]['precquery'], demo_matches.iloc[i]['prectarget']) for i in range(len(demo_matches))])\n",
    "preds_val_2 = np.array([testerooni.sim_func.predict(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], demo_matches_val.iloc[i]['precquery'], demo_matches_val.iloc[i]['prectarget']) for i in range(len(demo_matches_val))])\n",
    "\n",
    "plt.hist(preds_2, bins = 100)\n",
    "plt.title('Train Preds 2')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(preds_2, bins = 100)\n",
    "plt.title('Val Preds 2')\n",
    "plt.show()\n",
    "\n",
    "print('trained values')\n",
    "for i in testerooni.init_vals.keys():\n",
    "    print(i, getattr(testerooni.sim_func,i))\n",
    "\n",
    "preds_combined = preds + (2 * preds_2 - 1)\n",
    "preds_combined_val = preds_val + (2 * preds_val_2 - 1)\n",
    "\n",
    "plt.hist((1 + original_labels_train - preds_combined) / 2, bins = 100)\n",
    "plt.title('Two Stage Train Residuals')\n",
    "plt.show()\n",
    "\n",
    "plt.hist((1 + original_labels_val - preds_combined_val) / 2, bins = 100)\n",
    "plt.title('Two Stage Val Residuals')\n",
    "plt.show()\n",
    "\n",
    "print('\\n')\n",
    "print('aucs')\n",
    "print(round(roc_auc_score(original_labels_train , preds_combined), 4), round(roc_auc_score(original_labels_val, preds_combined_val), 4))\n",
    "for sim in sim_names:\n",
    "    print(sim, round(roc_auc_score(original_labels_train, np.load(f'{sims_output_dir}/train_{sim}.npy')),4), round(roc_auc_score(original_labels_val, np.load(f'{sims_output_dir}/val_{sim}.npy')),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#maintain mapping to 0 1 interval\n",
    "demo_matches['score'] = (demo_matches['score'] - preds_combined + 1) / 2\n",
    "plt.hist(demo_matches['score'], bins = 100)\n",
    "plt.title('train_residuals')\n",
    "plt.show()\n",
    "\n",
    "testerooni = func_ob('teesterooni',\n",
    "                     sim_func = TunaSims.ExpandedTuna,\n",
    "                     init_vals = init_vals,\n",
    "                     fixed_vals = fixed_vals,\n",
    "                     regularization_grad = regularization_grad,\n",
    "                     bounds = bounds,\n",
    "                     max_iter = 1000000,\n",
    "                     lambdas = 0.001,\n",
    "                     tol = 0,\n",
    "                     balance_classes = False)\n",
    "\n",
    "testerooni.fit(demo_matches, verbose = 100000)\n",
    "print(testerooni.converged)\n",
    "\n",
    "\n",
    "preds_3 = np.array([testerooni.sim_func.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'],demo_matches.iloc[i]['precquery'], demo_matches.iloc[i]['prectarget']) for i in range(len(demo_matches))])\n",
    "preds_val_3 = np.array([testerooni.sim_func.predict(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], demo_matches_val.iloc[i]['precquery'], demo_matches_val.iloc[i]['prectarget']) for i in range(len(demo_matches_val))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_train = dict()\n",
    "all_scores_val = dict()\n",
    "\n",
    "for sim in sim_names:\n",
    "\n",
    "    all_scores_train[sim] = np.load(f'{sims_output_dir}/train_{sim}.npy')\n",
    "    all_scores_val[sim] = np.load(f'{sims_output_dir}/val_{sim}.npy')\n",
    "\n",
    "all_scores_train['preds'] = preds\n",
    "all_scores_val['preds'] = preds_val\n",
    "\n",
    "all_scores_train['preds2'] = preds_2\n",
    "all_scores_val['preds2'] = preds_val_2\n",
    "\n",
    "all_scores_train['preds3'] = preds_3\n",
    "all_scores_val['preds3'] = preds_val_3\n",
    "\n",
    "all_scores_train['score'] = original_labels_train\n",
    "all_scores_val['score'] = original_labels_val\n",
    "\n",
    "train_data = pd.DataFrame(all_scores_train)\n",
    "val_data = pd.DataFrame(all_scores_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Val Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Train Models with Each Pair/Triplet of Sim Scores Old and New"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create column groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_sim_combos = list()\n",
    "for sim1 in sim_names:\n",
    "    for sim2 in sim_names:\n",
    "        for sim3 in sim_names:\n",
    "\n",
    "            old_sim_combos.append(list(set([sim1, sim2, sim3])))\n",
    "\n",
    "new_sim_combos = list()\n",
    "new_sims = ['preds', 'preds2', 'preds3']\n",
    "for sim1 in new_sims:\n",
    "    for sim2 in new_sims:\n",
    "        for sim3 in new_sims:  \n",
    "\n",
    "            new_sim_combos.append(list(set([sim1, sim2, sim3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Models for each Column Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_performance = dict()\n",
    "\n",
    "for combo in old_sim_combos:\n",
    "\n",
    "    print(combo)\n",
    "\n",
    "    model = hgbc()\n",
    "    model.fit(train_data[combo], train_data['score'])\n",
    "    train_auc = roc_auc_score(original_labels_train, model.predict_proba(train_data[combo])[:,1])\n",
    "    val_auc = roc_auc_score(original_labels_val, model.predict_proba(val_data[combo])[:,1])\n",
    "\n",
    "    sim_performance['_'.join(combo)] = (train_auc, val_auc)\n",
    "\n",
    "sim_performance_new = dict()\n",
    "for combo in new_sim_combos:\n",
    "\n",
    "    print(combo)\n",
    "\n",
    "    model = hgbc()\n",
    "    model.fit(train_data[combo], train_data['score'])\n",
    "    train_auc = roc_auc_score(original_labels_train, model.predict_proba(train_data[combo])[:,1])\n",
    "    val_auc = roc_auc_score(original_labels_val, model.predict_proba(val_data[combo])[:,1])\n",
    "\n",
    "    sim_performance_new['_'.join(combo)] = (train_auc, val_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([val[1] for key, val in sim_performance.items() if len(key.split('_'))==1]))\n",
    "print(np.mean([val[1] for key, val in sim_performance.items() if len(key.split('_'))==2]))\n",
    "print(np.mean([val[1] for key, val in sim_performance.items() if len(key.split('_'))==3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([val[1] for key, val in sim_performance_new.items() if len(key.split('_'))==1 and 'preds' in key]))\n",
    "print(np.mean([val[1] for key, val in sim_performance_new.items() if len(key.split('_'))==2 and 'preds' in key]))\n",
    "print(np.mean([val[1] for key, val in sim_performance_new.items() if len(key.split('_'))==3 and 'preds' in key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_performance_2 = dict()\n",
    "for combo in new_sim_combos:\n",
    "\n",
    "    if 'preds' not in combo:\n",
    "        continue\n",
    "\n",
    "    for sim in sim_names:\n",
    "\n",
    "        combo_new = combo + [sim]\n",
    "\n",
    "        model = hgbc()\n",
    "        model.fit(train_data[combo_new], train_data['score'])\n",
    "        train_auc = roc_auc_score(original_labels_train, model.predict_proba(train_data[combo_new])[:,1])\n",
    "        val_auc = roc_auc_score(original_labels_val, model.predict_proba(val_data[combo_new])[:,1])\n",
    "\n",
    "        sim_performance_2['_'.join(combo_new)] = (train_auc, val_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([val[1] for key, val in sim_performance_2.items() if len(key.split('_'))==2]))\n",
    "print(np.mean([val[1] for key, val in sim_performance_2.items() if len(key.split('_'))==3]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
