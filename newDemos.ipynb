{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TunaSims\n",
    "import numpy as np\n",
    "from funcOb import func_ob\n",
    "import pandas as pd\n",
    "import datasetBuilder\n",
    "import tools_fast\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier as hgbc\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _weight_intensity_by_entropy(x):\n",
    "    WEIGHT_START = 0.25\n",
    "    ENTROPY_CUTOFF = 3\n",
    "    weight_slope = (1 - WEIGHT_START) / ENTROPY_CUTOFF\n",
    "\n",
    "    if np.sum(x) > 0:\n",
    "        entropy_x = scipy.stats.entropy(x)\n",
    "        if entropy_x < ENTROPY_CUTOFF:\n",
    "            weight = WEIGHT_START + weight_slope * entropy_x\n",
    "            x = np.power(x, weight)\n",
    "            x_sum = np.sum(x)\n",
    "            x = x / x_sum\n",
    "    return x\n",
    "\n",
    "def ppm(base, ppm):\n",
    "    \"\"\"\n",
    "    convert ppm threshold to dalton based on precursor exact mass (base)\n",
    "    \"\"\"\n",
    "\n",
    "    return base * (ppm / 1e6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_mean_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Harmonic mean distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        1-2\\sum(\\frac{P_{i}Q_{i}}{P_{i}+Q_{i}})\n",
    "    \"\"\"\n",
    "    p = _weight_intensity_by_entropy(p)\n",
    "    q = _weight_intensity_by_entropy(q)\n",
    "    return 2 * np.sum(p * q / (p + q))\n",
    "\n",
    "def lorentzian_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Lorentzian distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\sum{\\ln(1+|P_i-Q_i|)}\n",
    "    \"\"\"\n",
    "    p = _weight_intensity_by_entropy(p)\n",
    "    q = _weight_intensity_by_entropy(q)\n",
    "    return 1 - np.sum(np.log(1 + np.abs(p - q)))\n",
    "\n",
    "def matusita_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Matusita distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\sqrt{\\sum(\\sqrt{P_{i}}-\\sqrt{Q_{i}})^2}\n",
    "    \"\"\"\n",
    "    p = _weight_intensity_by_entropy(p)\n",
    "    q = _weight_intensity_by_entropy(q)\n",
    "    return 1- np.sum(np.power(np.sqrt(p) - np.sqrt(q), 2))\n",
    "\n",
    "def probabilistic_symmetric_chi_squared_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Probabilistic symmetric Ï‡2 distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\frac{1}{2} \\times \\sum\\frac{(P_{i}-Q_{i}\\ )^2}{P_{i}+Q_{i}\\ }\n",
    "    \"\"\"\n",
    "    p = _weight_intensity_by_entropy(p)\n",
    "    q = _weight_intensity_by_entropy(q)\n",
    "    return 1- (1 / 2 * np.sum(np.power(p - q, 2) / (p + q)))\n",
    "\n",
    "def entropy_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Unweighted entropy distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        -\\frac{2\\times S_{PQ}-S_P-S_Q} {ln(4)}, S_I=\\sum_{i} {I_i ln(I_i)}\n",
    "    \"\"\"\n",
    "    p = _weight_intensity_by_entropy(p)\n",
    "    q = _weight_intensity_by_entropy(q)\n",
    "    merged = p + q\n",
    "    entropy_increase = 2 * \\\n",
    "                       scipy.stats.entropy(merged) - scipy.stats.entropy(p) - \\\n",
    "                       scipy.stats.entropy(q)\n",
    "    \n",
    "    return 1 - entropy_increase\n",
    "\n",
    "def dot_product_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Dot product distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        1 - \\sqrt{\\frac{(\\sum{Q_iP_i})^2}{\\sum{Q_i^2\\sum P_i^2}}}\n",
    "    \"\"\"\n",
    "    p = _weight_intensity_by_entropy(p)\n",
    "    q = _weight_intensity_by_entropy(q)    \n",
    "    score = np.power(np.sum(q * p), 2) / (\n",
    "        np.sum(np.power(q, 2)) * np.sum(np.power(p, 2))\n",
    "    )\n",
    "    return np.sqrt(score)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \n",
    "        return 1/(1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Matches DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_dataset = False\n",
    "if create_new_dataset:\n",
    "    n_dfs = 3\n",
    "    set_names = ['train','val','test']\n",
    "    dataset_sizes = 2e6\n",
    "\n",
    "    input_df = pd.read_pickle('/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist20.pkl')\n",
    "    input_df.sort_values(by = 'inchi_base', inplace = True)\n",
    "    input_df['queryID'] = [i for i in range(len(input_df))]\n",
    "    print(len(input_df))\n",
    "    all_bases = list(set(input_df['inchi_base']))\n",
    "    print(len(all_bases))\n",
    "    np.random.shuffle(all_bases)\n",
    "\n",
    "    base_sets = list()\n",
    "    separated_dfs = list()\n",
    "    assigned_inds = list()\n",
    "\n",
    "    for i in range(n_dfs):\n",
    "        base_set = all_bases[int(i * len(all_bases)/n_dfs): int((i + 1) *len(all_bases)/n_dfs)]\n",
    "        base_sets.append(set(base_set))\n",
    "        assigned_inds.append(list())\n",
    "\n",
    "    for i in range(len(input_df)):\n",
    "\n",
    "        for j in range(len(base_sets)):\n",
    "\n",
    "            if input_df.iloc[i]['inchi_base'] in base_sets[j]:\n",
    "\n",
    "                assigned_inds[j].append(i)\n",
    "                break\n",
    "\n",
    "    for i in range(n_dfs):   \n",
    "        datasetBuilder.create_matches_df_chunk(input_df.iloc[assigned_inds[i]], \n",
    "                                                input_df.iloc[assigned_inds[i]],\n",
    "                                                10,\n",
    "                                                dataset_sizes,\n",
    "                                                dataset_sizes,\n",
    "                                                f'/Users/jonahpoczobutt/projects/TunaRes/NIST20_inputs/{set_names[i]}',\n",
    "                                                f'loggy.log')\n",
    "        print(i)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create old similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_output_dir = '/Users/jonahpoczobutt/projects/TunaRes/oldSimRes'\n",
    "if create_new_dataset:\n",
    "\n",
    "     demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/Nist20_inputs/train/chunk_1.pkl')\n",
    "     demo_matches_val = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/Nist20_inputs/val/chunk_1.pkl')\n",
    "\n",
    "     queries = list()\n",
    "     targets = list()\n",
    "     indices = list()\n",
    "     for i in range(len(demo_matches)):\n",
    "\n",
    "          query = demo_matches.iloc[i]['query'][demo_matches.iloc[i]['query'][:,0] < demo_matches.iloc[i]['precquery'] - ppm(demo_matches.iloc[i]['precquery'],3)]\n",
    "          target = demo_matches.iloc[i]['target'][demo_matches.iloc[i]['target'][:,0] < demo_matches.iloc[i]['prectarget'] - ppm(demo_matches.iloc[i]['prectarget'],3)]\n",
    "\n",
    "          if len(query) > 0 and len(target) > 0:\n",
    "               indices.append(i)\n",
    "               queries.append(query)\n",
    "               targets.append(target)\n",
    "\n",
    "     demo_matches = demo_matches.iloc[indices]\n",
    "     demo_matches['query'] = queries\n",
    "     demo_matches['target'] = targets\n",
    "\n",
    "     queries = list()\n",
    "     targets = list()\n",
    "     indices = list()\n",
    "     for i in range(len(demo_matches_val)):\n",
    "\n",
    "          query = demo_matches_val.iloc[i]['query'][demo_matches_val.iloc[i]['query'][:,0] < demo_matches_val.iloc[i]['precquery'] - ppm(demo_matches_val.iloc[i]['precquery'],3)]\n",
    "          target = demo_matches_val.iloc[i]['target'][demo_matches_val.iloc[i]['target'][:,0] < demo_matches_val.iloc[i]['prectarget'] - ppm(demo_matches_val.iloc[i]['prectarget'],3)]\n",
    "\n",
    "          if len(query) > 0 and len(target) > 0:\n",
    "               indices.append(i)\n",
    "               queries.append(query)\n",
    "               targets.append(target)\n",
    "\n",
    "     demo_matches_val = demo_matches_val.iloc[indices]\n",
    "     demo_matches_val['query'] = queries\n",
    "     demo_matches_val['target'] = targets\n",
    "\n",
    "     demo_matches.to_pickle('/Users/jonahpoczobutt/projects/TunaRes/inputs/demo_matches_no_prec.pkl')\n",
    "     demo_matches_val.to_pickle('/Users/jonahpoczobutt/projects/TunaRes/inputs/demo_matches_val_no_prec.pkl')\n",
    "\n",
    "     sim_names = ['prob','matusita','entropy','dot','lorentzian','harmonic']\n",
    "     distances = [probabilistic_symmetric_chi_squared_distance,\n",
    "               matusita_distance,\n",
    "               entropy_distance,\n",
    "               dot_product_distance,\n",
    "               lorentzian_distance,\n",
    "               harmonic_mean_distance]\n",
    "\n",
    "     for _ in range(len(sim_names)):\n",
    "\n",
    "          matched_scores_val = list()\n",
    "          for i in range(len(demo_matches_val)):\n",
    "          \n",
    "               matched = tools_fast.match_spectrum(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], ms2_da = 0.05)\n",
    "               matched_scores_val.append(sigmoid(distances[_](matched[:,1]/sum(matched[:,1]), matched[:,2]/sum(matched[:,2]))))\n",
    "\n",
    "          np.save(f'{sims_output_dir}/val_{sim_names[_]}.npy', np.array(matched_scores_val))\n",
    "\n",
    "          matched_scores = list()\n",
    "          for i in range(len(demo_matches)):\n",
    "\n",
    "               matched = tools_fast.match_spectrum(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'], ms2_da = 0.05)\n",
    "               matched_scores.append(sigmoid(distances[_](matched[:,1]/sum(matched[:,1]), matched[:,2]/sum(matched[:,2]))))\n",
    "\n",
    "          np.save(f'{sims_output_dir}/train_{sim_names[_]}.npy', np.array(matched_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Metric Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_names = ['prob','matusita','entropy','dot','lorentzian','harmonic']\n",
    "\n",
    "demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/inputs/demo_matches_no_prec.pkl')\n",
    "demo_matches_val = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/inputs/demo_matches_val_no_prec.pkl')\n",
    "\n",
    "demo_matches['score'] = 1 * demo_matches['InchiCoreMatch']\n",
    "demo_matches_val['score'] = 1 * demo_matches_val['InchiCoreMatch']\n",
    "demo_matches['queryID_target_base'] = [str(demo_matches.iloc[i]['queryID']) + '_' + demo_matches.iloc[i]['target_base'] for i in range(len(demo_matches))]\n",
    "\n",
    "calc_base_sims = False\n",
    "if calc_base_sims:\n",
    "    for sim in sim_names:\n",
    "        demo_matches['preds'] = np.load(f'{sims_output_dir}/train_{sim}.npy')\n",
    "        demo_matches_val['preds'] = np.load(f'{sims_output_dir}/val_{sim}.npy')\n",
    "\n",
    "        temp = demo_matches.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "        temp_val = demo_matches_val.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "\n",
    "        print(sim, round(roc_auc_score(temp['score'] , temp['preds']), 4), round(roc_auc_score(temp_val['score'] , temp_val['preds']), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "init_vals = {\n",
    "    'mult_a' : 0.001,\n",
    "    'mult_b': 1,\n",
    "    'dif_a': 0.001,\n",
    "    'dif_b':1,\n",
    "    'add_norm_b' : 1,\n",
    "    # 'target_normalized_intensity_int': 0,\n",
    "    # 'query_normalized_intensity_int': 0,\n",
    "    'target_normalized_intensity_a': 0.1,\n",
    "    'query_normalized_intensity_a': 0.1,\n",
    "    'target_normalized_intensity_b': 0.1,\n",
    "    'query_normalized_intensity_b': 0.1,\n",
    "    # 'target_normalized_intensity_c': 0.1,\n",
    "    # 'query_normalized_intensity_c': 0.1,\n",
    "    # 'target_mz_int': 1e-10,\n",
    "    # 'query_mz_int': 1e-10,\n",
    "    # 'target_mz_a': 0.001,\n",
    "    # 'query_mz_a': 0.001,\n",
    "    # 'target_mz_b': 0.001,\n",
    "    # 'query_mz_b': 0.001,\n",
    "    # 'target_mz_c': 0.001,\n",
    "    # 'query_mz_c': 0.001,\n",
    "    # 'target_intensity_int': 0,\n",
    "    # 'query_intensity_int': 0,\n",
    "    # 'target_intensity_a': 1,\n",
    "    # 'query_intensity_a': 1,\n",
    "    # 'target_intensity_b': 1,\n",
    "    # 'query_intensity_b': 1,\n",
    "    # 'target_intensity_c': 1,\n",
    "    # 'query_intensity_c': 1,\n",
    "    }\n",
    "\n",
    "init_vals_2 = {\n",
    "    'mult_a' : 0.001,\n",
    "    'mult_b': 1,\n",
    "    'dif_a': 0.001,\n",
    "    'dif_b':1,\n",
    "    'add_norm_b' : 1,\n",
    "    # 'target_normalized_intensity_int': 0,\n",
    "    # 'query_normalized_intensity_int': 0,\n",
    "    'target_normalized_intensity_a': 0.1,\n",
    "    'query_normalized_intensity_a': 0.1,\n",
    "    'target_normalized_intensity_b': 0.1,\n",
    "    'query_normalized_intensity_b': 0.1,\n",
    "    # 'target_normalized_intensity_c': 0.1,\n",
    "    # 'query_normalized_intensity_c': 0.1,\n",
    "    'target_mz_int': 1e-10,\n",
    "    'query_mz_int': 1e-10,\n",
    "    'target_mz_a': 0.001,\n",
    "    'query_mz_a': 0.001,\n",
    "    'target_mz_b': 0.001,\n",
    "    'query_mz_b': 0.001,\n",
    "    # 'target_mz_c': 0.001,\n",
    "    # 'query_mz_c': 0.001,\n",
    "    # 'target_intensity_int': 0,\n",
    "    # 'query_intensity_int': 0,\n",
    "    # 'target_intensity_a': 1,\n",
    "    # 'query_intensity_a': 1,\n",
    "    # 'target_intensity_b': 1,\n",
    "    # 'query_intensity_b': 1,\n",
    "    # 'target_intensity_c': 1,\n",
    "    # 'query_intensity_c': 1,\n",
    "    }\n",
    "\n",
    "init_vals_3 = {\n",
    "    'mult_a' : 0.001,\n",
    "    'mult_b': 1,\n",
    "    'dif_a': 0.001,\n",
    "    'dif_b':1,\n",
    "    'add_norm_b' : 1,\n",
    "    'target_normalized_intensity_int': 0,\n",
    "    'query_normalized_intensity_int': 0,\n",
    "    'target_normalized_intensity_a': 1,\n",
    "    'query_normalized_intensity_a': 1,\n",
    "    'target_normalized_intensity_b': 1,\n",
    "    'query_normalized_intensity_b': 1,\n",
    "    # 'target_normalized_intensity_c': 0.1,\n",
    "    # 'query_normalized_intensity_c': 0.1,\n",
    "    'target_mz_int': 0,\n",
    "    'query_mz_int': 0,\n",
    "    'target_mz_a': 1,\n",
    "    'query_mz_a': 1,\n",
    "    'target_mz_b': 0.,\n",
    "    'query_mz_b': 0.,\n",
    "    # 'target_mz_c': 0.001,\n",
    "    # 'query_mz_c': 0.001,\n",
    "    # 'target_intensity_int': 0,\n",
    "    # 'query_intensity_int': 0,\n",
    "    # 'target_intensity_a': 1,\n",
    "    # 'query_intensity_a': 1,\n",
    "    # 'target_intensity_b': 1,\n",
    "    # 'query_intensity_b': 1,\n",
    "    # 'target_intensity_c': 1,\n",
    "    # 'query_intensity_c': 1,\n",
    "    }\n",
    "\n",
    "regularization_grad = lambda x: 0.\n",
    "\n",
    "fixed_vals = {'sigmoid_score' : True, \n",
    "              'weight_combine': 'multiply'\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "bounds = {'add_norm_b': (0, 2),\n",
    "          'mult_b': (1e-10, 2),\n",
    "          'add_norm_a': (1e-10, 3),\n",
    "          'dif_b': (1e-10, 2),\n",
    "          'dif_a':(-1.5,1.5),\n",
    "          'mult_a': (-1.5,1.5),\n",
    "          'target_normalized_intensity_int': (-0.2,1),\n",
    "          'query_normalized_intensity_int': (-0.2,1),\n",
    "          'target_normalized_intensity_a': (1e-10,2),\n",
    "          'query_normalized_intensity_a': (1e-10,2),\n",
    "          'target_normalized_intensity_b': (0,2),\n",
    "          'query_normalized_intensity_b': (0,2),\n",
    "          'target_normalized_intensity_c': (-2,2),\n",
    "          'query_normalized_intensity_c': (-2,2),\n",
    "          'target_mz_b': (-2,2),\n",
    "          'query_mz_b': (-2,2),\n",
    "          'target_mz_a': (-2,2),\n",
    "          'query_mz_a': (-2,2),\n",
    "          'target_mz_int': (-0.2,1),\n",
    "          'query_mz_int': (-0.2,1),\n",
    "          'target_mz_c': (-2,2),\n",
    "          'query_mz_c': (-2,2),\n",
    "           'target_intensity_int': (-0.2,1),\n",
    "           'query_intensity_int': (-0.2,1),\n",
    "          'target_intensity_a': (1e-10,2),\n",
    "          'query_intensity_a': (1e-10,2),\n",
    "          'target_intensity_b': (1e-10,2),\n",
    "          'query_intensity_b': (1e-10,2),\n",
    "          'target_intensity_c': (1e-10,2),\n",
    "          'query_intensity_c': (1e-10,2),\n",
    "          }\n",
    "\n",
    "\n",
    "init_names = ['intensity', 'full']\n",
    "inits = [init_vals, init_vals_3]\n",
    "ad_params = [(0.8,0.3), (0.8, 0.4), (0.8, 0.5)]\n",
    "func_obs = list()\n",
    "\n",
    "for i in range(1):\n",
    "    for momentum in [None]:\n",
    "        for sched in ['ad']:\n",
    "            for i in range(len(inits)):\n",
    "                for ad_param in ad_params:\n",
    "                \n",
    "                    func_obs.append(func_ob(f'{momentum}_{sched}_{init_names[i]}_{ad_param}',\n",
    "                                sim_func = TunaSims.ExpandedTuna,\n",
    "                                init_vals = inits[i].copy(),\n",
    "                                fixed_vals = fixed_vals,\n",
    "                                regularization_grad = regularization_grad,\n",
    "                                bounds = bounds,\n",
    "                                max_iter = 1e6,\n",
    "                                learning_rates = 0.001,\n",
    "                                momentum_type = momentum,\n",
    "                                learning_rate_scheduler = sched,\n",
    "                                learning_beta = 0.5,\n",
    "                                momentum_beta = 0.3,\n",
    "                                tol = 0,\n",
    "                                balance_classes = True,\n",
    "                                groupby_column = 'queryID_target_base',\n",
    "                                ad_int = ad_param[0],\n",
    "                                ad_slope= ad_param[1]))\n",
    "                \n",
    "print(len(func_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced\n",
      "pred_val=0.5065956686975224\n",
      "loss_grad=-0.9868086626049553\n",
      "grad=-0.027046744207195782\n",
      "unweighted_step=-0.027046744207195782\n",
      "learning_rate=0.0009500000000000001\n",
      "key='mult_a', -0.5, current_value=0.001, updated=0.001025694406996836, learning_rate=0.0009500000000000001, grad=-0.027046744207195782\n",
      "loss_grad=-0.9868086626049553\n",
      "grad=0.00014041359123899994\n",
      "unweighted_step=0.00014041359123899994\n",
      "learning_rate=0.0009500000000000001\n",
      "key='mult_b', 0.5, current_value=1, updated=0.9999998666070883, learning_rate=0.0009500000000000001, grad=0.00014041359123899994\n",
      "loss_grad=-0.9868086626049553\n",
      "grad=-6.480867846981509\n",
      "unweighted_step=-6.480867846981509\n",
      "learning_rate=0.0009500000000000001\n",
      "key='dif_a', -0.5, current_value=0.001, updated=0.007156824454632434, learning_rate=0.0009500000000000001, grad=-6.480867846981509\n",
      "loss_grad=-0.9868086626049553\n",
      "grad=0.01812037169742578\n",
      "unweighted_step=0.01812037169742578\n",
      "learning_rate=0.0009500000000000001\n",
      "key='dif_b', 0.5, current_value=1, updated=0.9999827856468875, learning_rate=0.0009500000000000001, grad=0.01812037169742578\n",
      "loss_grad=-0.9868086626049553\n",
      "grad=-0.018012212492743342\n",
      "unweighted_step=-0.018012212492743342\n",
      "learning_rate=0.0009500000000000001\n",
      "key='add_norm_b', -0.5, current_value=1, updated=1.0000171116018681, learning_rate=0.0009500000000000001, grad=-0.018012212492743342\n",
      "loss_grad=-0.9868086626049553\n",
      "grad=0.0035195084962210705\n",
      "unweighted_step=0.0035195084962210705\n",
      "learning_rate=0.0009500000000000001\n",
      "key='target_normalized_intensity_a', 0.5, current_value=0.1, updated=0.0999966564669286, learning_rate=0.0009500000000000001, grad=0.0035195084962210705\n",
      "loss_grad=-0.9868086626049553\n",
      "grad=-0.0037899761128797078\n",
      "unweighted_step=-0.0037899761128797078\n",
      "learning_rate=0.0009500000000000001\n",
      "key='query_normalized_intensity_a', -0.5, current_value=0.1, updated=0.10000360047730725, learning_rate=0.0009500000000000001, grad=-0.0037899761128797078\n",
      "loss_grad=-0.9868086626049553\n",
      "grad=-0.001470685569330481\n",
      "unweighted_step=-0.001470685569330481\n",
      "learning_rate=0.0009500000000000001\n",
      "key='target_normalized_intensity_b', -0.5, current_value=0.1, updated=0.10000139715129087, learning_rate=0.0009500000000000001, grad=-0.001470685569330481\n",
      "loss_grad=-0.9868086626049553\n",
      "grad=0.0008795829110421397\n",
      "unweighted_step=0.0008795829110421397\n",
      "learning_rate=0.0009500000000000001\n",
      "key='query_normalized_intensity_b', 0.5, current_value=0.1, updated=0.09999916439623452, learning_rate=0.0009500000000000001, grad=0.0008795829110421397\n",
      "pred_val=0.5451682683723191\n",
      "loss_grad=-0.9096634632553617\n",
      "grad=-0.007497016344161026\n",
      "unweighted_step=-0.007497016344161026\n",
      "learning_rate=0.00097375\n",
      "loss_grad=-0.9096634632553617\n",
      "grad=4.0933224327126315e-05\n",
      "unweighted_step=4.0933224327126315e-05\n",
      "learning_rate=0.00097375\n",
      "loss_grad=-0.9096634632553617\n",
      "grad=-5.7087224511972\n",
      "unweighted_step=-5.7087224511972\n",
      "learning_rate=0.00097375\n",
      "loss_grad=-0.9096634632553617\n",
      "grad=0.11426677618168166\n",
      "unweighted_step=0.11426677618168166\n",
      "learning_rate=0.00097375\n",
      "loss_grad=-0.9096634632553617\n",
      "grad=-0.11369632929660739\n",
      "unweighted_step=-0.11369632929660739\n",
      "learning_rate=0.00097375\n",
      "loss_grad=-0.9096634632553617\n",
      "grad=-0.007327848525561728\n",
      "unweighted_step=-0.007327848525561728\n",
      "learning_rate=0.0008312500000000001\n",
      "loss_grad=-0.9096634632553617\n",
      "grad=0.007264614360629224\n",
      "unweighted_step=0.007264614360629224\n",
      "learning_rate=0.0008312500000000001\n",
      "loss_grad=-0.9096634632553617\n",
      "grad=0.0003027286332418738\n",
      "unweighted_step=0.0003027286332418738\n",
      "learning_rate=0.0008312500000000001\n",
      "loss_grad=-0.9096634632553617\n",
      "grad=-0.004909663528576841\n",
      "unweighted_step=-0.004909663528576841\n",
      "learning_rate=0.0008312500000000001\n",
      "pred_val=0.5517222147134334\n",
      "loss_grad=-0.8965555705731332\n",
      "grad=-0.04161801125143967\n",
      "unweighted_step=-0.04161801125143967\n",
      "learning_rate=0.001034609375\n",
      "loss_grad=-0.8965555705731332\n",
      "grad=0.00022137882081571693\n",
      "unweighted_step=0.00022137882081571693\n",
      "learning_rate=0.001034609375\n",
      "loss_grad=-0.8965555705731332\n",
      "grad=-3.6173680180815424\n",
      "unweighted_step=-3.6173680180815424\n",
      "learning_rate=0.001034609375\n",
      "loss_grad=-0.8965555705731332\n",
      "grad=0.132073867364841\n",
      "unweighted_step=0.132073867364841\n",
      "learning_rate=0.001034609375\n",
      "loss_grad=-0.8965555705731332\n",
      "grad=-0.13017981313053267\n",
      "unweighted_step=-0.13017981313053267\n",
      "learning_rate=0.001034609375\n",
      "loss_grad=-0.8965555705731332\n",
      "grad=-0.04215788946648169\n",
      "unweighted_step=-0.04215788946648169\n",
      "learning_rate=0.0008208593750000001\n",
      "loss_grad=-0.8965555705731332\n",
      "grad=0.04184804565614131\n",
      "unweighted_step=0.04184804565614131\n",
      "learning_rate=0.0008208593750000001\n",
      "loss_grad=-0.8965555705731332\n",
      "grad=0.010881314779120146\n",
      "unweighted_step=0.010881314779120146\n",
      "learning_rate=0.0008208593750000001\n",
      "loss_grad=-0.8965555705731332\n",
      "grad=-0.01923096908079621\n",
      "unweighted_step=-0.01923096908079621\n",
      "learning_rate=0.0008208593750000001\n",
      "pred_val=0.626959587842066\n",
      "loss_grad=1.253919175684132\n",
      "grad=0.04278374335424518\n",
      "unweighted_step=0.04278374335424518\n",
      "learning_rate=0.00084708642578125\n",
      "loss_grad=1.253919175684132\n",
      "grad=-0.00023895290849897062\n",
      "unweighted_step=-0.00023895290849897062\n",
      "learning_rate=0.00084708642578125\n",
      "loss_grad=1.253919175684132\n",
      "grad=9.248697070304912\n",
      "unweighted_step=9.248697070304912\n",
      "learning_rate=0.00084708642578125\n",
      "loss_grad=1.253919175684132\n",
      "grad=-0.43626356356346063\n",
      "unweighted_step=-0.43626356356346063\n",
      "learning_rate=0.00084708642578125\n",
      "loss_grad=1.253919175684132\n",
      "grad=0.43155053139076865\n",
      "unweighted_step=0.43155053139076865\n",
      "learning_rate=0.00084708642578125\n",
      "loss_grad=1.253919175684132\n",
      "grad=0.04686492065624301\n",
      "unweighted_step=0.04686492065624301\n",
      "learning_rate=0.0007028608398437501\n",
      "loss_grad=1.253919175684132\n",
      "grad=-0.04724269722142751\n",
      "unweighted_step=-0.04724269722142751\n",
      "learning_rate=0.0007028608398437501\n",
      "loss_grad=1.253919175684132\n",
      "grad=-0.012785277711827489\n",
      "unweighted_step=-0.012785277711827489\n",
      "learning_rate=0.0007028608398437501\n",
      "loss_grad=1.253919175684132\n",
      "grad=0.03634996988884805\n",
      "unweighted_step=0.03634996988884805\n",
      "learning_rate=0.0007028608398437501\n",
      "pred_val=0.5724154183261536\n",
      "loss_grad=1.1448308366523072\n",
      "grad=0.06859254986407692\n",
      "unweighted_step=0.06859254986407692\n",
      "learning_rate=0.0008126735397338868\n",
      "loss_grad=1.1448308366523072\n",
      "grad=-0.00037691219415170716\n",
      "unweighted_step=-0.00037691219415170716\n",
      "learning_rate=0.0008126735397338868\n",
      "loss_grad=1.1448308366523072\n",
      "grad=9.470039976138438\n",
      "unweighted_step=9.470039976138438\n",
      "learning_rate=0.0008126735397338868\n",
      "loss_grad=1.1448308366523072\n",
      "grad=-0.2348035408148666\n",
      "unweighted_step=-0.2348035408148666\n",
      "learning_rate=0.0008126735397338868\n",
      "loss_grad=1.1448308366523072\n",
      "grad=0.23110870728739288\n",
      "unweighted_step=0.23110870728739288\n",
      "learning_rate=0.0008126735397338868\n",
      "loss_grad=1.1448308366523072\n",
      "grad=0.012948472681532057\n",
      "unweighted_step=0.012948472681532057\n",
      "learning_rate=0.0006874857589721681\n",
      "loss_grad=1.1448308366523072\n",
      "grad=-0.012066680822190503\n",
      "unweighted_step=-0.012066680822190503\n",
      "learning_rate=0.0006874857589721681\n",
      "loss_grad=1.1448308366523072\n",
      "grad=0.0035657436652400923\n",
      "unweighted_step=0.0035657436652400923\n",
      "learning_rate=0.0006479498367309572\n",
      "loss_grad=1.1448308366523072\n",
      "grad=0.015456485948846485\n",
      "unweighted_step=0.015456485948846485\n",
      "learning_rate=0.0006874857589721681\n",
      "pred_val=0.539404143729211\n",
      "loss_grad=1.078808287458422\n",
      "grad=0.10492498916651347\n",
      "unweighted_step=0.10492498916651347\n",
      "learning_rate=0.0008367997854447366\n",
      "loss_grad=1.078808287458422\n",
      "grad=-0.0005927709509654292\n",
      "unweighted_step=-0.0005927709509654292\n",
      "learning_rate=0.0008367997854447366\n",
      "loss_grad=1.078808287458422\n",
      "grad=45.5183137160904\n",
      "unweighted_step=45.5183137160904\n",
      "learning_rate=0.0008367997854447366\n",
      "loss_grad=1.078808287458422\n",
      "grad=-0.12371515561221702\n",
      "unweighted_step=-0.12371515561221702\n",
      "learning_rate=0.0008367997854447366\n",
      "loss_grad=1.078808287458422\n",
      "grad=0.12329510770577719\n",
      "unweighted_step=0.12329510770577719\n",
      "learning_rate=0.0008367997854447366\n",
      "loss_grad=1.078808287458422\n",
      "grad=-0.0019665629863359125\n",
      "unweighted_step=-0.0019665629863359125\n",
      "learning_rate=0.000591882270615101\n",
      "loss_grad=1.078808287458422\n",
      "grad=0.003246749163785856\n",
      "unweighted_step=0.003246749163785856\n",
      "learning_rate=0.000591882270615101\n",
      "loss_grad=1.078808287458422\n",
      "grad=0.0027014169229401916\n",
      "unweighted_step=0.0027014169229401916\n",
      "learning_rate=0.0006550367880702021\n",
      "loss_grad=1.078808287458422\n",
      "grad=-0.0007044190650411031\n",
      "unweighted_step=-0.0007044190650411031\n",
      "learning_rate=0.000591882270615101\n",
      "pred_val=0.17699299970418908\n",
      "loss_grad=0.35398599940837816\n",
      "grad=0.08430957200786024\n",
      "unweighted_step=0.08430957200786024\n",
      "learning_rate=0.0008910610215321689\n",
      "loss_grad=0.35398599940837816\n",
      "grad=-0.00042212476919863937\n",
      "unweighted_step=-0.00042212476919863937\n",
      "learning_rate=0.0008910610215321689\n",
      "loss_grad=0.35398599940837816\n",
      "grad=2.1344887806397175\n",
      "unweighted_step=2.1344887806397175\n",
      "learning_rate=0.0008910610215321689\n",
      "loss_grad=0.35398599940837816\n",
      "grad=0.24281275840503896\n",
      "unweighted_step=0.24281275840503896\n",
      "learning_rate=0.0006988585708128308\n",
      "loss_grad=0.35398599940837816\n",
      "grad=-0.2262057901673554\n",
      "unweighted_step=-0.2262057901673554\n",
      "learning_rate=0.0006988585708128308\n",
      "loss_grad=0.35398599940837816\n",
      "grad=-0.15156561614022165\n",
      "unweighted_step=-0.15156561614022165\n",
      "learning_rate=0.0005803220700171498\n",
      "loss_grad=0.35398599940837816\n",
      "grad=0.15168339065102085\n",
      "unweighted_step=0.15168339065102085\n",
      "learning_rate=0.0005803220700171498\n",
      "loss_grad=0.35398599940837816\n",
      "grad=0.050064622958284406\n",
      "unweighted_step=0.050064622958284406\n",
      "learning_rate=0.0006913708599084711\n",
      "loss_grad=0.35398599940837816\n",
      "grad=-0.11639118893028792\n",
      "unweighted_step=-0.11639118893028792\n",
      "learning_rate=0.0005803220700171498\n",
      "pred_val=0.383828572657445\n",
      "loss_grad=-1.2323428546851098\n",
      "grad=-0.2821942573258569\n",
      "unweighted_step=-0.2821942573258569\n",
      "learning_rate=0.0007285119992448553\n",
      "loss_grad=-1.2323428546851098\n",
      "grad=0.0012793760056423748\n",
      "unweighted_step=0.0012793760056423748\n",
      "learning_rate=0.0007285119992448553\n",
      "loss_grad=-1.2323428546851098\n",
      "grad=-3.5374217968640203\n",
      "unweighted_step=-3.5374217968640203\n",
      "learning_rate=0.0007285119992448553\n",
      "loss_grad=-1.2323428546851098\n",
      "grad=-0.4208019309958147\n",
      "unweighted_step=-0.4208019309958147\n",
      "learning_rate=0.00065163101895712\n",
      "loss_grad=-1.2323428546851098\n",
      "grad=0.38398935360976594\n",
      "unweighted_step=0.38398935360976594\n",
      "learning_rate=0.00065163101895712\n",
      "loss_grad=-1.2323428546851098\n",
      "grad=1.4129820814582743\n",
      "unweighted_step=1.4129820814582743\n",
      "learning_rate=0.0004989409672295886\n",
      "loss_grad=-1.2323428546851098\n",
      "grad=-1.4170604776466853\n",
      "unweighted_step=-1.4170604776466853\n",
      "learning_rate=0.0004989409672295886\n",
      "loss_grad=-1.2323428546851098\n",
      "grad=-0.6557226101827254\n",
      "unweighted_step=-0.6557226101827254\n",
      "learning_rate=0.0005684904922294264\n",
      "loss_grad=-1.2323428546851098\n",
      "grad=0.7835018647148915\n",
      "unweighted_step=0.7835018647148915\n",
      "learning_rate=0.0004989409672295886\n",
      "pred_val=0.24237970727345887\n",
      "loss_grad=-1.5152405854530824\n",
      "grad=-0.4075514130002483\n",
      "unweighted_step=-0.4075514130002483\n",
      "learning_rate=0.0006984893367759755\n",
      "loss_grad=-1.5152405854530824\n",
      "grad=0.002329221871224981\n",
      "unweighted_step=0.002329221871224981\n",
      "learning_rate=0.0006984893367759755\n",
      "loss_grad=-1.5152405854530824\n",
      "grad=-8.70256738589563\n",
      "unweighted_step=-8.70256738589563\n",
      "learning_rate=0.0006984893367759755\n",
      "loss_grad=-1.5152405854530824\n",
      "grad=-0.9810258593012683\n",
      "unweighted_step=-0.9810258593012683\n",
      "learning_rate=0.0006621945686784953\n",
      "loss_grad=-1.5152405854530824\n",
      "grad=0.9163441798890073\n",
      "unweighted_step=0.9163441798890073\n",
      "learning_rate=0.0006621945686784953\n",
      "loss_grad=-1.5152405854530824\n",
      "grad=0.29934063959979756\n",
      "unweighted_step=0.29934063959979756\n",
      "learning_rate=0.0004889036782403997\n",
      "loss_grad=-1.5152405854530824\n",
      "grad=-0.2966830954392349\n",
      "unweighted_step=-0.2966830954392349\n",
      "learning_rate=0.0004889036782403997\n",
      "loss_grad=-1.5152405854530824\n",
      "grad=-0.05747850995158782\n",
      "unweighted_step=-0.05747850995158782\n",
      "learning_rate=0.000546394865675978\n",
      "loss_grad=-1.5152405854530824\n",
      "grad=0.300744806061968\n",
      "unweighted_step=0.300744806061968\n",
      "learning_rate=0.0004889036782403997\n",
      "pred_val=0.3102551497216561\n",
      "loss_grad=0.6205102994433122\n",
      "grad=0.10928916905242801\n",
      "unweighted_step=0.10928916905242801\n",
      "learning_rate=0.0006081086364607248\n",
      "loss_grad=0.6205102994433122\n",
      "grad=-0.0007959848061686365\n",
      "unweighted_step=-0.0007959848061686365\n",
      "learning_rate=0.0006081086364607248\n",
      "loss_grad=0.6205102994433122\n",
      "grad=3.4934849811216915\n",
      "unweighted_step=3.4934849811216915\n",
      "learning_rate=0.0006081086364607248\n",
      "loss_grad=0.6205102994433122\n",
      "grad=0.3235079347063194\n",
      "unweighted_step=0.3235079347063194\n",
      "learning_rate=0.0005574979859938778\n",
      "loss_grad=0.6205102994433122\n",
      "grad=-0.3009285157805597\n",
      "unweighted_step=-0.3009285157805597\n",
      "learning_rate=0.0005574979859938778\n",
      "loss_grad=0.6205102994433122\n",
      "grad=-0.21974140890998592\n",
      "unweighted_step=-0.21974140890998592\n",
      "learning_rate=0.00042048580998664065\n",
      "loss_grad=0.6205102994433122\n",
      "grad=0.2154535835624477\n",
      "unweighted_step=0.2154535835624477\n",
      "learning_rate=0.00042048580998664065\n",
      "loss_grad=0.6205102994433122\n",
      "grad=0.07688881823889548\n",
      "unweighted_step=0.07688881823889548\n",
      "learning_rate=0.0004750540516712141\n",
      "loss_grad=0.6205102994433122\n",
      "grad=-0.17930155969406358\n",
      "unweighted_step=-0.17930155969406358\n",
      "learning_rate=0.00042048580998664065\n",
      "pred_val=0.41516795365343095\n",
      "loss_grad=0.8303359073068619\n",
      "grad=0.09489346561689528\n",
      "unweighted_step=0.09489346561689528\n",
      "learning_rate=0.0005991711023018051\n",
      "key='mult_a', 0.61767578125, current_value=0.001244934482130196, updated=0.0011880770597352825, learning_rate=0.0005991711023018051, grad=0.09489346561689528\n",
      "loss_grad=0.8303359073068619\n",
      "grad=-0.0006334638446499935\n",
      "unweighted_step=-0.0006334638446499935\n",
      "learning_rate=0.0005991711023018051\n",
      "key='mult_b', -0.61767578125, current_value=0.9999989036654456, updated=0.9999992832186757, learning_rate=0.0005991711023018051, grad=-0.0006334638446499935\n",
      "loss_grad=0.8303359073068619\n",
      "grad=2.1270212684020597\n",
      "unweighted_step=2.1270212684020597\n",
      "learning_rate=0.0005991711023018051\n",
      "key='dif_a', 0.61767578125, current_value=-0.03253262939352406, updated=-0.03380707907153191, learning_rate=0.0005991711023018051, grad=2.1270212684020597\n",
      "loss_grad=0.8303359073068619\n",
      "grad=0.20648117740556732\n",
      "unweighted_step=0.20648117740556732\n",
      "learning_rate=0.0005413011450922002\n",
      "key='dif_b', 0.56982421875, current_value=1.000972560682769, updated=1.0008607921849995, learning_rate=0.0005413011450922002, grad=0.20648117740556732\n",
      "loss_grad=0.8303359073068619\n",
      "grad=-0.19476539620605438\n",
      "unweighted_step=-0.19476539620605438\n",
      "learning_rate=0.0005413011450922002\n",
      "key='add_norm_b', -0.56982421875, current_value=0.9990747941935297, updated=0.9991802209255204, learning_rate=0.0005413011450922002, grad=-0.19476539620605438\n",
      "loss_grad=0.8303359073068619\n",
      "grad=-0.06541154550740783\n",
      "unweighted_step=-0.06541154550740783\n",
      "learning_rate=0.0004120884127071223\n",
      "key='target_normalized_intensity_a', -0.60009765625, current_value=0.09932568763578627, updated=0.09935264297574714, learning_rate=0.0004120884127071223, grad=-0.06541154550740783\n",
      "loss_grad=0.8303359073068619\n",
      "grad=0.06440438127367812\n",
      "unweighted_step=0.06440438127367812\n",
      "learning_rate=0.0004120884127071223\n",
      "key='query_normalized_intensity_a', 0.60009765625, current_value=0.10067624801394263, updated=0.10064970771469219, learning_rate=0.0004120884127071223, grad=0.06440438127367812\n",
      "loss_grad=0.8303359073068619\n",
      "grad=0.019525930479723525\n",
      "unweighted_step=0.019525930479723525\n",
      "learning_rate=0.0004677937041041687\n",
      "key='target_normalized_intensity_b', 0.61572265625, current_value=0.10033015826550049, updated=0.10032102415815529, learning_rate=0.0004677937041041687, grad=0.019525930479723525\n",
      "loss_grad=0.8303359073068619\n",
      "grad=-0.06000566691833876\n",
      "unweighted_step=-0.06000566691833876\n",
      "learning_rate=0.0004120884127071223\n",
      "key='query_normalized_intensity_b', -0.60009765625, current_value=0.09958825504126986, updated=0.09961298268130367, learning_rate=0.0004120884127071223, grad=-0.06000566691833876\n",
      "pred_val=0.2065255166579759\n",
      "loss_grad=0.4130510333159518\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=0.0005136985253792112\n",
      "loss_grad=0.4130510333159518\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=0.0006247265689942184\n",
      "loss_grad=0.4130510333159518\n",
      "grad=2.694926640423896\n",
      "unweighted_step=2.694926640423896\n",
      "learning_rate=0.0006247265689942184\n",
      "loss_grad=0.4130510333159518\n",
      "grad=0.2525735480106049\n",
      "unweighted_step=0.2525735480106049\n",
      "learning_rate=0.0005605030631541867\n",
      "loss_grad=0.4130510333159518\n",
      "grad=-0.25257353187257586\n",
      "unweighted_step=-0.25257353187257586\n",
      "learning_rate=0.0005605030631541867\n",
      "loss_grad=0.4130510333159518\n",
      "grad=-0.0001927192073049831\n",
      "unweighted_step=-0.0001927192073049831\n",
      "learning_rate=0.00042857798566676514\n",
      "loss_grad=0.4130510333159518\n",
      "grad=-0.0013309966735623518\n",
      "unweighted_step=-0.0013309966735623518\n",
      "learning_rate=0.0003543899984767672\n",
      "loss_grad=0.4130510333159518\n",
      "grad=3.870785048436471e-05\n",
      "unweighted_step=3.870785048436471e-05\n",
      "learning_rate=0.0004876086962091671\n",
      "loss_grad=0.4130510333159518\n",
      "grad=0.0006903914986306143\n",
      "unweighted_step=0.0006903914986306143\n",
      "learning_rate=0.0003543899984767672\n",
      "pred_val=0.4540268117282192\n",
      "loss_grad=0.9080536234564384\n",
      "grad=0.15591395991627285\n",
      "unweighted_step=0.15591395991627285\n",
      "learning_rate=0.0004732836450561031\n",
      "loss_grad=0.9080536234564384\n",
      "grad=-0.0009849514991197505\n",
      "unweighted_step=-0.0009849514991197505\n",
      "learning_rate=0.0006692856185869091\n",
      "loss_grad=0.9080536234564384\n",
      "grad=1.1748345597519996\n",
      "unweighted_step=1.1748345597519996\n",
      "learning_rate=0.0006692856185869091\n",
      "loss_grad=0.9080536234564384\n",
      "grad=0.12646013803006517\n",
      "unweighted_step=0.12646013803006517\n",
      "learning_rate=0.0005984697562382026\n",
      "loss_grad=0.9080536234564384\n",
      "grad=-0.11813563713679263\n",
      "unweighted_step=-0.11813563713679263\n",
      "learning_rate=0.0005984697562382026\n",
      "loss_grad=0.9080536234564384\n",
      "grad=0.7202559530093411\n",
      "unweighted_step=0.7202559530093411\n",
      "learning_rate=0.00035571658910449664\n",
      "loss_grad=0.9080536234564384\n",
      "grad=-0.7097084725460647\n",
      "unweighted_step=-0.7097084725460647\n",
      "learning_rate=0.0003472996028773602\n",
      "loss_grad=0.9080536234564384\n",
      "grad=-0.27662311370013876\n",
      "unweighted_step=-0.27662311370013876\n",
      "learning_rate=0.00040414023005998315\n",
      "loss_grad=0.9080536234564384\n",
      "grad=0.2683678696060331\n",
      "unweighted_step=0.2683678696060331\n",
      "learning_rate=0.0003472996028773602\n",
      "pred_val=0.4871739327639518\n",
      "loss_grad=-1.0256521344720964\n",
      "grad=-0.04741595722922777\n",
      "unweighted_step=-0.04741595722922777\n",
      "learning_rate=0.00042090873191821126\n",
      "loss_grad=-1.0256521344720964\n",
      "grad=0.0002730334591959357\n",
      "unweighted_step=0.0002730334591959357\n",
      "learning_rate=0.0005450241486662841\n",
      "loss_grad=-1.0256521344720964\n",
      "grad=-0.36392607196724913\n",
      "unweighted_step=-0.36392607196724913\n",
      "learning_rate=0.0005450241486662841\n",
      "loss_grad=-1.0256521344720964\n",
      "grad=-0.04184236023044215\n",
      "unweighted_step=-0.04184236023044215\n",
      "learning_rate=0.000488430074800972\n",
      "loss_grad=-1.0256521344720964\n",
      "grad=0.032345417423191976\n",
      "unweighted_step=0.032345417423191976\n",
      "learning_rate=0.000488430074800972\n",
      "loss_grad=-1.0256521344720964\n",
      "grad=-0.04665013232776812\n",
      "unweighted_step=-0.04665013232776812\n",
      "learning_rate=0.0003325963134857602\n",
      "loss_grad=-1.0256521344720964\n",
      "grad=0.045457539457887546\n",
      "unweighted_step=0.045457539457887546\n",
      "learning_rate=0.0002986789303236614\n",
      "loss_grad=-1.0256521344720964\n",
      "grad=0.02667398697577274\n",
      "unweighted_step=0.02667398697577274\n",
      "learning_rate=0.00037810939602786036\n",
      "loss_grad=-1.0256521344720964\n",
      "grad=0.011660920788846818\n",
      "unweighted_step=0.011660920788846818\n",
      "learning_rate=0.00036119031514332295\n",
      "pred_val=0.22625855832173847\n",
      "loss_grad=0.45251711664347694\n",
      "grad=0.07466632815250118\n",
      "unweighted_step=0.07466632815250118\n",
      "learning_rate=0.00038106190033607374\n",
      "loss_grad=0.45251711664347694\n",
      "grad=-0.00046703201739100084\n",
      "unweighted_step=-0.00046703201739100084\n",
      "learning_rate=0.0005138658925109939\n",
      "loss_grad=0.45251711664347694\n",
      "grad=2.702171147393049\n",
      "unweighted_step=2.702171147393049\n",
      "learning_rate=0.0005138658925109939\n",
      "loss_grad=0.45251711664347694\n",
      "grad=0.2826350203519166\n",
      "unweighted_step=0.2826350203519166\n",
      "learning_rate=0.00046006899376692026\n",
      "loss_grad=0.45251711664347694\n",
      "grad=-0.2674981099359482\n",
      "unweighted_step=-0.2674981099359482\n",
      "learning_rate=0.00046006899376692026\n",
      "loss_grad=0.45251711664347694\n",
      "grad=-0.1411566070065499\n",
      "unweighted_step=-0.1411566070065499\n",
      "learning_rate=0.00033841735797379984\n",
      "loss_grad=0.45251711664347694\n",
      "grad=0.13827468561931605\n",
      "unweighted_step=0.13827468561931605\n",
      "learning_rate=0.0002927058986146436\n",
      "loss_grad=0.45251711664347694\n",
      "grad=0.03500925219849089\n",
      "unweighted_step=0.03500925219849089\n",
      "learning_rate=0.00038483777703444743\n",
      "loss_grad=0.45251711664347694\n",
      "grad=-0.10094377152429046\n",
      "unweighted_step=-0.10094377152429046\n",
      "learning_rate=0.0002997886229281777\n",
      "pred_val=0.43267726889845864\n",
      "loss_grad=0.8653545377969173\n",
      "grad=0.00798138686895808\n",
      "unweighted_step=0.00798138686895808\n",
      "learning_rate=0.00038207770102559877\n",
      "loss_grad=0.8653545377969173\n",
      "grad=-4.5092855098037437e-05\n",
      "unweighted_step=-4.5092855098037437e-05\n",
      "learning_rate=0.0005248706956747387\n",
      "loss_grad=0.8653545377969173\n",
      "grad=1.536282418153365\n",
      "unweighted_step=1.536282418153365\n",
      "learning_rate=0.0005248706956747387\n",
      "loss_grad=0.8653545377969173\n",
      "grad=0.15048509983503489\n",
      "unweighted_step=0.15048509983503489\n",
      "learning_rate=0.00046971530730963157\n",
      "loss_grad=0.8653545377969173\n",
      "grad=-0.14771243934370581\n",
      "unweighted_step=-0.14771243934370581\n",
      "learning_rate=0.00046971530730963157\n",
      "loss_grad=0.8653545377969173\n",
      "grad=0.03705837266500258\n",
      "unweighted_step=0.03705837266500258\n",
      "learning_rate=0.0002846932925651143\n",
      "loss_grad=0.8653545377969173\n",
      "grad=-0.03694272380982558\n",
      "unweighted_step=-0.03694272380982558\n",
      "learning_rate=0.0002517268048283397\n",
      "loss_grad=0.8653545377969173\n",
      "grad=-0.0184445537931652\n",
      "unweighted_step=-0.0184445537931652\n",
      "learning_rate=0.00032368805487947584\n",
      "loss_grad=0.8653545377969173\n",
      "grad=0.00014365694407667532\n",
      "unweighted_step=0.00014365694407667532\n",
      "learning_rate=0.0002803020879731645\n",
      "pred_val=0.3348821107189799\n",
      "loss_grad=-1.3302357785620402\n",
      "grad=-0.25646496230290367\n",
      "unweighted_step=-0.25646496230290367\n",
      "learning_rate=0.0003242567916117402\n",
      "loss_grad=-1.3302357785620402\n",
      "grad=0.001585051041834279\n",
      "unweighted_step=0.001585051041834279\n",
      "learning_rate=0.0004405198519026028\n",
      "loss_grad=-1.3302357785620402\n",
      "grad=-5.319442653656537\n",
      "unweighted_step=-5.319442653656537\n",
      "learning_rate=0.0004405198519026028\n",
      "loss_grad=-1.3302357785620402\n",
      "grad=-0.6415883645686788\n",
      "unweighted_step=-0.6415883645686788\n",
      "learning_rate=0.0003943337268215993\n",
      "loss_grad=-1.3302357785620402\n",
      "grad=0.6056015162009232\n",
      "unweighted_step=0.6056015162009232\n",
      "learning_rate=0.0003943337268215993\n",
      "loss_grad=-1.3302357785620402\n",
      "grad=-0.5132982841857568\n",
      "unweighted_step=-0.5132982841857568\n",
      "learning_rate=0.0002645869590999501\n",
      "loss_grad=-1.3302357785620402\n",
      "grad=0.5036321741880548\n",
      "unweighted_step=0.5036321741880548\n",
      "learning_rate=0.000231588775673459\n",
      "loss_grad=-1.3302357785620402\n",
      "grad=0.27670019131364826\n",
      "unweighted_step=0.27670019131364826\n",
      "learning_rate=0.00030085144179745245\n",
      "loss_grad=-1.3302357785620402\n",
      "grad=-0.17403089898954607\n",
      "unweighted_step=-0.17403089898954607\n",
      "learning_rate=0.00024736672094843073\n",
      "pred_val=0.4327544776111703\n",
      "loss_grad=-1.1344910447776595\n",
      "grad=-0.1897082131167623\n",
      "unweighted_step=-0.1897082131167623\n",
      "learning_rate=0.0003159342781420393\n",
      "loss_grad=-1.1344910447776595\n",
      "grad=0.0012440100839737137\n",
      "unweighted_step=0.0012440100839737137\n",
      "learning_rate=0.0004271483442061681\n",
      "loss_grad=-1.1344910447776595\n",
      "grad=-2.103832954673404\n",
      "unweighted_step=-2.103832954673404\n",
      "learning_rate=0.0004271483442061681\n",
      "loss_grad=-1.1344910447776595\n",
      "grad=-0.23549668026977955\n",
      "unweighted_step=-0.23549668026977955\n",
      "learning_rate=0.000382408374777976\n",
      "loss_grad=-1.1344910447776595\n",
      "grad=0.21566911296026037\n",
      "unweighted_step=0.21566911296026037\n",
      "learning_rate=0.000382408374777976\n",
      "loss_grad=-1.1344910447776595\n",
      "grad=-0.20151957174050095\n",
      "unweighted_step=-0.20151957174050095\n",
      "learning_rate=0.00026847314062087955\n",
      "loss_grad=-1.1344910447776595\n",
      "grad=0.19742347368800384\n",
      "unweighted_step=0.19742347368800384\n",
      "learning_rate=0.00023390471643665775\n",
      "loss_grad=-1.1344910447776595\n",
      "grad=0.1535727106858133\n",
      "unweighted_step=0.1535727106858133\n",
      "learning_rate=0.0003052812837171378\n",
      "loss_grad=-1.1344910447776595\n",
      "grad=-0.09872320706723117\n",
      "unweighted_step=-0.09872320706723117\n",
      "learning_rate=0.00024520231875788123\n",
      "pred_val=0.316277647215669\n",
      "loss_grad=0.632555294431338\n",
      "grad=0.004563172717863825\n",
      "unweighted_step=0.004563172717863825\n",
      "learning_rate=0.0002725985885608226\n",
      "loss_grad=0.632555294431338\n",
      "grad=-3.07090769744222e-05\n",
      "unweighted_step=-3.07090769744222e-05\n",
      "learning_rate=0.00036955890753511783\n",
      "loss_grad=0.632555294431338\n",
      "grad=3.0104162421400855\n",
      "unweighted_step=3.0104162421400855\n",
      "learning_rate=0.00036955890753511783\n",
      "loss_grad=0.632555294431338\n",
      "grad=0.2779879875839867\n",
      "unweighted_step=0.2779879875839867\n",
      "learning_rate=0.0003308294726733847\n",
      "loss_grad=0.632555294431338\n",
      "grad=-0.2768888414702118\n",
      "unweighted_step=-0.2768888414702118\n",
      "learning_rate=0.0003308294726733847\n",
      "loss_grad=0.632555294431338\n",
      "grad=0.02239121911755923\n",
      "unweighted_step=0.02239121911755923\n",
      "learning_rate=0.00022623053917700036\n",
      "loss_grad=0.632555294431338\n",
      "grad=-0.023666828520310452\n",
      "unweighted_step=-0.023666828520310452\n",
      "learning_rate=0.00019764945862070534\n",
      "loss_grad=0.632555294431338\n",
      "grad=-0.01109032758264118\n",
      "unweighted_step=-0.01109032758264118\n",
      "learning_rate=0.0002572415569283435\n",
      "loss_grad=0.632555294431338\n",
      "grad=0.00730966419195634\n",
      "unweighted_step=0.00730966419195634\n",
      "learning_rate=0.00020949470302758632\n",
      "pred_val=0.40123359386595225\n",
      "loss_grad=0.8024671877319045\n",
      "grad=0.036962659333909534\n",
      "unweighted_step=0.036962659333909534\n",
      "learning_rate=0.00026753278183193026\n",
      "loss_grad=0.8024671877319045\n",
      "grad=-0.00023461055625302414\n",
      "unweighted_step=-0.00023461055625302414\n",
      "learning_rate=0.00036312432592869076\n",
      "loss_grad=0.8024671877319045\n",
      "grad=2.1365810967026397\n",
      "unweighted_step=2.1365810967026397\n",
      "learning_rate=0.00036312432592869076\n",
      "loss_grad=0.8024671877319045\n",
      "grad=0.22402223880458527\n",
      "unweighted_step=0.22402223880458527\n",
      "learning_rate=0.00032505995351794304\n",
      "loss_grad=0.8024671877319045\n",
      "grad=-0.22091628284549733\n",
      "unweighted_step=-0.22091628284549733\n",
      "learning_rate=0.00032505995351794304\n",
      "loss_grad=0.8024671877319045\n",
      "grad=-0.10435022407869905\n",
      "unweighted_step=-0.10435022407869905\n",
      "learning_rate=0.0002100939519447812\n",
      "loss_grad=0.8024671877319045\n",
      "grad=0.10234402510488817\n",
      "unweighted_step=0.10234402510488817\n",
      "learning_rate=0.00018331988418029694\n",
      "loss_grad=0.8024671877319045\n",
      "grad=0.03937678420667654\n",
      "unweighted_step=0.03937678420667654\n",
      "learning_rate=0.00023889536883683717\n",
      "loss_grad=0.8024671877319045\n",
      "grad=-0.04936611517217969\n",
      "unweighted_step=-0.04936611517217969\n",
      "learning_rate=0.0001933243426250276\n",
      "pred_val=0.38263015997278904\n",
      "loss_grad=0.7652603199455781\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=0.0002298886979826889\n",
      "key='mult_a', -0.19763898849487305, current_value=0.0012347045157462939, updated=0.0012347045157462939, learning_rate=0.0002298886979826889, grad=0.0\n",
      "loss_grad=0.7652603199455781\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=0.00037811926933895495\n",
      "key='mult_b', -0.804314136505127, current_value=0.9999989242007785, updated=0.9999989242007785, learning_rate=0.00037811926933895495, grad=0.0\n",
      "loss_grad=0.7652603199455781\n",
      "grad=2.3424137184905383\n",
      "unweighted_step=2.3424137184905383\n",
      "learning_rate=0.00037811926933895495\n",
      "key='dif_a', 0.804314136505127, current_value=-0.036919927787881245, updated=-0.03780563955160643, learning_rate=0.00037811926933895495, grad=2.3424137184905383\n",
      "loss_grad=0.7652603199455781\n",
      "grad=0.22878882024290972\n",
      "unweighted_step=0.22878882024290972\n",
      "learning_rate=0.000338478500542673\n",
      "key='dif_b', 0.804267406463623, current_value=1.000641530228272, updated=1.0005640901314554, learning_rate=0.000338478500542673, grad=0.22878882024290972\n",
      "loss_grad=0.7652603199455781\n",
      "grad=-0.22878884179274245\n",
      "unweighted_step=-0.22878884179274245\n",
      "learning_rate=0.000338478500542673\n",
      "key='add_norm_b', -0.804267406463623, current_value=0.9994112729234446, updated=0.9994887130275555, learning_rate=0.000338478500542673, grad=-0.22878884179274245\n",
      "loss_grad=0.7652603199455781\n",
      "grad=-8.241445212733874e-05\n",
      "unweighted_step=-8.241445212733874e-05\n",
      "learning_rate=0.0002131058517198258\n",
      "key='target_normalized_intensity_a', -0.7144532203674316, current_value=0.09935602621581345, updated=0.09935604377881546, learning_rate=0.0002131058517198258, grad=-8.241445212733874e-05\n",
      "loss_grad=0.7652603199455781\n",
      "grad=-0.0009755688403194708\n",
      "unweighted_step=-0.0009755688403194708\n",
      "learning_rate=0.00016246724210996423\n",
      "key='query_normalized_intensity_a', -0.28749990463256836, current_value=0.1006750115265059, updated=0.10067517002448488, learning_rate=0.00016246724210996423, grad=-0.0009755688403194708\n",
      "loss_grad=0.7652603199455781\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=0.00021157994246540546\n",
      "key='target_normalized_intensity_b', -0.28553152084350586, current_value=0.10027852896250823, updated=0.10027852896250823, learning_rate=0.00021157994246540546, grad=0.0\n",
      "loss_grad=0.7652603199455781\n",
      "grad=0.0003723880121800618\n",
      "unweighted_step=0.0003723880121800618\n",
      "learning_rate=0.0001717867970484043\n",
      "key='query_normalized_intensity_b', 0.29531240463256836, current_value=0.09962081262213411, updated=0.09962074865079024, learning_rate=0.0001717867970484043, grad=0.0003723880121800618\n",
      "pred_val=0.49033356515521836\n",
      "loss_grad=-1.0193328696895634\n",
      "grad=-0.042388804649720564\n",
      "unweighted_step=-0.042388804649720564\n",
      "learning_rate=0.00022520950854390976\n",
      "loss_grad=-1.0193328696895634\n",
      "grad=0.0002597260515155732\n",
      "unweighted_step=0.0002597260515155732\n",
      "learning_rate=0.00031359430482986057\n",
      "loss_grad=-1.0193328696895634\n",
      "grad=-0.2619506204833923\n",
      "unweighted_step=-0.2619506204833923\n",
      "learning_rate=0.00031359430482986057\n",
      "loss_grad=-1.0193328696895634\n",
      "grad=-0.028433683786332237\n",
      "unweighted_step=-0.028433683786332237\n",
      "learning_rate=0.0002807204916492666\n",
      "loss_grad=-1.0193328696895634\n",
      "grad=0.026885548389772144\n",
      "unweighted_step=0.026885548389772144\n",
      "learning_rate=0.0002807204916492666\n",
      "loss_grad=-1.0193328696895634\n",
      "grad=-0.0967017953177326\n",
      "unweighted_step=-0.0967017953177326\n",
      "learning_rate=0.00022528868343989062\n",
      "loss_grad=-1.0193328696895634\n",
      "grad=0.09502038237232767\n",
      "unweighted_step=0.09502038237232767\n",
      "learning_rate=0.00014733748251258635\n",
      "loss_grad=-1.0193328696895634\n",
      "grad=0.014541090746798161\n",
      "unweighted_step=0.014541090746798161\n",
      "learning_rate=0.0001919390339293159\n",
      "loss_grad=-1.0193328696895634\n",
      "grad=-0.012835141158297536\n",
      "unweighted_step=-0.012835141158297536\n",
      "learning_rate=0.0001555878413779104\n",
      "pred_val=0.3560446896226692\n",
      "loss_grad=0.7120893792453384\n",
      "grad=0.03857092656803347\n",
      "unweighted_step=0.03857092656803347\n",
      "learning_rate=0.00019372005651581762\n",
      "loss_grad=0.7120893792453384\n",
      "grad=-0.00025724779801036607\n",
      "unweighted_step=-0.00025724779801036607\n",
      "learning_rate=0.00029331214166378964\n",
      "loss_grad=0.7120893792453384\n",
      "grad=2.565903175427798\n",
      "unweighted_step=2.565903175427798\n",
      "learning_rate=0.00029331214166378964\n",
      "loss_grad=0.7120893792453384\n",
      "grad=0.2687908446725454\n",
      "unweighted_step=0.2687908446725454\n",
      "learning_rate=0.00026256350582510446\n",
      "loss_grad=0.7120893792453384\n",
      "grad=-0.259678157486262\n",
      "unweighted_step=-0.259678157486262\n",
      "learning_rate=0.00026256350582510446\n",
      "loss_grad=0.7120893792453384\n",
      "grad=-0.08947915299849554\n",
      "unweighted_step=-0.08947915299849554\n",
      "learning_rate=0.00024299276743058556\n",
      "loss_grad=0.7120893792453384\n",
      "grad=0.08776630194446296\n",
      "unweighted_step=0.08776630194446296\n",
      "learning_rate=0.00014784395616256318\n",
      "loss_grad=0.7120893792453384\n",
      "grad=0.017613908597605688\n",
      "unweighted_step=0.017613908597605688\n",
      "learning_rate=0.00019262716145751848\n",
      "loss_grad=0.7120893792453384\n",
      "grad=-0.06448917353813806\n",
      "unweighted_step=-0.06448917353813806\n",
      "learning_rate=0.00015603151094469058\n",
      "pred_val=0.3424501701308589\n",
      "loss_grad=-1.3150996597382822\n",
      "grad=-0.31705806994994623\n",
      "unweighted_step=-0.31705806994994623\n",
      "learning_rate=0.00017820530042146943\n",
      "loss_grad=-1.3150996597382822\n",
      "grad=0.0021159892454217346\n",
      "unweighted_step=0.0021159892454217346\n",
      "learning_rate=0.0002588005129451288\n",
      "loss_grad=-1.3150996597382822\n",
      "grad=-5.031348876517686\n",
      "unweighted_step=-5.031348876517686\n",
      "learning_rate=0.0002588005129451288\n",
      "loss_grad=-1.3150996597382822\n",
      "grad=-0.575044572468816\n",
      "unweighted_step=-0.575044572468816\n",
      "learning_rate=0.0002316702764455846\n",
      "loss_grad=-1.3150996597382822\n",
      "grad=0.53746114560878\n",
      "unweighted_step=0.53746114560878\n",
      "learning_rate=0.0002316702764455846\n",
      "loss_grad=-1.3150996597382822\n",
      "grad=0.570011971372699\n",
      "unweighted_step=0.570011971372699\n",
      "learning_rate=0.0001969961815274863\n",
      "loss_grad=-1.3150996597382822\n",
      "grad=-0.5650068702503044\n",
      "unweighted_step=-0.5650068702503044\n",
      "learning_rate=0.00012541325540979311\n",
      "loss_grad=-1.3150996597382822\n",
      "grad=-0.22424174015512066\n",
      "unweighted_step=-0.22424174015512066\n",
      "learning_rate=0.0001633877899594242\n",
      "loss_grad=-1.3150996597382822\n",
      "grad=0.3513159062778576\n",
      "unweighted_step=0.3513159062778576\n",
      "learning_rate=0.00013240431694226071\n",
      "pred_val=0.3984140584184279\n",
      "loss_grad=0.7968281168368558\n",
      "grad=0.005177638679008844\n",
      "unweighted_step=0.005177638679008844\n",
      "learning_rate=0.0001586106063394435\n",
      "loss_grad=0.7968281168368558\n",
      "grad=-3.891747438503446e-05\n",
      "unweighted_step=-3.891747438503446e-05\n",
      "learning_rate=0.0002352059005277691\n",
      "loss_grad=0.7968281168368558\n",
      "grad=2.117258171645527\n",
      "unweighted_step=2.117258171645527\n",
      "learning_rate=0.0002352059005277691\n",
      "loss_grad=0.7968281168368558\n",
      "grad=0.21017258411490017\n",
      "unweighted_step=0.21017258411490017\n",
      "learning_rate=0.00021054890055558376\n",
      "loss_grad=0.7968281168368558\n",
      "grad=-0.20834470738418254\n",
      "unweighted_step=-0.20834470738418254\n",
      "learning_rate=0.00021054890055558376\n",
      "loss_grad=0.7968281168368558\n",
      "grad=0.03479257467293043\n",
      "unweighted_step=0.03479257467293043\n",
      "learning_rate=0.00018820109042426986\n",
      "loss_grad=0.7968281168368558\n",
      "grad=-0.035176837227560785\n",
      "unweighted_step=-0.035176837227560785\n",
      "learning_rate=0.00012217014678392413\n",
      "loss_grad=0.7968281168368558\n",
      "grad=-0.02490752119933548\n",
      "unweighted_step=-0.02490752119933548\n",
      "learning_rate=0.00015915665335108673\n",
      "loss_grad=0.7968281168368558\n",
      "grad=0.017319788568840637\n",
      "unweighted_step=0.017319788568840637\n",
      "learning_rate=0.00012899981898568916\n",
      "pred_val=0.4577387131086091\n",
      "loss_grad=-1.084522573782782\n",
      "grad=-0.10807216465685436\n",
      "unweighted_step=-0.10807216465685436\n",
      "learning_rate=0.00014353908797344885\n",
      "loss_grad=-1.084522573782782\n",
      "grad=0.0007725542305196422\n",
      "unweighted_step=0.0007725542305196422\n",
      "learning_rate=0.0002106467717505424\n",
      "loss_grad=-1.084522573782782\n",
      "grad=-1.2145471722846313\n",
      "unweighted_step=-1.2145471722846313\n",
      "learning_rate=0.0002106467717505424\n",
      "loss_grad=-1.084522573782782\n",
      "grad=-0.14570121484779533\n",
      "unweighted_step=-0.14570121484779533\n",
      "learning_rate=0.00018856443564379028\n",
      "loss_grad=-1.084522573782782\n",
      "grad=0.12978723945666867\n",
      "unweighted_step=0.12978723945666867\n",
      "learning_rate=0.00018856443564379028\n",
      "loss_grad=-1.084522573782782\n",
      "grad=-0.45731964103093276\n",
      "unweighted_step=-0.45731964103093276\n",
      "learning_rate=0.000164172139602855\n",
      "loss_grad=-1.084522573782782\n",
      "grad=0.4498918077546341\n",
      "unweighted_step=0.4498918077546341\n",
      "learning_rate=0.0001054242466953094\n",
      "loss_grad=-1.084522573782782\n",
      "grad=0.24909841196521507\n",
      "unweighted_step=0.24909841196521507\n",
      "learning_rate=0.00013734393829499978\n",
      "loss_grad=-1.084522573782782\n",
      "grad=-0.20381040551570495\n",
      "unweighted_step=-0.20381040551570495\n",
      "learning_rate=0.00011130832537128254\n",
      "pred_val=0.4480073316629518\n",
      "loss_grad=-1.1039853366740964\n",
      "grad=-0.11340378161521143\n",
      "unweighted_step=-0.11340378161521143\n",
      "learning_rate=0.00014389634711164118\n",
      "loss_grad=-1.1039853366740964\n",
      "grad=0.000787238351608897\n",
      "unweighted_step=0.000787238351608897\n",
      "learning_rate=0.00021018172212986452\n",
      "loss_grad=-1.1039853366740964\n",
      "grad=-1.5269559655393117\n",
      "unweighted_step=-1.5269559655393117\n",
      "learning_rate=0.00021018172212986452\n",
      "loss_grad=-1.1039853366740964\n",
      "grad=-0.1746127899502752\n",
      "unweighted_step=-0.1746127899502752\n",
      "learning_rate=0.0001881481789982032\n",
      "loss_grad=-1.1039853366740964\n",
      "grad=0.15723428734909392\n",
      "unweighted_step=0.15723428734909392\n",
      "learning_rate=0.0001881481789982032\n",
      "loss_grad=-1.1039853366740964\n",
      "grad=-0.0011625713686272774\n",
      "unweighted_step=-0.0011625713686272774\n",
      "learning_rate=0.0001619002433620272\n",
      "loss_grad=-1.1039853366740964\n",
      "grad=0.00033936889472028706\n",
      "unweighted_step=0.00033936889472028706\n",
      "learning_rate=0.00010347019185740161\n",
      "loss_grad=-1.1039853366740964\n",
      "grad=0.0018753473710418475\n",
      "unweighted_step=0.0018753473710418475\n",
      "learning_rate=0.0001347995142450232\n",
      "loss_grad=-1.1039853366740964\n",
      "grad=0.06052405027422516\n",
      "unweighted_step=0.06052405027422516\n",
      "learning_rate=0.00010224468621969673\n",
      "pred_val=0.47086672771174853\n",
      "loss_grad=0.9417334554234971\n",
      "grad=0.04313391922782617\n",
      "unweighted_step=0.04313391922782617\n",
      "learning_rate=0.00012213282087881268\n",
      "loss_grad=0.9417334554234971\n",
      "grad=-0.00029620774694633073\n",
      "unweighted_step=-0.00029620774694633073\n",
      "learning_rate=0.00017888647527046476\n",
      "loss_grad=0.9417334554234971\n",
      "grad=0.7394689034556887\n",
      "unweighted_step=0.7394689034556887\n",
      "learning_rate=0.00017888647527046476\n",
      "loss_grad=0.9417334554234971\n",
      "grad=0.0772880927051285\n",
      "unweighted_step=0.0772880927051285\n",
      "learning_rate=0.00016013362102727197\n",
      "loss_grad=0.9417334554234971\n",
      "grad=-0.07273453435930005\n",
      "unweighted_step=-0.07273453435930005\n",
      "learning_rate=0.00016013362102727197\n",
      "loss_grad=0.9417334554234971\n",
      "grad=-0.1304119564417398\n",
      "unweighted_step=-0.1304119564417398\n",
      "learning_rate=0.00016887502722838052\n",
      "loss_grad=0.9417334554234971\n",
      "grad=0.12893315289352017\n",
      "unweighted_step=0.12893315289352017\n",
      "learning_rate=0.00010768478338675188\n",
      "loss_grad=0.9417334554234971\n",
      "grad=0.01548765088176275\n",
      "unweighted_step=0.01548765088176275\n",
      "learning_rate=0.00014029084684193897\n",
      "loss_grad=0.9417334554234971\n",
      "grad=-0.0314502174112589\n",
      "unweighted_step=-0.0314502174112589\n",
      "learning_rate=9.107078484705363e-05\n",
      "pred_val=0.458557482065596\n",
      "loss_grad=-1.082885035868808\n",
      "grad=-0.11532685259646287\n",
      "unweighted_step=-0.11532685259646287\n",
      "learning_rate=0.00011304885440759718\n",
      "loss_grad=-1.082885035868808\n",
      "grad=0.0008422678736327426\n",
      "unweighted_step=0.0008422678736327426\n",
      "learning_rate=0.00016537125670041493\n",
      "loss_grad=-1.082885035868808\n",
      "grad=-1.204050726081807\n",
      "unweighted_step=-1.204050726081807\n",
      "learning_rate=0.00016537125670041493\n",
      "loss_grad=-1.082885035868808\n",
      "grad=-0.14397794958872348\n",
      "unweighted_step=-0.14397794958872348\n",
      "learning_rate=0.00014803522556929436\n",
      "loss_grad=-1.082885035868808\n",
      "grad=0.12935342741754607\n",
      "unweighted_step=0.12935342741754607\n",
      "learning_rate=0.00014803522556929436\n",
      "loss_grad=-1.082885035868808\n",
      "grad=0.10044493061725293\n",
      "unweighted_step=0.10044493061725293\n",
      "learning_rate=0.0001399061417518449\n",
      "loss_grad=-1.082885035868808\n",
      "grad=-0.10013612135932821\n",
      "unweighted_step=-0.10013612135932821\n",
      "learning_rate=8.933893485349081e-05\n",
      "loss_grad=-1.082885035868808\n",
      "grad=-0.03303416925569968\n",
      "unweighted_step=-0.03303416925569968\n",
      "learning_rate=0.00011638970321172423\n",
      "loss_grad=-1.082885035868808\n",
      "grad=0.08430717096799117\n",
      "unweighted_step=0.08430717096799117\n",
      "learning_rate=8.238654293684532e-05\n",
      "pred_val=0.09507658664158797\n",
      "loss_grad=0.19015317328317594\n",
      "grad=0.008159230197967517\n",
      "unweighted_step=0.008159230197967517\n",
      "learning_rate=0.00010029568691211078\n",
      "loss_grad=0.19015317328317594\n",
      "grad=-6.127397473681398e-05\n",
      "unweighted_step=-6.127397473681398e-05\n",
      "learning_rate=0.00014681262705245945\n",
      "loss_grad=0.19015317328317594\n",
      "grad=0.9958088343182615\n",
      "unweighted_step=0.9958088343182615\n",
      "learning_rate=0.00014681262705245945\n",
      "loss_grad=0.19015317328317594\n",
      "grad=0.10605741418141522\n",
      "unweighted_step=0.10605741418141522\n",
      "learning_rate=0.00013142211122697383\n",
      "loss_grad=0.19015317328317594\n",
      "grad=-0.10372385378289427\n",
      "unweighted_step=-0.10372385378289427\n",
      "learning_rate=0.00013142211122697383\n",
      "loss_grad=0.19015317328317594\n",
      "grad=-0.014593794032901179\n",
      "unweighted_step=-0.014593794032901179\n",
      "learning_rate=0.000130919996490265\n",
      "loss_grad=0.19015317328317594\n",
      "grad=0.014053621373556955\n",
      "unweighted_step=0.014053621373556955\n",
      "learning_rate=8.354826254055008e-05\n",
      "loss_grad=0.19015317328317594\n",
      "grad=0.0010201242793530985\n",
      "unweighted_step=0.0010201242793530985\n",
      "learning_rate=0.00010884581833868002\n",
      "loss_grad=0.19015317328317594\n",
      "grad=-0.013358495287606336\n",
      "unweighted_step=-0.013358495287606336\n",
      "learning_rate=7.395663058132146e-05\n",
      "pred_val=0.45380604941074676\n",
      "loss_grad=0.9076120988214935\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=9.090856809039408e-05\n",
      "key='mult_a', -0.354685194324702, current_value=0.0013312412513442598, updated=0.0013312412513442598, learning_rate=9.090856809039408e-05, grad=0.0\n",
      "loss_grad=0.9076120988214935\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=0.00014591530572976975\n",
      "key='mult_b', -0.6462932755239308, current_value=0.9999979742378773, updated=0.9999979742378773, learning_rate=0.00014591530572976975, grad=0.0\n",
      "loss_grad=0.9076120988214935\n",
      "grad=1.121390576304241\n",
      "unweighted_step=1.121390576304241\n",
      "learning_rate=0.00014591530572976975\n",
      "key='dif_a', 0.6462932755239308, current_value=-0.0371745640690082, updated=-0.037338192117792116, learning_rate=0.00014591530572976975, grad=1.121390576304241\n",
      "loss_grad=0.9076120988214935\n",
      "grad=0.1042594228412049\n",
      "unweighted_step=0.1042594228412049\n",
      "learning_rate=0.00013061885520469768\n",
      "key='dif_b', 0.6462932298891246, current_value=1.0006457927538204, updated=1.0006321745073645, learning_rate=0.00013061885520469768, grad=0.1042594228412049\n",
      "loss_grad=0.9076120988214935\n",
      "grad=-0.1042594228412049\n",
      "unweighted_step=-0.1042594228412049\n",
      "learning_rate=0.00013061885520469768\n",
      "key='add_norm_b', -0.6462932298891246, current_value=0.999420774073504, updated=0.9994343923199599, learning_rate=0.00013061885520469768, grad=-0.1042594228412049\n",
      "loss_grad=0.9076120988214935\n",
      "grad=-0.00020553904199764833\n",
      "unweighted_step=-0.00020553904199764833\n",
      "learning_rate=0.0001332615128969102\n",
      "key='target_normalized_intensity_a', -0.7262836457230151, current_value=0.09936588264770116, updated=0.09936591003814485, learning_rate=0.0001332615128969102, grad=-0.00020553904199764833\n",
      "loss_grad=0.9076120988214935\n",
      "grad=-0.0003043556473624604\n",
      "unweighted_step=-0.0003043556473624604\n",
      "learning_rate=7.372369261781328e-05\n",
      "key='query_normalized_intensity_a', -0.27469482412561774, current_value=0.10066977429419814, updated=0.10066979673242034, learning_rate=7.372369261781328e-05, grad=-0.0003043556473624604\n",
      "loss_grad=0.9076120988214935\n",
      "grad=4.8500662209456846e-05\n",
      "unweighted_step=4.8500662209456846e-05\n",
      "learning_rate=0.0001107606480635978\n",
      "key='target_normalized_intensity_b', 0.7253070981241763, current_value=0.10028004367483939, updated=0.10028003830287462, learning_rate=0.0001107606480635978, grad=4.8500662209456846e-05\n",
      "loss_grad=0.9076120988214935\n",
      "grad=5.364023821935849e-05\n",
      "unweighted_step=5.364023821935849e-05\n",
      "learning_rate=6.664681165088864e-05\n",
      "key='query_normalized_intensity_b', 0.337202453520149, current_value=0.09959746189501732, updated=0.09959745832006647, learning_rate=6.664681165088864e-05, grad=5.364023821935849e-05\n",
      "pred_val=0.2888032273659302\n",
      "loss_grad=-1.4223935452681395\n",
      "grad=-0.3042152783111853\n",
      "unweighted_step=-0.3042152783111853\n",
      "learning_rate=9.119972815671265e-05\n",
      "loss_grad=-1.4223935452681395\n",
      "grad=0.002228438407806932\n",
      "unweighted_step=0.002228438407806932\n",
      "learning_rate=0.00012447392830990597\n",
      "loss_grad=-1.4223935452681395\n",
      "grad=-7.062340637260275\n",
      "unweighted_step=-7.062340637260275\n",
      "learning_rate=0.00012447392830990597\n",
      "loss_grad=-1.4223935452681395\n",
      "grad=-0.7873266353518537\n",
      "unweighted_step=-0.7873266353518537\n",
      "learning_rate=0.00011142520017226322\n",
      "loss_grad=-1.4223935452681395\n",
      "grad=0.7460633498364074\n",
      "unweighted_step=0.7460633498364074\n",
      "learning_rate=0.00011142520017226322\n",
      "loss_grad=-1.4223935452681395\n",
      "grad=0.597590094519226\n",
      "unweighted_step=0.597590094519226\n",
      "learning_rate=0.00011208058863886481\n",
      "loss_grad=-1.4223935452681395\n",
      "grad=-0.5907380024680888\n",
      "unweighted_step=-0.5907380024680888\n",
      "learning_rate=7.307523550355382e-05\n",
      "loss_grad=-1.4223935452681395\n",
      "grad=-0.2993295789800256\n",
      "unweighted_step=-0.2993295789800256\n",
      "learning_rate=9.317229302541372e-05\n",
      "loss_grad=-1.4223935452681395\n",
      "grad=0.44594207323659957\n",
      "unweighted_step=0.44594207323659957\n",
      "learning_rate=6.668549132954044e-05\n",
      "pred_val=0.4715661128190184\n",
      "loss_grad=-1.0568677743619632\n",
      "grad=-0.02109956912622417\n",
      "unweighted_step=-0.02109956912622417\n",
      "learning_rate=9.590576085840223e-05\n",
      "loss_grad=-1.0568677743619632\n",
      "grad=0.0001429912490530878\n",
      "unweighted_step=0.0001429912490530878\n",
      "learning_rate=0.00012155227680429812\n",
      "loss_grad=-1.0568677743619632\n",
      "grad=-0.8232427147522304\n",
      "unweighted_step=-0.8232427147522304\n",
      "learning_rate=0.00012155227680429812\n",
      "loss_grad=-1.0568677743619632\n",
      "grad=-0.08292284868321691\n",
      "unweighted_step=-0.08292284868321691\n",
      "learning_rate=0.00010880982873829173\n",
      "loss_grad=-1.0568677743619632\n",
      "grad=0.08008219823104266\n",
      "unweighted_step=0.08008219823104266\n",
      "learning_rate=0.00010880982873829173\n",
      "loss_grad=-1.0568677743619632\n",
      "grad=0.09527977743766637\n",
      "unweighted_step=0.09527977743766637\n",
      "learning_rate=0.00010877743096498022\n",
      "loss_grad=-1.0568677743619632\n",
      "grad=-0.09381904259496951\n",
      "unweighted_step=-0.09381904259496951\n",
      "learning_rate=7.640762056348668e-05\n",
      "loss_grad=-1.0568677743619632\n",
      "grad=-0.015593787315094225\n",
      "unweighted_step=-0.015593787315094225\n",
      "learning_rate=9.04332109400612e-05\n",
      "loss_grad=-1.0568677743619632\n",
      "grad=0.030009579560539523\n",
      "unweighted_step=0.030009579560539523\n",
      "learning_rate=7.003911695956776e-05\n",
      "pred_val=0.0918454347674898\n",
      "loss_grad=-1.8163091304650205\n",
      "grad=-0.3839551989270014\n",
      "unweighted_step=-0.3839551989270014\n",
      "learning_rate=0.00010317548416556189\n",
      "loss_grad=-1.8163091304650205\n",
      "grad=0.0029496167986071044\n",
      "unweighted_step=0.0029496167986071044\n",
      "learning_rate=0.0001262033533874452\n",
      "loss_grad=-1.8163091304650205\n",
      "grad=-9.561614509433282\n",
      "unweighted_step=-9.561614509433282\n",
      "learning_rate=0.0001262033533874452\n",
      "loss_grad=-1.8163091304650205\n",
      "grad=-1.056219160047213\n",
      "unweighted_step=-1.056219160047213\n",
      "learning_rate=0.00011297332844722422\n",
      "loss_grad=-1.8163091304650205\n",
      "grad=0.9927861611229583\n",
      "unweighted_step=0.9927861611229583\n",
      "learning_rate=0.00011297332844722422\n",
      "loss_grad=-1.8163091304650205\n",
      "grad=0.16023260135650447\n",
      "unweighted_step=0.16023260135650447\n",
      "learning_rate=0.00011261339780778037\n",
      "loss_grad=-1.8163091304650205\n",
      "grad=-0.158361226463404\n",
      "unweighted_step=-0.158361226463404\n",
      "learning_rate=8.197017601967501e-05\n",
      "loss_grad=-1.8163091304650205\n",
      "grad=0.038601071335310766\n",
      "unweighted_step=0.038601071335310766\n",
      "learning_rate=7.819750852983829e-05\n",
      "loss_grad=-1.8163091304650205\n",
      "grad=0.20216172926922088\n",
      "unweighted_step=0.20216172926922088\n",
      "learning_rate=7.530221284758391e-05\n",
      "pred_val=0.31711990709377724\n",
      "loss_grad=0.6342398141875545\n",
      "grad=0.04613502717711445\n",
      "unweighted_step=0.04613502717711445\n",
      "learning_rate=8.378877484835116e-05\n",
      "loss_grad=0.6342398141875545\n",
      "grad=-0.00034650283074615237\n",
      "unweighted_step=-0.00034650283074615237\n",
      "learning_rate=0.00010485832768552469\n",
      "loss_grad=0.6342398141875545\n",
      "grad=2.998825738527237\n",
      "unweighted_step=2.998825738527237\n",
      "learning_rate=0.00010485832768552469\n",
      "loss_grad=0.6342398141875545\n",
      "grad=0.3022650992122106\n",
      "unweighted_step=0.3022650992122106\n",
      "learning_rate=9.386592324116761e-05\n",
      "loss_grad=0.6342398141875545\n",
      "grad=-0.29314359891518954\n",
      "unweighted_step=-0.29314359891518954\n",
      "learning_rate=9.386592324116761e-05\n",
      "loss_grad=0.6342398141875545\n",
      "grad=-0.09702806790437593\n",
      "unweighted_step=-0.09702806790437593\n",
      "learning_rate=9.373576825106564e-05\n",
      "loss_grad=0.6342398141875545\n",
      "grad=0.09456794048492922\n",
      "unweighted_step=0.09456794048492922\n",
      "learning_rate=6.669089193326009e-05\n",
      "loss_grad=0.6342398141875545\n",
      "grad=0.019694457597347666\n",
      "unweighted_step=0.019694457597347666\n",
      "learning_rate=7.68172840381368e-05\n",
      "loss_grad=0.6342398141875545\n",
      "grad=-0.06480065904230052\n",
      "unweighted_step=-0.06480065904230052\n",
      "learning_rate=6.117758506406493e-05\n",
      "pred_val=0.4379886583361219\n",
      "loss_grad=0.8759773166722438\n",
      "grad=0.0068608140419981955\n",
      "unweighted_step=0.0068608140419981955\n",
      "learning_rate=8.010624363992457e-05\n",
      "loss_grad=0.8759773166722438\n",
      "grad=-5.27311861746323e-05\n",
      "unweighted_step=-5.27311861746323e-05\n",
      "learning_rate=0.00010123379467391749\n",
      "loss_grad=0.8759773166722438\n",
      "grad=1.5161040753385127\n",
      "unweighted_step=1.5161040753385127\n",
      "learning_rate=0.00010123379467391749\n",
      "loss_grad=0.8759773166722438\n",
      "grad=0.13654023049652145\n",
      "unweighted_step=0.13654023049652145\n",
      "learning_rate=9.062135364738323e-05\n",
      "loss_grad=0.8759773166722438\n",
      "grad=-0.13525055343858433\n",
      "unweighted_step=-0.13525055343858433\n",
      "learning_rate=9.062135364738323e-05\n",
      "loss_grad=0.8759773166722438\n",
      "grad=-0.038278893498014874\n",
      "unweighted_step=-0.038278893498014874\n",
      "learning_rate=9.05659909986789e-05\n",
      "loss_grad=0.8759773166722438\n",
      "grad=0.0369902092803776\n",
      "unweighted_step=0.0369902092803776\n",
      "learning_rate=6.380982779693652e-05\n",
      "loss_grad=0.8759773166722438\n",
      "grad=0.014444370839208994\n",
      "unweighted_step=0.014444370839208994\n",
      "learning_rate=7.998021681480538e-05\n",
      "loss_grad=0.8759773166722438\n",
      "grad=-0.019459645778387588\n",
      "unweighted_step=-0.019459645778387588\n",
      "learning_rate=5.849884662286192e-05\n",
      "pred_val=0.34369775178224976\n",
      "loss_grad=0.6873955035644995\n",
      "grad=0.011140827023598605\n",
      "unweighted_step=0.011140827023598605\n",
      "learning_rate=8.235121416083461e-05\n",
      "loss_grad=0.6873955035644995\n",
      "grad=-8.153745775884094e-05\n",
      "unweighted_step=-8.153745775884094e-05\n",
      "learning_rate=0.00010454586070683155\n",
      "loss_grad=0.6873955035644995\n",
      "grad=2.8162219203592302\n",
      "unweighted_step=2.8162219203592302\n",
      "learning_rate=0.00010454586070683155\n",
      "loss_grad=0.6873955035644995\n",
      "grad=0.2658162357193741\n",
      "unweighted_step=0.2658162357193741\n",
      "learning_rate=9.358621243072286e-05\n",
      "loss_grad=0.6873955035644995\n",
      "grad=-0.2630971750386858\n",
      "unweighted_step=-0.2630971750386858\n",
      "learning_rate=9.358621243072286e-05\n",
      "loss_grad=0.6873955035644995\n",
      "grad=0.00018592437259573304\n",
      "unweighted_step=0.00018592437259573304\n",
      "learning_rate=7.851238623734537e-05\n",
      "loss_grad=0.6873955035644995\n",
      "grad=-0.0015598816661899202\n",
      "unweighted_step=-0.0015598816661899202\n",
      "learning_rate=5.561665432994709e-05\n",
      "loss_grad=0.6873955035644995\n",
      "grad=-0.0024301334504274108\n",
      "unweighted_step=-0.0024301334504274108\n",
      "learning_rate=6.633660141468031e-05\n",
      "loss_grad=0.6873955035644995\n",
      "grad=-0.009978309036946189\n",
      "unweighted_step=-0.009978309036946189\n",
      "learning_rate=6.0143065719746915e-05\n",
      "pred_val=0.2806794949446889\n",
      "loss_grad=-1.4386410101106222\n",
      "grad=-0.6560006290998435\n",
      "unweighted_step=-0.6560006290998435\n",
      "learning_rate=6.88445892243162e-05\n",
      "loss_grad=-1.4386410101106222\n",
      "grad=0.005153727282285335\n",
      "unweighted_step=0.005153727282285335\n",
      "learning_rate=8.715376815259913e-05\n",
      "loss_grad=-1.4386410101106222\n",
      "grad=-7.636562948062401\n",
      "unweighted_step=-7.636562948062401\n",
      "learning_rate=8.715376815259913e-05\n",
      "loss_grad=-1.4386410101106222\n",
      "grad=-0.8422005421673472\n",
      "unweighted_step=-0.8422005421673472\n",
      "learning_rate=7.801735053275618e-05\n",
      "loss_grad=-1.4386410101106222\n",
      "grad=0.7908880210987761\n",
      "unweighted_step=0.7908880210987761\n",
      "learning_rate=7.801735053275618e-05\n",
      "loss_grad=-1.4386410101106222\n",
      "grad=0.8310236485719626\n",
      "unweighted_step=0.8310236485719626\n",
      "learning_rate=7.721332207731242e-05\n",
      "loss_grad=-1.4386410101106222\n",
      "grad=-0.824173068808833\n",
      "unweighted_step=-0.824173068808833\n",
      "learning_rate=5.48269015088298e-05\n",
      "loss_grad=-1.4386410101106222\n",
      "grad=-0.5002227914751277\n",
      "unweighted_step=-0.5002227914751277\n",
      "learning_rate=6.399533806745707e-05\n",
      "loss_grad=-1.4386410101106222\n",
      "grad=0.6644289529770746\n",
      "unweighted_step=0.6644289529770746\n",
      "learning_rate=5.0276389392482984e-05\n",
      "pred_val=0.43834615434257573\n",
      "loss_grad=0.8766923086851515\n",
      "grad=0.009535697320914725\n",
      "unweighted_step=0.009535697320914725\n",
      "learning_rate=6.41635859409848e-05\n",
      "loss_grad=0.8766923086851515\n",
      "grad=-6.656206593212503e-05\n",
      "unweighted_step=-6.656206593212503e-05\n",
      "learning_rate=8.133008799341207e-05\n",
      "loss_grad=0.8766923086851515\n",
      "grad=1.518210290518003\n",
      "unweighted_step=1.518210290518003\n",
      "learning_rate=8.133008799341207e-05\n",
      "loss_grad=0.8766923086851515\n",
      "grad=0.144417448382033\n",
      "unweighted_step=0.144417448382033\n",
      "learning_rate=7.280417264768696e-05\n",
      "loss_grad=0.8766923086851515\n",
      "grad=-0.1430549954713032\n",
      "unweighted_step=-0.1430549954713032\n",
      "learning_rate=7.280417264768696e-05\n",
      "loss_grad=0.8766923086851515\n",
      "grad=0.037841456429050735\n",
      "unweighted_step=0.037841456429050735\n",
      "learning_rate=8.043520324420968e-05\n",
      "loss_grad=0.8766923086851515\n",
      "grad=-0.03798047672265286\n",
      "unweighted_step=-0.03798047672265286\n",
      "learning_rate=5.7178977392683085e-05\n",
      "loss_grad=0.8766923086851515\n",
      "grad=-0.0066405250890580955\n",
      "unweighted_step=-0.0066405250890580955\n",
      "learning_rate=6.606578920686812e-05\n",
      "loss_grad=0.8766923086851515\n",
      "grad=0.002697202231146069\n",
      "unweighted_step=0.002697202231146069\n",
      "learning_rate=4.8666201754200075e-05\n",
      "pred_val=0.26078318444671\n",
      "loss_grad=-1.47843363110658\n",
      "grad=-0.5185601765797916\n",
      "unweighted_step=-0.5185601765797916\n",
      "learning_rate=5.6720410157187564e-05\n",
      "loss_grad=-1.47843363110658\n",
      "grad=0.004193625953787751\n",
      "unweighted_step=0.004193625953787751\n",
      "learning_rate=7.184784353579803e-05\n",
      "loss_grad=-1.47843363110658\n",
      "grad=-8.415957930940577\n",
      "unweighted_step=-8.415957930940577\n",
      "learning_rate=7.184784353579803e-05\n",
      "loss_grad=-1.47843363110658\n",
      "grad=-0.9217581588303084\n",
      "unweighted_step=-0.9217581588303084\n",
      "learning_rate=6.431596146467068e-05\n",
      "loss_grad=-1.47843363110658\n",
      "grad=0.8466193272224545\n",
      "unweighted_step=0.8466193272224545\n",
      "learning_rate=6.431596146467068e-05\n",
      "loss_grad=-1.47843363110658\n",
      "grad=0.043791466163168494\n",
      "unweighted_step=0.043791466163168494\n",
      "learning_rate=8.613512372588284e-05\n",
      "loss_grad=-1.47843363110658\n",
      "grad=-0.04582033543276253\n",
      "unweighted_step=-0.04582033543276253\n",
      "learning_rate=6.126441627075828e-05\n",
      "loss_grad=-1.47843363110658\n",
      "grad=0.09756147918948534\n",
      "unweighted_step=0.09756147918948534\n",
      "learning_rate=5.5087202442045125e-05\n",
      "loss_grad=-1.47843363110658\n",
      "grad=0.17062829289132644\n",
      "unweighted_step=0.17062829289132644\n",
      "learning_rate=5.03202025337668e-05\n",
      "pred_val=0.11762314592917682\n",
      "loss_grad=0.23524629185835363\n",
      "grad=0.017732463922726968\n",
      "unweighted_step=0.017732463922726968\n",
      "learning_rate=5.150222074048739e-05\n",
      "key='mult_a', 0.3600051902399173, current_value=0.0014692544789407435, updated=0.0014683412176695226, learning_rate=5.150222074048739e-05, grad=0.017732463922726968\n",
      "loss_grad=0.23524629185835363\n",
      "grad=-0.00014729038146545399\n",
      "unweighted_step=-0.00014729038146545399\n",
      "learning_rate=6.525902357166017e-05\n",
      "key='mult_b', -0.36098270827687884, current_value=0.9999966123630054, updated=0.9999966219750319, learning_rate=6.525902357166017e-05, grad=-0.00014729038146545399\n",
      "loss_grad=0.23524629185835363\n",
      "grad=1.415873158679532\n",
      "unweighted_step=1.415873158679532\n",
      "learning_rate=6.525902357166017e-05\n",
      "key='dif_a', 0.36098270827687884, current_value=-0.03476794967085131, updated=-0.03486034817068806, learning_rate=6.525902357166017e-05, grad=1.415873158679532\n",
      "loss_grad=0.23524629185835363\n",
      "grad=0.1467284042717875\n",
      "unweighted_step=0.1467284042717875\n",
      "learning_rate=5.8417854157361126e-05\n",
      "key='dif_b', 0.3609827082323136, current_value=1.000897103186829, updated=1.0008885316283076, learning_rate=5.8417854157361126e-05, grad=0.1467284042717875\n",
      "loss_grad=0.23524629185835363\n",
      "grad=-0.14265260038793173\n",
      "unweighted_step=-0.14265260038793173\n",
      "learning_rate=5.8417854157361126e-05\n",
      "key='add_norm_b', -0.3609827082323136, current_value=0.9991890459020212, updated=0.9991973793608258, learning_rate=5.8417854157361126e-05, grad=-0.14265260038793173\n",
      "loss_grad=0.23524629185835363\n",
      "grad=-0.020594261296906433\n",
      "unweighted_step=-0.020594261296906433\n",
      "learning_rate=7.016293675246e-05\n",
      "key='target_normalized_intensity_a', -0.04856082387277638, current_value=0.09921208846531414, updated=0.09921353341916697, learning_rate=7.016293675246e-05, grad=-0.020594261296906433\n",
      "loss_grad=0.23524629185835363\n",
      "grad=0.019699328754030777\n",
      "unweighted_step=0.019699328754030777\n",
      "learning_rate=4.988608205348587e-05\n",
      "key='query_normalized_intensity_a', 0.047583305835814826, current_value=0.10077469973129688, updated=0.10077371700896626, learning_rate=4.988608205348587e-05, grad=0.019699328754030777\n",
      "loss_grad=0.23524629185835363\n",
      "grad=0.008083052655574952\n",
      "unweighted_step=0.008083052655574952\n",
      "learning_rate=5.32644611709615e-05\n",
      "key='target_normalized_intensity_b', 0.5563723702130119, current_value=0.10033088853213608, updated=0.10033045799269176, learning_rate=5.32644611709615e-05, grad=0.008083052655574952\n",
      "loss_grad=0.23524629185835363\n",
      "grad=-0.027078572937301763\n",
      "unweighted_step=-0.027078572937301763\n",
      "learning_rate=4.191706479768521e-05\n",
      "key='query_normalized_intensity_b', -0.11002226322898423, current_value=0.0995139758229413, updated=0.09951511087723774, learning_rate=4.191706479768521e-05, grad=-0.027078572937301763\n",
      "pred_val=0.47149407204377136\n",
      "loss_grad=0.9429881440875427\n",
      "grad=0.019185941337235224\n",
      "unweighted_step=0.019185941337235224\n",
      "learning_rate=5.170826971978163e-05\n",
      "loss_grad=0.9429881440875427\n",
      "grad=-0.0001401381577498723\n",
      "unweighted_step=-0.0001401381577498723\n",
      "learning_rate=6.552967925333755e-05\n",
      "loss_grad=0.9429881440875427\n",
      "grad=0.770234290684512\n",
      "unweighted_step=0.770234290684512\n",
      "learning_rate=6.552967925333755e-05\n",
      "loss_grad=0.9429881440875427\n",
      "grad=0.07206257914754632\n",
      "unweighted_step=0.07206257914754632\n",
      "learning_rate=5.866013672991975e-05\n",
      "loss_grad=0.9429881440875427\n",
      "grad=-0.0675981559638055\n",
      "unweighted_step=-0.0675981559638055\n",
      "learning_rate=5.866013672991975e-05\n",
      "loss_grad=0.9429881440875427\n",
      "grad=-0.0011611909349412753\n",
      "unweighted_step=-0.0011611909349412753\n",
      "learning_rate=6.716586541694194e-05\n",
      "loss_grad=0.9429881440875427\n",
      "grad=0.0009723137550511505\n",
      "unweighted_step=0.0009723137550511505\n",
      "learning_rate=4.774783965570681e-05\n",
      "loss_grad=0.9429881440875427\n",
      "grad=-0.018844425611047964\n",
      "unweighted_step=-0.018844425611047964\n",
      "learning_rate=4.615600693594241e-05\n",
      "loss_grad=0.9429881440875427\n",
      "grad=-0.004434816803092853\n",
      "unweighted_step=-0.004434816803092853\n",
      "learning_rate=4.051298310834454e-05\n",
      "pred_val=0.41666633966921934\n",
      "loss_grad=0.8333326793384387\n",
      "grad=0.08815695794440799\n",
      "unweighted_step=0.08815695794440799\n",
      "learning_rate=5.43971198735847e-05\n",
      "loss_grad=0.8333326793384387\n",
      "grad=-0.0006947261711712127\n",
      "unweighted_step=-0.0006947261711712127\n",
      "learning_rate=6.894205231637486e-05\n",
      "loss_grad=0.8333326793384387\n",
      "grad=1.9558621634616953\n",
      "unweighted_step=1.9558621634616953\n",
      "learning_rate=6.894205231637486e-05\n",
      "loss_grad=0.8333326793384387\n",
      "grad=0.2061346018076325\n",
      "unweighted_step=0.2061346018076325\n",
      "learning_rate=6.171478727482106e-05\n",
      "loss_grad=0.8333326793384387\n",
      "grad=-0.1933691469004747\n",
      "unweighted_step=-0.1933691469004747\n",
      "learning_rate=6.171478727482106e-05\n",
      "loss_grad=0.8333326793384387\n",
      "grad=0.10624771772088892\n",
      "unweighted_step=0.10624771772088892\n",
      "learning_rate=5.8525510007766094e-05\n",
      "loss_grad=0.8333326793384387\n",
      "grad=-0.10445856952545643\n",
      "unweighted_step=-0.10445856952545643\n",
      "learning_rate=4.160895969444319e-05\n",
      "loss_grad=0.8333326793384387\n",
      "grad=-0.07571015559696315\n",
      "unweighted_step=-0.07571015559696315\n",
      "learning_rate=4.538391258595209e-05\n",
      "loss_grad=0.8333326793384387\n",
      "grad=0.02873766018178226\n",
      "unweighted_step=0.02873766018178226\n",
      "learning_rate=3.511456046292115e-05\n",
      "pred_val=0.4643602235321678\n",
      "loss_grad=0.9287204470643357\n",
      "grad=0.08108113538061486\n",
      "unweighted_step=0.08108113538061486\n",
      "learning_rate=5.8531311571506e-05\n",
      "loss_grad=0.9287204470643357\n",
      "grad=-0.0006247524578939653\n",
      "unweighted_step=-0.0006247524578939653\n",
      "learning_rate=7.41841889146232e-05\n",
      "loss_grad=0.9287204470643357\n",
      "grad=0.944646176193869\n",
      "unweighted_step=0.944646176193869\n",
      "learning_rate=7.41841889146232e-05\n",
      "loss_grad=0.9287204470643357\n",
      "grad=0.09593256483519148\n",
      "unweighted_step=0.09593256483519148\n",
      "learning_rate=6.64073853938141e-05\n",
      "loss_grad=0.9287204470643357\n",
      "grad=-0.0921459244494651\n",
      "unweighted_step=-0.0921459244494651\n",
      "learning_rate=6.64073853938141e-05\n",
      "loss_grad=0.9287204470643357\n",
      "grad=0.2438383103704626\n",
      "unweighted_step=0.2438383103704626\n",
      "learning_rate=5.768736437078584e-05\n",
      "loss_grad=0.9287204470643357\n",
      "grad=-0.23940525704874993\n",
      "unweighted_step=-0.23940525704874993\n",
      "learning_rate=4.101460175371325e-05\n",
      "loss_grad=0.9287204470643357\n",
      "grad=-0.07455740820202858\n",
      "unweighted_step=-0.07455740820202858\n",
      "learning_rate=4.727351880951211e-05\n",
      "loss_grad=0.9287204470643357\n",
      "grad=0.07008152045406016\n",
      "unweighted_step=0.07008152045406016\n",
      "learning_rate=3.453075157909383e-05\n",
      "pred_val=0.46911807273330836\n",
      "loss_grad=-1.0617638545333832\n",
      "grad=-0.16841869988911792\n",
      "unweighted_step=-0.16841869988911792\n",
      "learning_rate=4.752741929997132e-05\n",
      "loss_grad=-1.0617638545333832\n",
      "grad=0.0012969993064265194\n",
      "unweighted_step=0.0012969993064265194\n",
      "learning_rate=6.023619449711541e-05\n",
      "loss_grad=-1.0617638545333832\n",
      "grad=-0.9383653055697028\n",
      "unweighted_step=-0.9383653055697028\n",
      "learning_rate=6.023619449711541e-05\n",
      "loss_grad=-1.0617638545333832\n",
      "grad=-0.10443241405786086\n",
      "unweighted_step=-0.10443241405786086\n",
      "learning_rate=5.392157333194619e-05\n",
      "loss_grad=-1.0617638545333832\n",
      "grad=0.08725574553267952\n",
      "unweighted_step=0.08725574553267952\n",
      "learning_rate=5.392157333194619e-05\n",
      "loss_grad=-1.0617638545333832\n",
      "grad=0.09249167342751771\n",
      "unweighted_step=0.09249167342751771\n",
      "learning_rate=6.015866132561595e-05\n",
      "loss_grad=-1.0617638545333832\n",
      "grad=-0.09292828116937207\n",
      "unweighted_step=-0.09292828116937207\n",
      "learning_rate=4.2772397886582314e-05\n",
      "loss_grad=-1.0617638545333832\n",
      "grad=-0.037744988703649035\n",
      "unweighted_step=-0.037744988703649035\n",
      "learning_rate=5.062133571825326e-05\n",
      "loss_grad=-1.0617638545333832\n",
      "grad=0.09249154733824691\n",
      "unweighted_step=0.09249154733824691\n",
      "learning_rate=3.597023787118426e-05\n",
      "pred_val=0.26572150831196967\n",
      "loss_grad=0.5314430166239393\n",
      "grad=0.004524725221978773\n",
      "unweighted_step=0.004524725221978773\n",
      "learning_rate=4.486588613178582e-05\n",
      "loss_grad=0.5314430166239393\n",
      "grad=-3.222711139086546e-05\n",
      "unweighted_step=-3.222711139086546e-05\n",
      "learning_rate=5.6863522554716635e-05\n",
      "loss_grad=0.5314430166239393\n",
      "grad=3.0064289697038866\n",
      "unweighted_step=3.0064289697038866\n",
      "learning_rate=5.6863522554716635e-05\n",
      "loss_grad=0.5314430166239393\n",
      "grad=0.28725164493662214\n",
      "unweighted_step=0.28725164493662214\n",
      "learning_rate=5.090246199886357e-05\n",
      "loss_grad=0.5314430166239393\n",
      "grad=-0.2863002172023792\n",
      "unweighted_step=-0.2863002172023792\n",
      "learning_rate=5.090246199886357e-05\n",
      "loss_grad=0.5314430166239393\n",
      "grad=0.01631393247024732\n",
      "unweighted_step=0.01631393247024732\n",
      "learning_rate=6.44551774131722e-05\n",
      "loss_grad=0.5314430166239393\n",
      "grad=-0.017720273564663658\n",
      "unweighted_step=-0.017720273564663658\n",
      "learning_rate=4.582758349633697e-05\n",
      "loss_grad=0.5314430166239393\n",
      "grad=-0.003651554334835432\n",
      "unweighted_step=-0.003651554334835432\n",
      "learning_rate=5.49448538376864e-05\n",
      "loss_grad=0.5314430166239393\n",
      "grad=0.00018111754465926575\n",
      "unweighted_step=0.00018111754465926575\n",
      "learning_rate=3.851849690274307e-05\n",
      "pred_val=0.40190666648141077\n",
      "loss_grad=-1.1961866670371784\n",
      "grad=-0.26358846798166025\n",
      "unweighted_step=-0.26358846798166025\n",
      "learning_rate=3.9392246932154606e-05\n",
      "loss_grad=-1.1961866670371784\n",
      "grad=0.0021711932612790445\n",
      "unweighted_step=0.0021711932612790445\n",
      "learning_rate=4.992591086434933e-05\n",
      "loss_grad=-1.1961866670371784\n",
      "grad=-3.2554437053459133\n",
      "unweighted_step=-3.2554437053459133\n",
      "learning_rate=4.992591086434933e-05\n",
      "loss_grad=-1.1961866670371784\n",
      "grad=-0.36281561258043143\n",
      "unweighted_step=-0.36281561258043143\n",
      "learning_rate=4.469212715561886e-05\n",
      "loss_grad=-1.1961866670371784\n",
      "grad=0.3234991010790135\n",
      "unweighted_step=0.3234991010790135\n",
      "learning_rate=4.469212715561886e-05\n",
      "loss_grad=-1.1961866670371784\n",
      "grad=-0.6641032645155787\n",
      "unweighted_step=-0.6641032645155787\n",
      "learning_rate=5.2485214721588854e-05\n",
      "loss_grad=-1.1961866670371784\n",
      "grad=0.6515380581349104\n",
      "unweighted_step=0.6515380581349104\n",
      "learning_rate=3.731673888336755e-05\n",
      "loss_grad=-1.1961866670371784\n",
      "grad=0.455989707282114\n",
      "unweighted_step=0.455989707282114\n",
      "learning_rate=4.435673300326647e-05\n",
      "loss_grad=-1.1961866670371784\n",
      "grad=-0.3314423237105695\n",
      "unweighted_step=-0.3314423237105695\n",
      "learning_rate=3.1376329004602187e-05\n",
      "pred_val=0.46822277125816203\n",
      "loss_grad=-1.063554457483676\n",
      "grad=-0.19589460502845166\n",
      "unweighted_step=-0.19589460502845166\n",
      "learning_rate=3.8958931736709e-05\n",
      "loss_grad=-1.063554457483676\n",
      "grad=0.0016008127124799512\n",
      "unweighted_step=0.0016008127124799512\n",
      "learning_rate=4.937661085435906e-05\n",
      "loss_grad=-1.063554457483676\n",
      "grad=-0.9694088880900734\n",
      "unweighted_step=-0.9694088880900734\n",
      "learning_rate=4.937661085435906e-05\n",
      "loss_grad=-1.063554457483676\n",
      "grad=-0.11031727192020187\n",
      "unweighted_step=-0.11031727192020187\n",
      "learning_rate=4.420041082099781e-05\n",
      "loss_grad=-1.063554457483676\n",
      "grad=0.09120265014731235\n",
      "unweighted_step=0.09120265014731235\n",
      "learning_rate=4.420041082099781e-05\n",
      "loss_grad=-1.063554457483676\n",
      "grad=0.18254803666027025\n",
      "unweighted_step=0.18254803666027025\n",
      "learning_rate=4.948594374465132e-05\n",
      "loss_grad=-1.063554457483676\n",
      "grad=-0.18205837973944122\n",
      "unweighted_step=-0.18205837973944122\n",
      "learning_rate=3.5184356930333895e-05\n",
      "loss_grad=-1.063554457483676\n",
      "grad=-0.1643243925496365\n",
      "unweighted_step=-0.1643243925496365\n",
      "learning_rate=4.197709418041753e-05\n",
      "loss_grad=-1.063554457483676\n",
      "grad=0.23211938362387546\n",
      "unweighted_step=0.23211938362387546\n",
      "learning_rate=2.9578806894037603e-05\n",
      "pred_val=0.441956363396885\n",
      "loss_grad=-1.11608727320623\n",
      "grad=-0.1992324465987314\n",
      "unweighted_step=-0.1992324465987314\n",
      "learning_rate=4.0692603962032156e-05\n",
      "loss_grad=-1.11608727320623\n",
      "grad=0.0016480310235613101\n",
      "unweighted_step=0.0016480310235613101\n",
      "learning_rate=5.15738131747169e-05\n",
      "loss_grad=-1.11608727320623\n",
      "grad=-1.8416385286547567\n",
      "unweighted_step=-1.8416385286547567\n",
      "learning_rate=5.15738131747169e-05\n",
      "loss_grad=-1.11608727320623\n",
      "grad=-0.2109533196563859\n",
      "unweighted_step=-0.2109533196563859\n",
      "learning_rate=4.6167278200843635e-05\n",
      "loss_grad=-1.11608727320623\n",
      "grad=0.18765150775932912\n",
      "unweighted_step=0.18765150775932912\n",
      "learning_rate=4.6167278200843635e-05\n",
      "loss_grad=-1.11608727320623\n",
      "grad=-0.8693105433947985\n",
      "unweighted_step=-0.8693105433947985\n",
      "learning_rate=4.3476990904810904e-05\n",
      "loss_grad=-1.11608727320623\n",
      "grad=0.8539659827793942\n",
      "unweighted_step=0.8539659827793942\n",
      "learning_rate=3.091196925795714e-05\n",
      "loss_grad=-1.11608727320623\n",
      "grad=0.5186095352232885\n",
      "unweighted_step=0.5186095352232885\n",
      "learning_rate=3.680651833313592e-05\n",
      "loss_grad=-1.11608727320623\n",
      "grad=-0.46592962608157346\n",
      "unweighted_step=-0.46592962608157346\n",
      "learning_rate=2.598925769004877e-05\n",
      "pred_val=0.4826037912718444\n",
      "loss_grad=-1.0347924174563112\n",
      "grad=-0.012860890488027974\n",
      "unweighted_step=-0.012860890488027974\n",
      "learning_rate=4.363264447453641e-05\n",
      "loss_grad=-1.0347924174563112\n",
      "grad=8.847483043350638e-05\n",
      "unweighted_step=8.847483043350638e-05\n",
      "learning_rate=5.529999148009816e-05\n",
      "loss_grad=-1.0347924174563112\n",
      "grad=-0.51556825107133\n",
      "unweighted_step=-0.51556825107133\n",
      "learning_rate=5.529999148009816e-05\n",
      "loss_grad=-1.0347924174563112\n",
      "grad=-0.04780373773830636\n",
      "unweighted_step=-0.04780373773830636\n",
      "learning_rate=4.950283746747704e-05\n",
      "loss_grad=-1.0347924174563112\n",
      "grad=0.04756387575173031\n",
      "unweighted_step=0.04756387575173031\n",
      "learning_rate=4.950283746747704e-05\n",
      "loss_grad=-1.0347924174563112\n",
      "grad=-0.04525483932109877\n",
      "unweighted_step=-0.04525483932109877\n",
      "learning_rate=4.3011189993820466e-05\n",
      "loss_grad=-1.0347924174563112\n",
      "grad=0.04469253549164176\n",
      "unweighted_step=0.04469253549164176\n",
      "learning_rate=3.058076893996377e-05\n",
      "loss_grad=-1.0347924174563112\n",
      "grad=-0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=3.355238275544507e-05\n",
      "loss_grad=-1.0347924174563112\n",
      "grad=-0.00039538173159387027\n",
      "unweighted_step=-0.00039538173159387027\n",
      "learning_rate=2.5711751638857253e-05\n",
      "pred_val=0.00785590381312692\n",
      "loss_grad=0.01571180762625384\n",
      "grad=1.444729688235314e-05\n",
      "unweighted_step=1.444729688235314e-05\n",
      "learning_rate=3.551151858806017e-05\n",
      "key='mult_a', 0.04625000506859367, current_value=0.0014922930046667152, updated=0.0014922924916212633, learning_rate=3.551151858806017e-05, grad=1.444729688235314e-05\n",
      "loss_grad=0.01571180762625384\n",
      "grad=-1.2143807817584867e-07\n",
      "unweighted_step=-1.2143807817584867e-07\n",
      "learning_rate=4.500729648688813e-05\n",
      "key='mult_b', -0.04625095967605164, current_value=0.9999963717776783, updated=0.9999963717831439, learning_rate=4.500729648688813e-05, grad=-1.2143807817584867e-07\n",
      "loss_grad=0.01571180762625384\n",
      "grad=0.01698062684689663\n",
      "unweighted_step=0.01698062684689663\n",
      "learning_rate=4.500729648688813e-05\n",
      "key='dif_a', 0.04625095967605164, current_value=-0.03489628456618119, updated=-0.03489704881828822, learning_rate=4.500729648688813e-05, grad=0.01698062684689663\n",
      "loss_grad=0.01571180762625384\n",
      "grad=0.0017015194140990386\n",
      "unweighted_step=0.0017015194140990386\n",
      "learning_rate=4.028913609584851e-05\n",
      "key='dif_b', 0.04625095967600812, current_value=1.0008894182192754, updated=1.000889349666528, learning_rate=4.028913609584851e-05, grad=0.0017015194140990386\n",
      "loss_grad=0.01571180762625384\n",
      "grad=-0.0016982012416309601\n",
      "unweighted_step=-0.0016982012416309601\n",
      "learning_rate=4.028913609584851e-05\n",
      "key='add_norm_b', -0.04625095967600812, current_value=0.9991997590386873, updated=0.9991998274577483, learning_rate=4.028913609584851e-05, grad=-0.0016982012416309601\n",
      "loss_grad=0.01571180762625384\n",
      "grad=-7.148015281969095e-07\n",
      "unweighted_step=-7.148015281969095e-07\n",
      "learning_rate=4.4931344271519835e-05\n",
      "key='target_normalized_intensity_a', -0.8154771101795633, current_value=0.09925227463921085, updated=0.09925227467132784, learning_rate=4.4931344271519835e-05, grad=-7.148015281969095e-07\n",
      "loss_grad=0.01571180762625384\n",
      "grad=-9.02016643990162e-06\n",
      "unweighted_step=-9.02016643990162e-06\n",
      "learning_rate=2.6157479467079996e-05\n",
      "key='query_normalized_intensity_a', -0.1845238444278947, current_value=0.10074695077862185, updated=0.10074695101456667, learning_rate=2.6157479467079996e-05, grad=-9.02016643990162e-06\n",
      "loss_grad=0.01571180762625384\n",
      "grad=-4.946400286482406e-06\n",
      "unweighted_step=-4.946400286482406e-06\n",
      "learning_rate=3.3746786319109365e-05\n",
      "key='target_normalized_intensity_b', -0.6859801051072139, current_value=0.10030798316155562, updated=0.10030798332848073, learning_rate=3.3746786319109365e-05, grad=-4.946400286482406e-06\n",
      "loss_grad=0.01571180762625384\n",
      "grad=-3.599806947523356e-06\n",
      "unweighted_step=-3.599806947523356e-06\n",
      "learning_rate=2.686006776134538e-05\n",
      "key='query_normalized_intensity_b', -0.8155371311164346, current_value=0.09952418050772723, updated=0.09952418060441828, learning_rate=2.686006776134538e-05, grad=-3.599806947523356e-06\n",
      "pred_val=0.4706470866683614\n",
      "loss_grad=-1.0587058266632772\n",
      "grad=-0.09784815923985765\n",
      "unweighted_step=-0.09784815923985765\n",
      "learning_rate=3.3489581471453477e-05\n",
      "loss_grad=-1.0587058266632772\n",
      "grad=0.0007807916399803181\n",
      "unweighted_step=0.0007807916399803181\n",
      "learning_rate=4.244468706430225e-05\n",
      "loss_grad=-1.0587058266632772\n",
      "grad=-0.8926447568077402\n",
      "unweighted_step=-0.8926447568077402\n",
      "learning_rate=4.244468706430225e-05\n",
      "loss_grad=-1.0587058266632772\n",
      "grad=-0.09866067388891561\n",
      "unweighted_step=-0.09866067388891561\n",
      "learning_rate=3.799516760971354e-05\n",
      "loss_grad=-1.0587058266632772\n",
      "grad=0.08668849696505018\n",
      "unweighted_step=0.08668849696505018\n",
      "learning_rate=3.799516760971354e-05\n",
      "loss_grad=-1.0587058266632772\n",
      "grad=-0.04629117936480733\n",
      "unweighted_step=-0.04629117936480733\n",
      "learning_rate=4.818084947539716e-05\n",
      "loss_grad=-1.0587058266632772\n",
      "grad=0.04467623938832552\n",
      "unweighted_step=0.04467623938832552\n",
      "learning_rate=2.4125603692954598e-05\n",
      "loss_grad=-1.0587058266632772\n",
      "grad=0.08099322229237692\n",
      "unweighted_step=0.08099322229237692\n",
      "learning_rate=2.85870033992219e-05\n",
      "loss_grad=-1.0587058266632772\n",
      "grad=-0.04520755006167891\n",
      "unweighted_step=-0.04520755006167891\n",
      "learning_rate=2.8802871763830208e-05\n",
      "pred_val=0.48259490676059674\n",
      "loss_grad=-1.0348101864788064\n",
      "grad=-0.012877260876690952\n",
      "unweighted_step=-0.012877260876690952\n",
      "learning_rate=3.4210654009779824e-05\n",
      "loss_grad=-1.0348101864788064\n",
      "grad=8.876529065492714e-05\n",
      "unweighted_step=8.876529065492714e-05\n",
      "learning_rate=4.335857117766928e-05\n",
      "loss_grad=-1.0348101864788064\n",
      "grad=-0.5168126267542545\n",
      "unweighted_step=-0.5168126267542545\n",
      "learning_rate=4.335857117766928e-05\n",
      "loss_grad=-1.0348101864788064\n",
      "grad=-0.04916168678157595\n",
      "unweighted_step=-0.04916168678157595\n",
      "learning_rate=3.881324832758138e-05\n",
      "loss_grad=-1.0348101864788064\n",
      "grad=0.04873887795708094\n",
      "unweighted_step=0.04873887795708094\n",
      "learning_rate=3.881324832758138e-05\n",
      "loss_grad=-1.0348101864788064\n",
      "grad=-0.04488923926380499\n",
      "unweighted_step=-0.04488923926380499\n",
      "learning_rate=5.233214920449659e-05\n",
      "loss_grad=-1.0348101864788064\n",
      "grad=0.044336837139717464\n",
      "unweighted_step=0.044336837139717464\n",
      "learning_rate=2.439486259958588e-05\n",
      "loss_grad=-1.0348101864788064\n",
      "grad=5.00416570820438e-05\n",
      "unweighted_step=5.00416570820438e-05\n",
      "learning_rate=2.783091981446506e-05\n",
      "loss_grad=-1.0348101864788064\n",
      "grad=-0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=3.128467941339011e-05\n",
      "pred_val=0.49992592240315115\n",
      "loss_grad=-1.0001481551936977\n",
      "grad=-0.02265762101503871\n",
      "unweighted_step=-0.02265762101503871\n",
      "learning_rate=3.6289485775840354e-05\n",
      "loss_grad=-1.0001481551936977\n",
      "grad=0.00016244825993896102\n",
      "unweighted_step=0.00016244825993896102\n",
      "learning_rate=4.5993280294005506e-05\n",
      "loss_grad=-1.0001481551936977\n",
      "grad=-0.0030996067828389763\n",
      "unweighted_step=-0.0030996067828389763\n",
      "learning_rate=4.5993280294005506e-05\n",
      "loss_grad=-1.0001481551936977\n",
      "grad=-0.000724220229435115\n",
      "unweighted_step=-0.000724220229435115\n",
      "learning_rate=4.11717582236817e-05\n",
      "loss_grad=-1.0001481551936977\n",
      "grad=0.00012403334298316502\n",
      "unweighted_step=0.00012403334298316502\n",
      "learning_rate=4.11717582236817e-05\n",
      "loss_grad=-1.0001481551936977\n",
      "grad=-0.0873465417032792\n",
      "unweighted_step=-0.0873465417032792\n",
      "learning_rate=5.720324614738146e-05\n",
      "loss_grad=-1.0001481551936977\n",
      "grad=0.08572159017172677\n",
      "unweighted_step=0.08572159017172677\n",
      "learning_rate=2.5750737743391282e-05\n",
      "loss_grad=-1.0001481551936977\n",
      "grad=0.008587619026539204\n",
      "unweighted_step=0.008587619026539204\n",
      "learning_rate=2.8854422654135728e-05\n",
      "loss_grad=-1.0001481551936977\n",
      "grad=-0.008830891368702172\n",
      "unweighted_step=-0.008830891368702172\n",
      "learning_rate=3.4196740040353e-05\n",
      "pred_val=0.17649527667395573\n",
      "loss_grad=0.35299055334791146\n",
      "grad=0.0258283452294876\n",
      "unweighted_step=0.0258283452294876\n",
      "learning_rate=2.974348627086434e-05\n",
      "loss_grad=0.35299055334791146\n",
      "grad=-0.00021256454573515555\n",
      "unweighted_step=-0.00021256454573515555\n",
      "learning_rate=3.769688386607165e-05\n",
      "loss_grad=0.35299055334791146\n",
      "grad=2.2695359489406153\n",
      "unweighted_step=2.2695359489406153\n",
      "learning_rate=3.769688386607165e-05\n",
      "loss_grad=0.35299055334791146\n",
      "grad=0.2280716563127598\n",
      "unweighted_step=0.2280716563127598\n",
      "learning_rate=3.374508142056553e-05\n",
      "loss_grad=0.35299055334791146\n",
      "grad=-0.2226193521964571\n",
      "unweighted_step=-0.2226193521964571\n",
      "learning_rate=3.374508142056553e-05\n",
      "loss_grad=0.35299055334791146\n",
      "grad=0.01788581488131903\n",
      "unweighted_step=0.01788581488131903\n",
      "learning_rate=4.596050894827188e-05\n",
      "loss_grad=0.35299055334791146\n",
      "grad=-0.01857338181071078\n",
      "unweighted_step=-0.01857338181071078\n",
      "learning_rate=2.1172509498500332e-05\n",
      "loss_grad=0.35299055334791146\n",
      "grad=-0.016809002983073692\n",
      "unweighted_step=-0.016809002983073692\n",
      "learning_rate=2.3995687795919106e-05\n",
      "loss_grad=0.35299055334791146\n",
      "grad=-0.0077751717695901295\n",
      "unweighted_step=-0.0077751717695901295\n",
      "learning_rate=3.749813850487001e-05\n",
      "pred_val=0.3571716034922958\n",
      "loss_grad=-1.2856567930154084\n",
      "grad=-0.1970071499855777\n",
      "unweighted_step=-0.1970071499855777\n",
      "learning_rate=2.796457018236816e-05\n",
      "loss_grad=-1.2856567930154084\n",
      "grad=0.0016166169440890526\n",
      "unweighted_step=0.0016166169440890526\n",
      "learning_rate=3.544228591412737e-05\n",
      "loss_grad=-1.2856567930154084\n",
      "grad=-4.975770807587882\n",
      "unweighted_step=-4.975770807587882\n",
      "learning_rate=3.544228591412737e-05\n",
      "loss_grad=-1.2856567930154084\n",
      "grad=-0.5094232941576584\n",
      "unweighted_step=-0.5094232941576584\n",
      "learning_rate=3.172683525121902e-05\n",
      "loss_grad=-1.2856567930154084\n",
      "grad=0.482520411541862\n",
      "unweighted_step=0.482520411541862\n",
      "learning_rate=3.172683525121902e-05\n",
      "loss_grad=-1.2856567930154084\n",
      "grad=0.35937275447812383\n",
      "unweighted_step=0.35937275447812383\n",
      "learning_rate=4.374199068144035e-05\n",
      "loss_grad=-1.2856567930154084\n",
      "grad=-0.35410710528189726\n",
      "unweighted_step=-0.35410710528189726\n",
      "learning_rate=2.034900285808172e-05\n",
      "loss_grad=-1.2856567930154084\n",
      "grad=-0.07396542945513127\n",
      "unweighted_step=-0.07396542945513127\n",
      "learning_rate=2.317518077080081e-05\n",
      "loss_grad=-1.2856567930154084\n",
      "grad=0.17567006127974702\n",
      "unweighted_step=0.17567006127974702\n",
      "learning_rate=3.0063357812081024e-05\n",
      "pred_val=0.49078186216219144\n",
      "loss_grad=0.9815637243243829\n",
      "grad=0.030200830005045823\n",
      "unweighted_step=0.030200830005045823\n",
      "learning_rate=2.460614546439266e-05\n",
      "loss_grad=0.9815637243243829\n",
      "grad=-0.00022537116391895708\n",
      "unweighted_step=-0.00022537116391895708\n",
      "learning_rate=3.1185819826349433e-05\n",
      "loss_grad=0.9815637243243829\n",
      "grad=0.2616571611932446\n",
      "unweighted_step=0.2616571611932446\n",
      "learning_rate=3.1185819826349433e-05\n",
      "loss_grad=0.9815637243243829\n",
      "grad=0.02678004622761814\n",
      "unweighted_step=0.02678004622761814\n",
      "learning_rate=2.791657880651541e-05\n",
      "loss_grad=0.9815637243243829\n",
      "grad=-0.024742714663391355\n",
      "unweighted_step=-0.024742714663391355\n",
      "learning_rate=2.791657880651541e-05\n",
      "loss_grad=0.9815637243243829\n",
      "grad=0.1281105520817296\n",
      "unweighted_step=0.1281105520817296\n",
      "learning_rate=4.487337512907186e-05\n",
      "loss_grad=0.9815637243243829\n",
      "grad=-0.12590145299676633\n",
      "unweighted_step=-0.12590145299676633\n",
      "learning_rate=2.097071486279499e-05\n",
      "loss_grad=0.9815637243243829\n",
      "grad=-0.028545568360695155\n",
      "unweighted_step=-0.028545568360695155\n",
      "learning_rate=2.393771447934505e-05\n",
      "loss_grad=0.9815637243243829\n",
      "grad=0.0245722502809731\n",
      "unweighted_step=0.0245722502809731\n",
      "learning_rate=2.8586184795993954e-05\n",
      "pred_val=0.35131065014353147\n",
      "loss_grad=-1.297378699712937\n",
      "grad=-0.24585695588148782\n",
      "unweighted_step=-0.24585695588148782\n",
      "learning_rate=2.2392769814812585e-05\n",
      "loss_grad=-1.297378699712937\n",
      "grad=0.0020750629937019032\n",
      "unweighted_step=0.0020750629937019032\n",
      "learning_rate=2.8380588262040106e-05\n",
      "loss_grad=-1.297378699712937\n",
      "grad=-5.227984260402681\n",
      "unweighted_step=-5.227984260402681\n",
      "learning_rate=2.8380588262040106e-05\n",
      "loss_grad=-1.297378699712937\n",
      "grad=-0.5412831870284949\n",
      "unweighted_step=-0.5412831870284949\n",
      "learning_rate=2.540542250305347e-05\n",
      "loss_grad=-1.297378699712937\n",
      "grad=0.49615492824909047\n",
      "unweighted_step=0.49615492824909047\n",
      "learning_rate=2.540542250305347e-05\n",
      "loss_grad=-1.297378699712937\n",
      "grad=0.6096529956898405\n",
      "unweighted_step=0.6096529956898405\n",
      "learning_rate=4.769736771101732e-05\n",
      "loss_grad=-1.297378699712937\n",
      "grad=-0.6010683651168572\n",
      "unweighted_step=-0.6010683651168572\n",
      "learning_rate=2.233960402247674e-05\n",
      "loss_grad=-1.297378699712937\n",
      "grad=-0.16361846057967713\n",
      "unweighted_step=-0.16361846057967713\n",
      "learning_rate=2.5528411892992973e-05\n",
      "loss_grad=-1.297378699712937\n",
      "grad=0.3545092991981347\n",
      "unweighted_step=0.3545092991981347\n",
      "learning_rate=2.9313198219779517e-05\n",
      "pred_val=0.12091107879746418\n",
      "loss_grad=-1.7581778424050716\n",
      "grad=-0.4508485126039726\n",
      "unweighted_step=-0.4508485126039726\n",
      "learning_rate=2.250526942826581e-05\n",
      "loss_grad=-1.7581778424050716\n",
      "grad=0.003907344483284374\n",
      "unweighted_step=0.003907344483284374\n",
      "learning_rate=2.8523170199179424e-05\n",
      "loss_grad=-1.7581778424050716\n",
      "grad=-10.732841520394524\n",
      "unweighted_step=-10.732841520394524\n",
      "learning_rate=2.8523170199179424e-05\n",
      "loss_grad=-1.7581778424050716\n",
      "grad=-1.1377818204238177\n",
      "unweighted_step=-1.1377818204238177\n",
      "learning_rate=2.5533057431578655e-05\n",
      "loss_grad=-1.7581778424050716\n",
      "grad=1.0582135405142858\n",
      "unweighted_step=1.0582135405142858\n",
      "learning_rate=2.5533057431578655e-05\n",
      "loss_grad=-1.7581778424050716\n",
      "grad=0.3436781591017785\n",
      "unweighted_step=0.3436781591017785\n",
      "learning_rate=5.158309280956115e-05\n",
      "loss_grad=-1.7581778424050716\n",
      "grad=-0.3386985195389391\n",
      "unweighted_step=-0.3386985195389391\n",
      "learning_rate=2.4185706763724503e-05\n",
      "loss_grad=-1.7581778424050716\n",
      "grad=-0.05105607568365035\n",
      "unweighted_step=-0.05105607568365035\n",
      "learning_rate=2.765303332168135e-05\n",
      "loss_grad=-1.7581778424050716\n",
      "grad=0.3598724081100662\n",
      "unweighted_step=0.3598724081100662\n",
      "learning_rate=3.115160966707797e-05\n",
      "pred_val=0.3401411231953107\n",
      "loss_grad=-1.3197177536093787\n",
      "grad=-0.34847526770966897\n",
      "unweighted_step=-0.34847526770966897\n",
      "learning_rate=2.3687065301259583e-05\n",
      "loss_grad=-1.3197177536093787\n",
      "grad=0.0029039734010112652\n",
      "unweighted_step=0.0029039734010112652\n",
      "learning_rate=3.0020977838162845e-05\n",
      "loss_grad=-1.3197177536093787\n",
      "grad=-5.738070629611479\n",
      "unweighted_step=-5.738070629611479\n",
      "learning_rate=3.0020977838162845e-05\n",
      "loss_grad=-1.3197177536093787\n",
      "grad=-0.5865166651910044\n",
      "unweighted_step=-0.5865166651910044\n",
      "learning_rate=2.6873848381552413e-05\n",
      "loss_grad=-1.3197177536093787\n",
      "grad=0.5457632159177541\n",
      "unweighted_step=0.5457632159177541\n",
      "learning_rate=2.6873848381552413e-05\n",
      "loss_grad=-1.3197177536093787\n",
      "grad=0.044240310151596844\n",
      "unweighted_step=0.044240310151596844\n",
      "learning_rate=5.626338770168804e-05\n",
      "loss_grad=-1.3197177536093787\n",
      "grad=-0.04528336306749383\n",
      "unweighted_step=-0.04528336306749383\n",
      "learning_rate=2.639432269681749e-05\n",
      "loss_grad=-1.3197177536093787\n",
      "grad=0.02915623922981664\n",
      "unweighted_step=0.02915623922981664\n",
      "learning_rate=2.235435599472048e-05\n",
      "loss_grad=-1.3197177536093787\n",
      "grad=0.1200019895342792\n",
      "unweighted_step=0.1200019895342792\n",
      "learning_rate=3.368604493414302e-05\n",
      "pred_val=0.17338254163525008\n",
      "loss_grad=0.34676508327050015\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=2.549334571333117e-05\n",
      "key='mult_a', -0.9208532714794252, current_value=0.0015247362695681849, updated=0.0015247362695681849, learning_rate=2.549334571333117e-05, grad=0.0\n",
      "loss_grad=0.34676508327050015\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=2.472960093379003e-05\n",
      "key='mult_b', -0.07914672945280865, current_value=0.9999960275457469, updated=0.9999960275457469, learning_rate=2.472960093379003e-05, grad=0.0\n",
      "loss_grad=0.34676508327050015\n",
      "grad=2.274477558226021\n",
      "unweighted_step=2.274477558226021\n",
      "learning_rate=2.472960093379003e-05\n",
      "key='dif_a', 0.07914672945280865, current_value=-0.03412720128200902, updated=-0.034183448204356814, learning_rate=2.472960093379003e-05, grad=2.274477558226021\n",
      "loss_grad=0.34676508327050015\n",
      "grad=0.2171901314361589\n",
      "unweighted_step=0.2171901314361589\n",
      "learning_rate=2.213717186740509e-05\n",
      "key='dif_b', 0.0791467294528086, current_value=1.0009613192591496, updated=1.000956511283882, learning_rate=2.213717186740509e-05, grad=0.2171901314361589\n",
      "loss_grad=0.34676508327050015\n",
      "grad=-0.2171901314361589\n",
      "unweighted_step=-0.2171901314361589\n",
      "learning_rate=2.213717186740509e-05\n",
      "key='add_norm_b', -0.0791467294528086, current_value=0.9991332398768384, updated=0.9991380478521059, learning_rate=2.213717186740509e-05, grad=-0.2171901314361589\n",
      "loss_grad=0.34676508327050015\n",
      "grad=-0.00027989471518598236\n",
      "unweighted_step=-0.00027989471518598236\n",
      "learning_rate=4.5271403223949226e-05\n",
      "key='target_normalized_intensity_a', -0.015444801865409752, current_value=0.09919026426685966, updated=0.09919027693808617, learning_rate=4.5271403223949226e-05, grad=-0.00027989471518598236\n",
      "loss_grad=0.34676508327050015\n",
      "grad=-0.0011325828786041276\n",
      "unweighted_step=-0.0011325828786041276\n",
      "learning_rate=2.891919116028132e-05\n",
      "key='query_normalized_intensity_a', -0.9855317615668241, current_value=0.10077563791919457, updated=0.10077567067257534, learning_rate=2.891919116028132e-05, grad=-0.0011325828786041276\n",
      "loss_grad=0.34676508327050015\n",
      "grad=0.00013585306512002774\n",
      "unweighted_step=0.00013585306512002774\n",
      "learning_rate=2.1330382539646357e-05\n",
      "key='target_normalized_intensity_b', 0.5139785350536062, current_value=0.10031315661878254, updated=0.10031315372098469, learning_rate=2.1330382539646357e-05, grad=0.00013585306512002774\n",
      "loss_grad=0.34676508327050015\n",
      "grad=0.0005762769450151594\n",
      "unweighted_step=0.0005762769450151594\n",
      "learning_rate=3.674066321269326e-05\n",
      "key='query_normalized_intensity_b', 0.9689301395203941, current_value=0.09949444779649615, updated=0.09949442662369899, learning_rate=3.674066321269326e-05, grad=0.0005762769450151594\n",
      "pred_val=0.49077255914366513\n",
      "loss_grad=-1.0184548817126697\n",
      "grad=-0.059730143496158634\n",
      "unweighted_step=-0.059730143496158634\n",
      "learning_rate=2.7740023047826162e-05\n",
      "loss_grad=-1.0184548817126697\n",
      "grad=0.0004877249205329775\n",
      "unweighted_step=0.0004877249205329775\n",
      "learning_rate=2.319953083191314e-05\n",
      "loss_grad=-1.0184548817126697\n",
      "grad=-0.27752301046201666\n",
      "unweighted_step=-0.27752301046201666\n",
      "learning_rate=2.319953083191314e-05\n",
      "loss_grad=-1.0184548817126697\n",
      "grad=-0.03061877230108209\n",
      "unweighted_step=-0.03061877230108209\n",
      "learning_rate=2.076750056113886e-05\n",
      "loss_grad=-1.0184548817126697\n",
      "grad=0.02722347213727102\n",
      "unweighted_step=0.02722347213727102\n",
      "learning_rate=2.076750056113886e-05\n",
      "loss_grad=-1.0184548817126697\n",
      "grad=-0.043908441492157495\n",
      "unweighted_step=-0.043908441492157495\n",
      "learning_rate=4.311271424069621e-05\n",
      "loss_grad=-1.0184548817126697\n",
      "grad=0.042484367057646834\n",
      "unweighted_step=0.042484367057646834\n",
      "learning_rate=2.3198114391175288e-05\n",
      "loss_grad=-1.0184548817126697\n",
      "grad=-0.009246838866394307\n",
      "unweighted_step=-0.009246838866394307\n",
      "learning_rate=1.861935959718497e-05\n",
      "loss_grad=-1.0184548817126697\n",
      "grad=0.017242459680467996\n",
      "unweighted_step=0.017242459680467996\n",
      "learning_rate=4.02435004419706e-05\n",
      "pred_val=0.21874270030442877\n",
      "loss_grad=-1.5625145993911425\n",
      "grad=-0.5116764115066514\n",
      "unweighted_step=-0.5116764115066514\n",
      "learning_rate=3.034936044710972e-05\n",
      "loss_grad=-1.5625145993911425\n",
      "grad=0.004472053720217689\n",
      "unweighted_step=0.004472053720217689\n",
      "learning_rate=2.3641806578447058e-05\n",
      "loss_grad=-1.5625145993911425\n",
      "grad=-9.968864628839514\n",
      "unweighted_step=-9.968864628839514\n",
      "learning_rate=2.3641806578447058e-05\n",
      "loss_grad=-1.5625145993911425\n",
      "grad=-1.0566791117305836\n",
      "unweighted_step=-1.0566791117305836\n",
      "learning_rate=2.116341209404307e-05\n",
      "loss_grad=-1.5625145993911425\n",
      "grad=0.9875505895604597\n",
      "unweighted_step=0.9875505895604597\n",
      "learning_rate=2.116341209404307e-05\n",
      "loss_grad=-1.5625145993911425\n",
      "grad=-0.0011216489751605747\n",
      "unweighted_step=-0.0011216489751605747\n",
      "learning_rate=4.424047214641319e-05\n",
      "loss_grad=-1.5625145993911425\n",
      "grad=-0.0005004603010016622\n",
      "unweighted_step=-0.0005004603010016622\n",
      "learning_rate=2.2013035982850652e-05\n",
      "loss_grad=-1.5625145993911425\n",
      "grad=0.10606076232315358\n",
      "unweighted_step=0.10606076232315358\n",
      "learning_rate=1.7009685985241656e-05\n",
      "loss_grad=-1.5625145993911425\n",
      "grad=0.12365039646718252\n",
      "unweighted_step=0.12365039646718252\n",
      "learning_rate=4.417407349037194e-05\n",
      "pred_val=0.4620814920911462\n",
      "loss_grad=0.9241629841822924\n",
      "grad=0.06995474707157605\n",
      "unweighted_step=0.06995474707157605\n",
      "learning_rate=2.4369565329890794e-05\n",
      "loss_grad=0.9241629841822924\n",
      "grad=-0.0005533697650459652\n",
      "unweighted_step=-0.0005533697650459652\n",
      "learning_rate=1.987018194703841e-05\n",
      "loss_grad=0.9241629841822924\n",
      "grad=1.031669477865988\n",
      "unweighted_step=1.031669477865988\n",
      "learning_rate=1.987018194703841e-05\n",
      "loss_grad=0.9241629841822924\n",
      "grad=0.10360126445503515\n",
      "unweighted_step=0.10360126445503515\n",
      "learning_rate=1.778717068568503e-05\n",
      "loss_grad=0.9241629841822924\n",
      "grad=-0.09329667851934631\n",
      "unweighted_step=-0.09329667851934631\n",
      "learning_rate=1.778717068568503e-05\n",
      "loss_grad=0.9241629841822924\n",
      "grad=-0.0006055722216072499\n",
      "unweighted_step=-0.0006055722216072499\n",
      "learning_rate=4.703112485531652e-05\n",
      "loss_grad=0.9241629841822924\n",
      "grad=0.0010200431648526337\n",
      "unweighted_step=0.0010200431648526337\n",
      "learning_rate=1.9273349854490745e-05\n",
      "loss_grad=0.9241629841822924\n",
      "grad=-0.03716036565406786\n",
      "unweighted_step=-0.03716036565406786\n",
      "learning_rate=1.5193490455867398e-05\n",
      "loss_grad=0.9241629841822924\n",
      "grad=-0.0024496247657927323\n",
      "unweighted_step=-0.0024496247657927323\n",
      "learning_rate=3.5390726878553616e-05\n",
      "pred_val=0.46961059294844704\n",
      "loss_grad=-1.0607788141031058\n",
      "grad=-0.08591077342007421\n",
      "unweighted_step=-0.08591077342007421\n",
      "learning_rate=2.311492260018383e-05\n",
      "loss_grad=-1.0607788141031058\n",
      "grad=0.0006916970949045764\n",
      "unweighted_step=0.0006916970949045764\n",
      "learning_rate=1.847461956477814e-05\n",
      "loss_grad=-1.0607788141031058\n",
      "grad=-0.9507533824757549\n",
      "unweighted_step=-0.9507533824757549\n",
      "learning_rate=1.847461956477814e-05\n",
      "loss_grad=-1.0607788141031058\n",
      "grad=-0.09669247358398253\n",
      "unweighted_step=-0.09669247358398253\n",
      "learning_rate=1.6537906518806856e-05\n",
      "loss_grad=-1.0607788141031058\n",
      "grad=0.08326698752904522\n",
      "unweighted_step=0.08326698752904522\n",
      "learning_rate=1.6537906518806856e-05\n",
      "loss_grad=-1.0607788141031058\n",
      "grad=-0.04696784236424858\n",
      "unweighted_step=-0.04696784236424858\n",
      "learning_rate=5.086602349490282e-05\n",
      "loss_grad=-1.0607788141031058\n",
      "grad=0.045511839601359766\n",
      "unweighted_step=0.045511839601359766\n",
      "learning_rate=1.9037661445455253e-05\n",
      "loss_grad=-1.0607788141031058\n",
      "grad=0.010749768223861516\n",
      "unweighted_step=0.010749768223861516\n",
      "learning_rate=1.372560324431007e-05\n",
      "loss_grad=-1.0607788141031058\n",
      "grad=0.04346164408539728\n",
      "unweighted_step=0.04346164408539728\n",
      "learning_rate=3.360057331688115e-05\n",
      "pred_val=0.4907252455451982\n",
      "loss_grad=-1.0185495089096035\n",
      "grad=-0.0716821832010839\n",
      "unweighted_step=-0.0716821832010839\n",
      "learning_rate=2.3675644379215198e-05\n",
      "loss_grad=-1.0185495089096035\n",
      "grad=0.0005836412319144236\n",
      "unweighted_step=0.0005836412319144236\n",
      "learning_rate=1.87495773168862e-05\n",
      "loss_grad=-1.0185495089096035\n",
      "grad=-0.28149651823363975\n",
      "unweighted_step=-0.28149651823363975\n",
      "learning_rate=1.87495773168862e-05\n",
      "loss_grad=-1.0185495089096035\n",
      "grad=-0.029678410026987623\n",
      "unweighted_step=-0.029678410026987623\n",
      "learning_rate=1.6784040171791716e-05\n",
      "loss_grad=-1.0185495089096035\n",
      "grad=0.025650227480715958\n",
      "unweighted_step=0.025650227480715958\n",
      "learning_rate=1.6784040171791716e-05\n",
      "loss_grad=-1.0185495089096035\n",
      "grad=-0.1732900781162416\n",
      "unweighted_step=-0.1732900781162416\n",
      "learning_rate=5.548312202088988e-05\n",
      "loss_grad=-1.0185495089096035\n",
      "grad=0.1696532055985991\n",
      "unweighted_step=0.1696532055985991\n",
      "learning_rate=1.9873141397053325e-05\n",
      "loss_grad=-1.0185495089096035\n",
      "grad=0.05559669876001301\n",
      "unweighted_step=0.05559669876001301\n",
      "learning_rate=1.3748848222748827e-05\n",
      "loss_grad=-1.0185495089096035\n",
      "grad=-0.04883992493301719\n",
      "unweighted_step=-0.04883992493301719\n",
      "learning_rate=2.941028882531779e-05\n",
      "pred_val=0.32248715065655287\n",
      "loss_grad=-1.3550256986868943\n",
      "grad=-0.2669739408491136\n",
      "unweighted_step=-0.2669739408491136\n",
      "learning_rate=2.514658848198694e-05\n",
      "loss_grad=-1.3550256986868943\n",
      "grad=0.00224978174341603\n",
      "unweighted_step=0.00224978174341603\n",
      "learning_rate=1.982658115681593e-05\n",
      "loss_grad=-1.3550256986868943\n",
      "grad=-6.4879871373172096\n",
      "unweighted_step=-6.4879871373172096\n",
      "learning_rate=1.982658115681593e-05\n",
      "loss_grad=-1.3550256986868943\n",
      "grad=-0.6578090524938092\n",
      "unweighted_step=-0.6578090524938092\n",
      "learning_rate=1.7748140610379978e-05\n",
      "loss_grad=-1.3550256986868943\n",
      "grad=0.6256838844723072\n",
      "unweighted_step=0.6256838844723072\n",
      "learning_rate=1.7748140610379978e-05\n",
      "loss_grad=-1.3550256986868943\n",
      "grad=0.20066932749975697\n",
      "unweighted_step=0.20066932749975697\n",
      "learning_rate=4.464255791137317e-05\n",
      "loss_grad=-1.3550256986868943\n",
      "grad=-0.19757276205462515\n",
      "unweighted_step=-0.19757276205462515\n",
      "learning_rate=1.6456097425615338e-05\n",
      "loss_grad=-1.3550256986868943\n",
      "grad=-0.029124224329557555\n",
      "unweighted_step=-0.029124224329557555\n",
      "learning_rate=1.1674878816864848e-05\n",
      "loss_grad=-1.3550256986868943\n",
      "grad=0.13543572190224767\n",
      "unweighted_step=0.13543572190224767\n",
      "learning_rate=2.683260523949092e-05\n",
      "pred_val=0.49911198063957146\n",
      "loss_grad=-1.0017760387208572\n",
      "grad=-0.05068675919599097\n",
      "unweighted_step=-0.05068675919599097\n",
      "learning_rate=2.7185084100339463e-05\n",
      "loss_grad=-1.0017760387208572\n",
      "grad=0.00039393123327013144\n",
      "unweighted_step=0.00039393123327013144\n",
      "learning_rate=2.138734449249877e-05\n",
      "loss_grad=-1.0017760387208572\n",
      "grad=-0.028636101274279815\n",
      "unweighted_step=-0.028636101274279815\n",
      "learning_rate=2.138734449249877e-05\n",
      "loss_grad=-1.0017760387208572\n",
      "grad=-0.0053081617774733275\n",
      "unweighted_step=-0.0053081617774733275\n",
      "learning_rate=1.9145287547722822e-05\n",
      "loss_grad=-1.0017760387208572\n",
      "grad=0.0016775571461245277\n",
      "unweighted_step=0.0016775571461245277\n",
      "learning_rate=1.9145287547722822e-05\n",
      "loss_grad=-1.0017760387208572\n",
      "grad=-0.12750211104446257\n",
      "unweighted_step=-0.12750211104446257\n",
      "learning_rate=4.2307415025688106e-05\n",
      "loss_grad=-1.0017760387208572\n",
      "grad=0.12474256890824527\n",
      "unweighted_step=0.12474256890824527\n",
      "learning_rate=1.5402436709461622e-05\n",
      "loss_grad=-1.0017760387208572\n",
      "grad=0.027397942551693737\n",
      "unweighted_step=0.027397942551693737\n",
      "learning_rate=1.0804205899753732e-05\n",
      "loss_grad=-1.0017760387208572\n",
      "grad=-0.019734890322156542\n",
      "unweighted_step=-0.019734890322156542\n",
      "learning_rate=2.3983594882719454e-05\n",
      "pred_val=0.00015483422229446475\n",
      "loss_grad=0.0003096684445889295\n",
      "grad=1.8978211734796492e-07\n",
      "unweighted_step=1.8978211734796492e-07\n",
      "learning_rate=2.200544886223307e-05\n",
      "loss_grad=0.0003096684445889295\n",
      "grad=-1.7030910684949293e-09\n",
      "unweighted_step=-1.7030910684949293e-09\n",
      "learning_rate=1.7337428920718648e-05\n",
      "loss_grad=0.0003096684445889295\n",
      "grad=1.2448214958957269e-05\n",
      "unweighted_step=1.2448214958957269e-05\n",
      "learning_rate=1.7337428920718648e-05\n",
      "loss_grad=0.0003096684445889295\n",
      "grad=1.2765171637162037e-06\n",
      "unweighted_step=1.2765171637162037e-06\n",
      "learning_rate=1.5519928719611864e-05\n",
      "loss_grad=0.0003096684445889295\n",
      "grad=-1.2348062546030147e-06\n",
      "unweighted_step=-1.2348062546030147e-06\n",
      "learning_rate=1.5519928719611864e-05\n",
      "loss_grad=0.0003096684445889295\n",
      "grad=-5.76522566714199e-07\n",
      "unweighted_step=-5.76522566714199e-07\n",
      "learning_rate=4.331628713720939e-05\n",
      "loss_grad=0.0003096684445889295\n",
      "grad=5.625807397362671e-07\n",
      "unweighted_step=5.625807397362671e-07\n",
      "learning_rate=1.5679460391889563e-05\n",
      "loss_grad=0.0003096684445889295\n",
      "grad=2.599226796060018e-07\n",
      "unweighted_step=2.599226796060018e-07\n",
      "learning_rate=1.094154565469697e-05\n",
      "loss_grad=0.0003096684445889295\n",
      "grad=-4.4596582833986543e-07\n",
      "unweighted_step=-4.4596582833986543e-07\n",
      "learning_rate=2.390951939122674e-05\n",
      "pred_val=0.4995312023889057\n",
      "loss_grad=-1.0009375952221886\n",
      "grad=-0.05954422518997371\n",
      "unweighted_step=-0.05954422518997371\n",
      "learning_rate=2.0801005374091183e-05\n",
      "loss_grad=-1.0009375952221886\n",
      "grad=0.0004923852024160123\n",
      "unweighted_step=0.0004923852024160123\n",
      "learning_rate=1.6378325602954593e-05\n",
      "loss_grad=-1.0009375952221886\n",
      "grad=-0.016611872637758176\n",
      "unweighted_step=-0.016611872637758176\n",
      "learning_rate=1.6378325602954593e-05\n",
      "loss_grad=-1.0009375952221886\n",
      "grad=-0.0037392409267573558\n",
      "unweighted_step=-0.0037392409267573558\n",
      "learning_rate=1.4661369172258612e-05\n",
      "loss_grad=-1.0009375952221886\n",
      "grad=0.0009587886747775235\n",
      "unweighted_step=0.0009587886747775235\n",
      "learning_rate=1.4661369172258612e-05\n",
      "loss_grad=-1.0009375952221886\n",
      "grad=-0.29672150346538084\n",
      "unweighted_step=-0.29672150346538084\n",
      "learning_rate=4.599856644520012e-05\n",
      "loss_grad=-1.0009375952221886\n",
      "grad=0.2911881433415375\n",
      "unweighted_step=0.2911881433415375\n",
      "learning_rate=1.660443648580851e-05\n",
      "loss_grad=-1.0009375952221886\n",
      "grad=0.11941149694972644\n",
      "unweighted_step=0.11941149694972644\n",
      "learning_rate=1.1558165725345116e-05\n",
      "loss_grad=-1.0009375952221886\n",
      "grad=-0.11906269942640739\n",
      "unweighted_step=-0.11906269942640739\n",
      "learning_rate=2.5068072009453126e-05\n",
      "pred_val=0.38054518900628925\n",
      "loss_grad=-1.2389096219874216\n",
      "grad=-0.3241986962227292\n",
      "unweighted_step=-0.3241986962227292\n",
      "learning_rate=2.1271795828408883e-05\n",
      "key='mult_a', -0.7421102082729292, current_value=0.0015532303961569378, updated=0.001560126684630824, learning_rate=2.1271795828408883e-05, grad=-0.3241986962227292\n",
      "loss_grad=-1.2389096219874216\n",
      "grad=0.0027884829266522164\n",
      "unweighted_step=0.0027884829266522164\n",
      "learning_rate=1.6744218932091356e-05\n",
      "key='mult_b', 0.7411336457720188, current_value=0.9999958366819804, updated=0.9999957899910118, learning_rate=1.6744218932091356e-05, grad=0.0027884829266522164\n",
      "loss_grad=-1.2389096219874216\n",
      "grad=-4.223666615648993\n",
      "unweighted_step=-4.223666615648993\n",
      "learning_rate=1.6744218932091356e-05\n",
      "key='dif_a', -0.7411336457720188, current_value=-0.033809465631293306, updated=-0.033738743632784715, learning_rate=1.6744218932091356e-05, grad=-4.223666615648993\n",
      "loss_grad=-1.2389096219874216\n",
      "grad=-0.4567377272697206\n",
      "unweighted_step=-0.4567377272697206\n",
      "learning_rate=1.4988905533800543e-05\n",
      "key='dif_b', -0.7411336457720188, current_value=1.0009915958525093, updated=1.000998441851157, learning_rate=1.4988905533800543e-05, grad=-0.4567377272697206\n",
      "loss_grad=-1.2389096219874216\n",
      "grad=0.4121791361038811\n",
      "unweighted_step=0.4121791361038811\n",
      "learning_rate=1.4988905533800543e-05\n",
      "key='add_norm_b', 0.7411336457720188, current_value=0.9991052835755196, updated=0.9990991054613856, learning_rate=1.4988905533800543e-05, grad=0.4121791361038811\n",
      "loss_grad=-1.2389096219874216\n",
      "grad=-0.29903644548244124\n",
      "unweighted_step=-0.29903644548244124\n",
      "learning_rate=4.9722681962710426e-05\n",
      "key='target_normalized_intensity_a', -0.9365385203143217, current_value=0.09921433647654267, updated=0.09922920537061665, learning_rate=4.9722681962710426e-05, grad=-0.29903644548244124\n",
      "loss_grad=-1.2389096219874216\n",
      "grad=0.2921005792689589\n",
      "unweighted_step=0.2921005792689589\n",
      "learning_rate=1.7924429846188056e-05\n",
      "key='query_normalized_intensity_a', 0.9316547541390949, current_value=0.10076693340447886, updated=0.10076169766813772, learning_rate=1.7924429846188056e-05, grad=0.2921005792689589\n",
      "loss_grad=-1.2389096219874216\n",
      "grad=0.2203586781497367\n",
      "unweighted_step=0.2203586781497367\n",
      "learning_rate=1.2461759120053416e-05\n",
      "key='target_normalized_intensity_b', 0.9272597446631383, current_value=0.10030983831703648, updated=0.10030709226026936, learning_rate=1.2461759120053416e-05, grad=0.2203586781497367\n",
      "loss_grad=-1.2389096219874216\n",
      "grad=-0.07437757449531328\n",
      "unweighted_step=-0.07437757449531328\n",
      "learning_rate=2.6928821160984754e-05\n",
      "key='query_normalized_intensity_b', -0.9140928416606247, current_value=0.09948815724254464, updated=0.09949016014294662, learning_rate=2.6928821160984754e-05, grad=-0.07437757449531328\n",
      "pred_val=0.4488741902937724\n",
      "loss_grad=0.8977483805875448\n",
      "grad=0.017404558127240986\n",
      "unweighted_step=0.017404558127240986\n",
      "learning_rate=1.784030351210448e-05\n",
      "loss_grad=0.8977483805875448\n",
      "grad=-0.00013637107943923374\n",
      "unweighted_step=-0.00013637107943923374\n",
      "learning_rate=1.404555238207493e-05\n",
      "loss_grad=0.8977483805875448\n",
      "grad=1.3517008064614169\n",
      "unweighted_step=1.3517008064614169\n",
      "learning_rate=1.404555238207493e-05\n",
      "loss_grad=0.8977483805875448\n",
      "grad=0.12822310405510232\n",
      "unweighted_step=0.12822310405510232\n",
      "learning_rate=1.2573142926450817e-05\n",
      "loss_grad=0.8977483805875448\n",
      "grad=-0.1258971339629875\n",
      "unweighted_step=-0.1258971339629875\n",
      "learning_rate=1.2573142926450817e-05\n",
      "loss_grad=0.8977483805875448\n",
      "grad=0.07469440552433507\n",
      "unweighted_step=0.07469440552433507\n",
      "learning_rate=4.025146681586244e-05\n",
      "loss_grad=0.8977483805875448\n",
      "grad=-0.07414843286226777\n",
      "unweighted_step=-0.07414843286226777\n",
      "learning_rate=1.4523301311663586e-05\n",
      "loss_grad=0.8977483805875448\n",
      "grad=-0.023830484204442108\n",
      "unweighted_step=-0.023830484204442108\n",
      "learning_rate=1.0105378027093607e-05\n",
      "loss_grad=0.8977483805875448\n",
      "grad=0.01669300261570543\n",
      "unweighted_step=0.01669300261570543\n",
      "learning_rate=2.189006370429322e-05\n",
      "pred_val=0.4996126912437489\n",
      "loss_grad=-1.0007746175125023\n",
      "grad=-0.032332474884933564\n",
      "unweighted_step=-0.032332474884933564\n",
      "learning_rate=1.660322592471793e-05\n",
      "loss_grad=-1.0007746175125023\n",
      "grad=0.00024651045912835114\n",
      "unweighted_step=0.00024651045912835114\n",
      "learning_rate=1.3070580692601241e-05\n",
      "loss_grad=-1.0007746175125023\n",
      "grad=-0.01297579498072951\n",
      "unweighted_step=-0.01297579498072951\n",
      "learning_rate=1.3070580692601241e-05\n",
      "loss_grad=-1.0007746175125023\n",
      "grad=-0.002501159060457017\n",
      "unweighted_step=-0.002501159060457017\n",
      "learning_rate=1.1700378504836452e-05\n",
      "loss_grad=-1.0007746175125023\n",
      "grad=0.0007134328214168349\n",
      "unweighted_step=0.0007134328214168349\n",
      "learning_rate=1.1700378504836452e-05\n",
      "loss_grad=-1.0007746175125023\n",
      "grad=-0.04252565232896594\n",
      "unweighted_step=-0.04252565232896594\n",
      "learning_rate=3.8047312151795294e-05\n",
      "loss_grad=-1.0007746175125023\n",
      "grad=0.04138445462003722\n",
      "unweighted_step=0.04138445462003722\n",
      "learning_rate=1.3722691351166082e-05\n",
      "loss_grad=-1.0007746175125023\n",
      "grad=-0.004924485503838\n",
      "unweighted_step=-0.004924485503838\n",
      "learning_rate=9.655239209086399e-06\n",
      "loss_grad=-1.0007746175125023\n",
      "grad=0.008792230414531432\n",
      "unweighted_step=0.008792230414531432\n",
      "learning_rate=2.093659900673134e-05\n",
      "pred_val=0.4461216105950139\n",
      "loss_grad=0.8922432211900277\n",
      "grad=0.1009088515334063\n",
      "unweighted_step=0.1009088515334063\n",
      "learning_rate=1.4688390276967801e-05\n",
      "loss_grad=0.8922432211900277\n",
      "grad=-0.0008544177445357687\n",
      "unweighted_step=-0.0008544177445357687\n",
      "learning_rate=1.15636406149587e-05\n",
      "loss_grad=0.8922432211900277\n",
      "grad=1.4176698558479264\n",
      "unweighted_step=1.4176698558479264\n",
      "learning_rate=1.15636406149587e-05\n",
      "loss_grad=0.8922432211900277\n",
      "grad=0.13965424692284464\n",
      "unweighted_step=0.13965424692284464\n",
      "learning_rate=1.0351412478980687e-05\n",
      "loss_grad=0.8922432211900277\n",
      "grad=-0.1302755635996934\n",
      "unweighted_step=-0.1302755635996934\n",
      "learning_rate=1.0351412478980687e-05\n",
      "loss_grad=0.8922432211900277\n",
      "grad=-0.44429075230232606\n",
      "unweighted_step=-0.44429075230232606\n",
      "learning_rate=3.890794975331959e-05\n",
      "loss_grad=0.8922432211900277\n",
      "grad=0.43818869202827887\n",
      "unweighted_step=0.43818869202827887\n",
      "learning_rate=1.4030588108160156e-05\n",
      "loss_grad=0.8922432211900277\n",
      "grad=0.17235475887521734\n",
      "unweighted_step=0.17235475887521734\n",
      "learning_rate=8.421997136747821e-06\n",
      "loss_grad=0.8922432211900277\n",
      "grad=-0.2150051837362965\n",
      "unweighted_step=-0.2150051837362965\n",
      "learning_rate=1.825207649116645e-05\n",
      "pred_val=0.20141990258975334\n",
      "loss_grad=0.4028398051795067\n",
      "grad=0.0019714204999506306\n",
      "unweighted_step=0.0019714204999506306\n",
      "learning_rate=1.4575810134305701e-05\n",
      "loss_grad=0.4028398051795067\n",
      "grad=-1.719441691763427e-05\n",
      "unweighted_step=-1.719441691763427e-05\n",
      "learning_rate=1.1475222060164159e-05\n",
      "loss_grad=0.4028398051795067\n",
      "grad=2.642774105943752\n",
      "unweighted_step=2.642774105943752\n",
      "learning_rate=1.1475222060164159e-05\n",
      "loss_grad=0.4028398051795067\n",
      "grad=0.24828005417369803\n",
      "unweighted_step=0.24828005417369803\n",
      "learning_rate=1.0272262930672375e-05\n",
      "loss_grad=0.4028398051795067\n",
      "grad=-0.24825916746411594\n",
      "unweighted_step=-0.24825916746411594\n",
      "learning_rate=1.0272262930672375e-05\n",
      "loss_grad=0.4028398051795067\n",
      "grad=0.010558384250155602\n",
      "unweighted_step=0.010558384250155602\n",
      "learning_rate=3.263170459458419e-05\n",
      "loss_grad=0.4028398051795067\n",
      "grad=-0.012046770866001175\n",
      "unweighted_step=-0.012046770866001175\n",
      "learning_rate=1.1768597365468357e-05\n",
      "loss_grad=0.4028398051795067\n",
      "grad=-0.005141653354899969\n",
      "unweighted_step=-0.005141653354899969\n",
      "learning_rate=7.696558978948138e-06\n",
      "loss_grad=0.4028398051795067\n",
      "grad=0.0061325825063236\n",
      "unweighted_step=0.0061325825063236\n",
      "learning_rate=1.6684419498660928e-05\n",
      "pred_val=0.47476306156061326\n",
      "loss_grad=0.9495261231212265\n",
      "grad=0.009166719616450183\n",
      "unweighted_step=0.009166719616450183\n",
      "learning_rate=1.5248742008679601e-05\n",
      "loss_grad=0.9495261231212265\n",
      "grad=-7.306879404032866e-05\n",
      "unweighted_step=-7.306879404032866e-05\n",
      "learning_rate=1.2005111921221385e-05\n",
      "loss_grad=0.9495261231212265\n",
      "grad=0.7080965191206648\n",
      "unweighted_step=0.7080965191206648\n",
      "learning_rate=1.2005111921221385e-05\n",
      "loss_grad=0.9495261231212265\n",
      "grad=0.05656184127028555\n",
      "unweighted_step=0.05656184127028555\n",
      "learning_rate=1.0746603901900553e-05\n",
      "loss_grad=0.9495261231212265\n",
      "grad=-0.05636287625907556\n",
      "unweighted_step=-0.05636287625907556\n",
      "learning_rate=1.0746603901900553e-05\n",
      "loss_grad=0.9495261231212265\n",
      "grad=-0.04008200816479331\n",
      "unweighted_step=-0.04008200816479331\n",
      "learning_rate=3.0368860626285543e-05\n",
      "loss_grad=0.9495261231212265\n",
      "grad=0.03917402389858785\n",
      "unweighted_step=0.03917402389858785\n",
      "learning_rate=1.0951965724588836e-05\n",
      "loss_grad=0.9495261231212265\n",
      "grad=0.009587972246072447\n",
      "unweighted_step=0.009587972246072447\n",
      "learning_rate=6.873550996822299e-06\n",
      "loss_grad=0.9495261231212265\n",
      "grad=-0.010593079501440027\n",
      "unweighted_step=-0.010593079501440027\n",
      "learning_rate=1.4898262635668781e-05\n",
      "pred_val=0.29907712799808617\n",
      "loss_grad=0.5981542559961723\n",
      "grad=0.09327230074115961\n",
      "unweighted_step=0.09327230074115961\n",
      "learning_rate=1.6363178910400562e-05\n",
      "loss_grad=0.5981542559961723\n",
      "grad=-0.0008026234434936777\n",
      "unweighted_step=-0.0008026234434936777\n",
      "learning_rate=1.2882546775973915e-05\n",
      "loss_grad=0.5981542559961723\n",
      "grad=3.162724716324327\n",
      "unweighted_step=3.162724716324327\n",
      "learning_rate=1.2882546775973915e-05\n",
      "loss_grad=0.5981542559961723\n",
      "grad=0.3177171806579892\n",
      "unweighted_step=0.3177171806579892\n",
      "learning_rate=1.153205637378286e-05\n",
      "loss_grad=0.5981542559961723\n",
      "grad=-0.30264324025285694\n",
      "unweighted_step=-0.30264324025285694\n",
      "learning_rate=1.153205637378286e-05\n",
      "loss_grad=0.5981542559961723\n",
      "grad=0.1263814864715589\n",
      "unweighted_step=0.1263814864715589\n",
      "learning_rate=2.686649511401993e-05\n",
      "loss_grad=0.5981542559961723\n",
      "grad=-0.12504664961643783\n",
      "unweighted_step=-0.12504664961643783\n",
      "learning_rate=9.68915335076188e-06\n",
      "loss_grad=0.5981542559961723\n",
      "grad=-0.09235172669234115\n",
      "unweighted_step=-0.09235172669234115\n",
      "learning_rate=6.210019418323482e-06\n",
      "loss_grad=0.5981542559961723\n",
      "grad=0.03815152821367393\n",
      "unweighted_step=0.03815152821367393\n",
      "learning_rate=1.346099282130973e-05\n",
      "pred_val=0.43050823797969345\n",
      "loss_grad=0.8610164759593869\n",
      "grad=0.07330135860608053\n",
      "unweighted_step=0.07330135860608053\n",
      "learning_rate=1.7779279981140913e-05\n",
      "loss_grad=0.8610164759593869\n",
      "grad=-0.0005835752378319535\n",
      "unweighted_step=-0.0005835752378319535\n",
      "learning_rate=1.3997456712777301e-05\n",
      "loss_grad=0.8610164759593869\n",
      "grad=1.7479471778691873\n",
      "unweighted_step=1.7479471778691873\n",
      "learning_rate=1.3997456712777301e-05\n",
      "loss_grad=0.8610164759593869\n",
      "grad=0.16949441096035495\n",
      "unweighted_step=0.16949441096035495\n",
      "learning_rate=1.2530089174788183e-05\n",
      "loss_grad=0.8610164759593869\n",
      "grad=-0.16219863236314455\n",
      "unweighted_step=-0.16219863236314455\n",
      "learning_rate=1.2530089174788183e-05\n",
      "loss_grad=0.8610164759593869\n",
      "grad=-0.10749457380912189\n",
      "unweighted_step=-0.10749457380912189\n",
      "learning_rate=2.4385744031163633e-05\n",
      "loss_grad=0.8610164759593869\n",
      "grad=0.10590864877151386\n",
      "unweighted_step=0.10590864877151386\n",
      "learning_rate=8.794382477251157e-06\n",
      "loss_grad=0.8610164759593869\n",
      "grad=0.03447728293409415\n",
      "unweighted_step=0.03447728293409415\n",
      "learning_rate=5.578255606154372e-06\n",
      "loss_grad=0.8610164759593869\n",
      "grad=-0.05925453377191235\n",
      "unweighted_step=-0.05925453377191235\n",
      "learning_rate=1.2091150435610214e-05\n",
      "pred_val=0.396132837494835\n",
      "loss_grad=0.79226567498967\n",
      "grad=0.02063709547263263\n",
      "unweighted_step=0.02063709547263263\n",
      "learning_rate=1.9437570574869707e-05\n",
      "loss_grad=0.79226567498967\n",
      "grad=-0.00016609792142551491\n",
      "unweighted_step=-0.00016609792142551491\n",
      "learning_rate=1.5303029021138275e-05\n",
      "loss_grad=0.79226567498967\n",
      "grad=2.3594741569460935\n",
      "unweighted_step=2.3594741569460935\n",
      "learning_rate=1.5303029021138275e-05\n",
      "loss_grad=0.79226567498967\n",
      "grad=0.21695891452363017\n",
      "unweighted_step=0.21695891452363017\n",
      "learning_rate=1.3698797018189774e-05\n",
      "loss_grad=0.79226567498967\n",
      "grad=-0.21152208825479746\n",
      "unweighted_step=-0.21152208825479746\n",
      "learning_rate=1.3698797018189774e-05\n",
      "loss_grad=0.79226567498967\n",
      "grad=0.0290853306235045\n",
      "unweighted_step=0.0290853306235045\n",
      "learning_rate=2.1853726356909274e-05\n",
      "loss_grad=0.79226567498967\n",
      "grad=-0.029818398923918174\n",
      "unweighted_step=-0.029818398923918174\n",
      "learning_rate=7.88129553340311e-06\n",
      "loss_grad=0.79226567498967\n",
      "grad=-0.040098166731498526\n",
      "unweighted_step=-0.040098166731498526\n",
      "learning_rate=5.0252635621106875e-06\n",
      "loss_grad=0.79226567498967\n",
      "grad=0.009432208065217294\n",
      "unweighted_step=0.009432208065217294\n",
      "learning_rate=1.089269884907894e-05\n",
      "pred_val=0.3210277040452376\n",
      "loss_grad=-1.3579445919095248\n",
      "grad=-0.43493841240939296\n",
      "unweighted_step=-0.43493841240939296\n",
      "learning_rate=1.5615454510143022e-05\n",
      "loss_grad=-1.3579445919095248\n",
      "grad=0.003737470365445122\n",
      "unweighted_step=0.003737470365445122\n",
      "learning_rate=1.2293901772454848e-05\n",
      "loss_grad=-1.3579445919095248\n",
      "grad=-6.557405006689725\n",
      "unweighted_step=-6.557405006689725\n",
      "learning_rate=1.2293901772454848e-05\n",
      "loss_grad=-1.3579445919095248\n",
      "grad=-0.6778933508605499\n",
      "unweighted_step=-0.6778933508605499\n",
      "learning_rate=1.100511962107588e-05\n",
      "loss_grad=-1.3579445919095248\n",
      "grad=0.6353845038422742\n",
      "unweighted_step=0.6353845038422742\n",
      "learning_rate=1.100511962107588e-05\n",
      "loss_grad=-1.3579445919095248\n",
      "grad=-1.1564883517687814\n",
      "unweighted_step=-1.1564883517687814\n",
      "learning_rate=1.9710224161479153e-05\n",
      "loss_grad=-1.3579445919095248\n",
      "grad=1.1365434501966867\n",
      "unweighted_step=1.1365434501966867\n",
      "learning_rate=7.108243520705618e-06\n",
      "loss_grad=-1.3579445919095248\n",
      "grad=0.6486902318721062\n",
      "unweighted_step=0.6486902318721062\n",
      "learning_rate=4.520560029339433e-06\n",
      "loss_grad=-1.3579445919095248\n",
      "grad=-0.5430033114628884\n",
      "unweighted_step=-0.5430033114628884\n",
      "learning_rate=9.79862570633658e-06\n",
      "pred_val=0.49734366114213324\n",
      "loss_grad=-1.0053126777157335\n",
      "grad=-0.07359132227176542\n",
      "unweighted_step=-0.07359132227176542\n",
      "learning_rate=1.4860951021183579e-05\n",
      "key='mult_a', -0.5056075295002666, current_value=0.0015622634501013524, updated=0.0015633570871372172, learning_rate=1.4860951021183579e-05, grad=-0.07359132227176542\n",
      "loss_grad=-1.0053126777157335\n",
      "grad=0.0005967104101906184\n",
      "unweighted_step=0.0005967104101906184\n",
      "learning_rate=1.1699884691577317e-05\n",
      "key='mult_b', 0.5056065758259493, current_value=0.9999957747411837, updated=0.9999957677597408, learning_rate=1.1699884691577317e-05, grad=0.0005967104101906184\n",
      "loss_grad=-1.0053126777157335\n",
      "grad=-0.08232596915314885\n",
      "unweighted_step=-0.08232596915314885\n",
      "learning_rate=1.1699884691577317e-05\n",
      "key='dif_a', -0.5056065758259493, current_value=-0.03383348181101867, updated=-0.033832518606672454, learning_rate=1.1699884691577317e-05, grad=-0.08232596915314885\n",
      "loss_grad=-1.0053126777157335\n",
      "grad=-0.014084784934905442\n",
      "unweighted_step=-0.014084784934905442\n",
      "learning_rate=1.0473373951310846e-05\n",
      "key='dif_b', -0.5056065758259493, current_value=1.0009909555925982, updated=1.0009911031078178, learning_rate=1.0473373951310846e-05, grad=-0.014084784934905442\n",
      "loss_grad=-1.0053126777157335\n",
      "grad=0.005412435726986274\n",
      "unweighted_step=0.005412435726986274\n",
      "learning_rate=1.0473373951310846e-05\n",
      "key='add_norm_b', 0.5056065758259493, current_value=0.9991066120435718, updated=0.9991065553571085, learning_rate=1.0473373951310846e-05, grad=0.005412435726986274\n",
      "loss_grad=-1.0053126777157335\n",
      "grad=-0.1702340045874409\n",
      "unweighted_step=-0.1702340045874409\n",
      "learning_rate=1.9729105970233463e-05\n",
      "key='target_normalized_intensity_a', -0.6698599008987445, current_value=0.09926736087648007, updated=0.09927071944119631, learning_rate=1.9729105970233463e-05, grad=-0.1702340045874409\n",
      "loss_grad=-1.0053126777157335\n",
      "grad=0.16664180876071297\n",
      "unweighted_step=0.16664180876071297\n",
      "learning_rate=7.115042836257754e-06\n",
      "key='query_normalized_intensity_a', 0.669855131595839, current_value=0.10074820771304742, updated=0.10074702204943978, learning_rate=7.115042836257754e-06, grad=0.16664180876071297\n",
      "loss_grad=-1.0053126777157335\n",
      "grad=0.09190512307131535\n",
      "unweighted_step=0.09190512307131535\n",
      "learning_rate=4.519580771514133e-06\n",
      "key='target_normalized_intensity_b', 0.6659445895943976, current_value=0.10030355296538453, updated=0.1003031375927575, learning_rate=4.519580771514133e-06, grad=0.09190512307131535\n",
      "loss_grad=-1.0053126777157335\n",
      "grad=-0.06687424368627254\n",
      "unweighted_step=-0.06687424368627254\n",
      "learning_rate=9.796465299336301e-06\n",
      "key='query_normalized_intensity_b', -0.6659317312906842, current_value=0.09949901128565314, updated=0.09949966641686082, learning_rate=9.796465299336301e-06, grad=-0.06687424368627254\n",
      "pred_val=0.43640482384865864\n",
      "loss_grad=0.8728096476973173\n",
      "grad=0.1557332157162646\n",
      "unweighted_step=0.1557332157162646\n",
      "learning_rate=1.2990832160347637e-05\n",
      "loss_grad=0.8728096476973173\n",
      "grad=-0.0013572336499966889\n",
      "unweighted_step=-0.0013572336499966889\n",
      "learning_rate=1.0227559661528425e-05\n",
      "loss_grad=0.8728096476973173\n",
      "grad=1.6300697559820994\n",
      "unweighted_step=1.6300697559820994\n",
      "learning_rate=1.0227559661528425e-05\n",
      "loss_grad=0.8728096476973173\n",
      "grad=0.16790356888748525\n",
      "unweighted_step=0.16790356888748525\n",
      "learning_rate=9.15539424261526e-06\n",
      "loss_grad=0.8728096476973173\n",
      "grad=-0.15479209564929644\n",
      "unweighted_step=-0.15479209564929644\n",
      "learning_rate=9.15539424261526e-06\n",
      "loss_grad=0.8728096476973173\n",
      "grad=0.4385033223354117\n",
      "unweighted_step=0.4385033223354117\n",
      "learning_rate=1.676029012621558e-05\n",
      "loss_grad=0.8728096476973173\n",
      "grad=-0.4306905581384604\n",
      "unweighted_step=-0.4306905581384604\n",
      "learning_rate=6.0443835011361464e-06\n",
      "loss_grad=0.8728096476973173\n",
      "grad=-0.23666371953387583\n",
      "unweighted_step=-0.23666371953387583\n",
      "learning_rate=3.84213317863472e-06\n",
      "loss_grad=0.8728096476973173\n",
      "grad=0.19885499401007609\n",
      "unweighted_step=0.19885499401007609\n",
      "learning_rate=8.328075469772066e-06\n",
      "pred_val=0.4400469748097661\n",
      "loss_grad=-1.1199060503804676\n",
      "grad=-0.07037884455317678\n",
      "unweighted_step=-0.07037884455317678\n",
      "learning_rate=1.185959783191013e-05\n",
      "loss_grad=-1.1199060503804676\n",
      "grad=0.0005717866605377964\n",
      "unweighted_step=0.0005717866605377964\n",
      "learning_rate=9.336948810301446e-06\n",
      "loss_grad=-1.1199060503804676\n",
      "grad=-1.9677348448845149\n",
      "unweighted_step=-1.9677348448845149\n",
      "learning_rate=9.336948810301446e-06\n",
      "loss_grad=-1.1199060503804676\n",
      "grad=-0.19460656958407915\n",
      "unweighted_step=-0.19460656958407915\n",
      "learning_rate=8.35814702728925e-06\n",
      "loss_grad=-1.1199060503804676\n",
      "grad=0.18878931701379276\n",
      "unweighted_step=0.18878931701379276\n",
      "learning_rate=8.35814702728925e-06\n",
      "loss_grad=-1.1199060503804676\n",
      "grad=-0.04622122852998801\n",
      "unweighted_step=-0.04622122852998801\n",
      "learning_rate=1.5507282331662205e-05\n",
      "loss_grad=-1.1199060503804676\n",
      "grad=0.0456916861055646\n",
      "unweighted_step=0.0456916861055646\n",
      "learning_rate=5.592500161411824e-06\n",
      "loss_grad=-1.1199060503804676\n",
      "grad=0.013632596426416638\n",
      "unweighted_step=0.013632596426416638\n",
      "learning_rate=3.5537651165163486e-06\n",
      "loss_grad=-1.1199060503804676\n",
      "grad=0.0010097422210235895\n",
      "unweighted_step=0.0010097422210235895\n",
      "learning_rate=8.120332627823509e-06\n",
      "pred_val=0.4746788129159186\n",
      "loss_grad=-1.0506423741681628\n",
      "grad=-0.012997932474584886\n",
      "unweighted_step=-0.012997932474584886\n",
      "learning_rate=1.193621418253596e-05\n",
      "loss_grad=-1.0506423741681628\n",
      "grad=9.376179092188242e-05\n",
      "unweighted_step=9.376179092188242e-05\n",
      "learning_rate=9.39726780204663e-06\n",
      "loss_grad=-1.0506423741681628\n",
      "grad=-0.7856239600608582\n",
      "unweighted_step=-0.7856239600608582\n",
      "learning_rate=9.39726780204663e-06\n",
      "loss_grad=-1.0506423741681628\n",
      "grad=-0.07062942146406709\n",
      "unweighted_step=-0.07062942146406709\n",
      "learning_rate=8.412142718150044e-06\n",
      "loss_grad=-1.0506423741681628\n",
      "grad=0.0700788920847423\n",
      "unweighted_step=0.0700788920847423\n",
      "learning_rate=8.412142718150044e-06\n",
      "loss_grad=-1.0506423741681628\n",
      "grad=-0.043956011473815185\n",
      "unweighted_step=-0.043956011473815185\n",
      "learning_rate=1.5702980300237534e-05\n",
      "loss_grad=-1.0506423741681628\n",
      "grad=0.04360349621431815\n",
      "unweighted_step=0.04360349621431815\n",
      "learning_rate=5.663075094328137e-06\n",
      "loss_grad=-1.0506423741681628\n",
      "grad=0.000388435439820913\n",
      "unweighted_step=0.000388435439820913\n",
      "learning_rate=3.5980909520111454e-06\n",
      "loss_grad=-1.0506423741681628\n",
      "grad=-0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=7.003563094508542e-06\n",
      "pred_val=0.4914059112877446\n",
      "loss_grad=-1.0171881774245108\n",
      "grad=-0.021214801124740397\n",
      "unweighted_step=-0.021214801124740397\n",
      "learning_rate=1.2571580548604746e-05\n",
      "loss_grad=-1.0171881774245108\n",
      "grad=0.00016326468879502093\n",
      "unweighted_step=0.00016326468879502093\n",
      "learning_rate=9.897485525802023e-06\n",
      "loss_grad=-1.0171881774245108\n",
      "grad=-0.2593836596073739\n",
      "unweighted_step=-0.2593836596073739\n",
      "learning_rate=9.897485525802023e-06\n",
      "loss_grad=-1.0171881774245108\n",
      "grad=-0.025831815967562944\n",
      "unweighted_step=-0.025831815967562944\n",
      "learning_rate=8.859922112226915e-06\n",
      "loss_grad=-1.0171881774245108\n",
      "grad=0.024828395192108756\n",
      "unweighted_step=0.024828395192108756\n",
      "learning_rate=8.859922112226915e-06\n",
      "loss_grad=-1.0171881774245108\n",
      "grad=-1.1538862551352223e-05\n",
      "unweighted_step=-1.1538862551352223e-05\n",
      "learning_rate=1.65872131288929e-05\n",
      "loss_grad=-1.0171881774245108\n",
      "grad=-0.0001536494632576645\n",
      "unweighted_step=-0.0001536494632576645\n",
      "learning_rate=4.777881051158186e-06\n",
      "loss_grad=-1.0171881774245108\n",
      "grad=-0.01566554392337392\n",
      "unweighted_step=-0.01566554392337392\n",
      "learning_rate=3.035937955115286e-06\n",
      "loss_grad=-1.0171881774245108\n",
      "grad=0.017976962577835327\n",
      "unweighted_step=0.017976962577835327\n",
      "learning_rate=6.4346201024474e-06\n",
      "pred_val=0.24944058350355003\n",
      "loss_grad=0.49888116700710006\n",
      "grad=0.034039578613488095\n",
      "unweighted_step=0.034039578613488095\n",
      "learning_rate=1.0351249962596802e-05\n",
      "loss_grad=0.49888116700710006\n",
      "grad=-0.00028700935340208127\n",
      "unweighted_step=-0.00028700935340208127\n",
      "learning_rate=8.149440509498666e-06\n",
      "loss_grad=0.49888116700710006\n",
      "grad=3.043772297704973\n",
      "unweighted_step=3.043772297704973\n",
      "learning_rate=8.149440509498666e-06\n",
      "loss_grad=0.49888116700710006\n",
      "grad=0.28918408830664105\n",
      "unweighted_step=0.28918408830664105\n",
      "learning_rate=7.295126422176218e-06\n",
      "loss_grad=0.49888116700710006\n",
      "grad=-0.2833213923978842\n",
      "unweighted_step=-0.2833213923978842\n",
      "learning_rate=7.295126422176218e-06\n",
      "loss_grad=0.49888116700710006\n",
      "grad=0.015550577434386665\n",
      "unweighted_step=0.015550577434386665\n",
      "learning_rate=1.3632119226026559e-05\n",
      "loss_grad=0.49888116700710006\n",
      "grad=-0.01672074530780306\n",
      "unweighted_step=-0.01672074530780306\n",
      "learning_rate=4.6433603268490435e-06\n",
      "loss_grad=0.49888116700710006\n",
      "grad=-0.03282734987072586\n",
      "unweighted_step=-0.03282734987072586\n",
      "learning_rate=2.950572751826327e-06\n",
      "loss_grad=0.49888116700710006\n",
      "grad=0.010193653702285619\n",
      "unweighted_step=0.010193653702285619\n",
      "learning_rate=6.494989000563572e-06\n",
      "pred_val=0.4768767631805966\n",
      "loss_grad=-1.0462464736388069\n",
      "grad=-0.17906278823457772\n",
      "unweighted_step=-0.17906278823457772\n",
      "learning_rate=9.712655839621827e-06\n",
      "loss_grad=-1.0462464736388069\n",
      "grad=0.0015181069844396957\n",
      "unweighted_step=0.0015181069844396957\n",
      "learning_rate=7.646681402066274e-06\n",
      "loss_grad=-1.0462464736388069\n",
      "grad=-0.7220372781739864\n",
      "unweighted_step=-0.7220372781739864\n",
      "learning_rate=7.646681402066274e-06\n",
      "loss_grad=-1.0462464736388069\n",
      "grad=-0.08350770657073003\n",
      "unweighted_step=-0.08350770657073003\n",
      "learning_rate=6.845072060242432e-06\n",
      "loss_grad=-1.0462464736388069\n",
      "grad=0.062345409184131236\n",
      "unweighted_step=0.062345409184131236\n",
      "learning_rate=6.845072060242432e-06\n",
      "loss_grad=-1.0462464736388069\n",
      "grad=-0.13458477018139156\n",
      "unweighted_step=-0.13458477018139156\n",
      "learning_rate=1.2801616010142605e-05\n",
      "loss_grad=-1.0462464736388069\n",
      "grad=0.13027905149984276\n",
      "unweighted_step=0.13027905149984276\n",
      "learning_rate=4.0122229317534195e-06\n",
      "loss_grad=-1.0462464736388069\n",
      "grad=0.08485560685571399\n",
      "unweighted_step=0.08485560685571399\n",
      "learning_rate=2.5494692814643068e-06\n",
      "loss_grad=-1.0462464736388069\n",
      "grad=-0.017801696101733846\n",
      "unweighted_step=-0.017801696101733846\n",
      "learning_rate=5.490273014239599e-06\n",
      "pred_val=0.21159671801660365\n",
      "loss_grad=-1.5768065639667928\n",
      "grad=-0.3177480738147918\n",
      "unweighted_step=-0.3177480738147918\n",
      "learning_rate=9.898689792811809e-06\n",
      "loss_grad=-1.5768065639667928\n",
      "grad=0.0027776397266776327\n",
      "unweighted_step=0.0027776397266776327\n",
      "learning_rate=7.79314414382246e-06\n",
      "loss_grad=-1.5768065639667928\n",
      "grad=-10.23907495760337\n",
      "unweighted_step=-10.23907495760337\n",
      "learning_rate=7.79314414382246e-06\n",
      "loss_grad=-1.5768065639667928\n",
      "grad=-1.0369091129220829\n",
      "unweighted_step=-1.0369091129220829\n",
      "learning_rate=6.976180964713182e-06\n",
      "loss_grad=-1.5768065639667928\n",
      "grad=0.9868769577395268\n",
      "unweighted_step=0.9868769577395268\n",
      "learning_rate=6.976180964713182e-06\n",
      "loss_grad=-1.5768065639667928\n",
      "grad=0.6183767218258638\n",
      "unweighted_step=0.6183767218258638\n",
      "learning_rate=1.1271327037308834e-05\n",
      "loss_grad=-1.5768065639667928\n",
      "grad=-0.607737577773723\n",
      "unweighted_step=-0.607737577773723\n",
      "learning_rate=3.6830652861827736e-06\n",
      "loss_grad=-1.5768065639667928\n",
      "grad=-0.15427545956898603\n",
      "unweighted_step=-0.15427545956898603\n",
      "learning_rate=2.3403374441097693e-06\n",
      "loss_grad=-1.5768065639667928\n",
      "grad=0.35221270172977337\n",
      "unweighted_step=0.35221270172977337\n",
      "learning_rate=5.091379822570305e-06\n",
      "pred_val=0.41438218898271806\n",
      "loss_grad=0.8287643779654361\n",
      "grad=0.04242841992793203\n",
      "unweighted_step=0.04242841992793203\n",
      "learning_rate=8.319087721738096e-06\n",
      "loss_grad=0.8287643779654361\n",
      "grad=-0.00035138696558706395\n",
      "unweighted_step=-0.00035138696558706395\n",
      "learning_rate=6.549538494872394e-06\n",
      "loss_grad=0.8287643779654361\n",
      "grad=2.0624239470070553\n",
      "unweighted_step=2.0624239470070553\n",
      "learning_rate=6.549538494872394e-06\n",
      "loss_grad=0.8287643779654361\n",
      "grad=0.18988463753891496\n",
      "unweighted_step=0.18988463753891496\n",
      "learning_rate=5.862943753170996e-06\n",
      "loss_grad=0.8287643779654361\n",
      "grad=-0.1838412403254147\n",
      "unweighted_step=-0.1838412403254147\n",
      "learning_rate=5.862943753170996e-06\n",
      "loss_grad=0.8287643779654361\n",
      "grad=-0.06764857844342498\n",
      "unweighted_step=-0.06764857844342498\n",
      "learning_rate=1.025430806478683e-05\n",
      "loss_grad=0.8287643779654361\n",
      "grad=0.06599637321804827\n",
      "unweighted_step=0.06599637321804827\n",
      "learning_rate=3.2816824795923428e-06\n",
      "loss_grad=0.8287643779654361\n",
      "grad=0.003676846803465289\n",
      "unweighted_step=0.003676846803465289\n",
      "learning_rate=2.0852752501181073e-06\n",
      "loss_grad=0.8287643779654361\n",
      "grad=-0.02525570128251591\n",
      "unweighted_step=-0.02525570128251591\n",
      "learning_rate=4.5126287468385115e-06\n",
      "pred_val=0.48917105219959695\n",
      "loss_grad=-1.0216578956008062\n",
      "grad=-0.07546259235816559\n",
      "unweighted_step=-0.07546259235816559\n",
      "learning_rate=7.734991610922588e-06\n",
      "loss_grad=-1.0216578956008062\n",
      "grad=0.0006373180041454797\n",
      "unweighted_step=0.0006373180041454797\n",
      "learning_rate=6.089685188729028e-06\n",
      "loss_grad=-1.0216578956008062\n",
      "grad=-0.3309741599367505\n",
      "unweighted_step=-0.3309741599367505\n",
      "learning_rate=6.089685188729028e-06\n",
      "loss_grad=-1.0216578956008062\n",
      "grad=-0.03845603184456348\n",
      "unweighted_step=-0.03845603184456348\n",
      "learning_rate=5.451297334001303e-06\n",
      "loss_grad=-1.0216578956008062\n",
      "grad=0.030287126105613828\n",
      "unweighted_step=0.030287126105613828\n",
      "learning_rate=5.451297334001303e-06\n",
      "loss_grad=-1.0216578956008062\n",
      "grad=-0.21617003512589616\n",
      "unweighted_step=-0.21617003512589616\n",
      "learning_rate=1.0304397114560714e-05\n",
      "loss_grad=-1.0216578956008062\n",
      "grad=0.2120073263256324\n",
      "unweighted_step=0.2120073263256324\n",
      "learning_rate=3.266946677552863e-06\n",
      "loss_grad=-1.0216578956008062\n",
      "grad=0.10676018661487376\n",
      "unweighted_step=0.10676018661487376\n",
      "learning_rate=2.0759069204421194e-06\n",
      "loss_grad=-1.0216578956008062\n",
      "grad=-0.08436499100107127\n",
      "unweighted_step=-0.08436499100107127\n",
      "learning_rate=4.481778755508265e-06\n",
      "pred_val=0.44976143407952685\n",
      "loss_grad=0.8995228681590537\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=7.850198285067765e-06\n",
      "key='mult_a', -0.7163140698530276, current_value=0.001567353346984906, updated=0.001567353346984906, learning_rate=7.850198285067765e-06, grad=0.0\n",
      "loss_grad=0.8995228681590537\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=5.390015554804712e-06\n",
      "key='mult_b', -0.283685931078295, current_value=0.9999957413094666, updated=0.9999957413094666, learning_rate=5.390015554804712e-06, grad=0.0\n",
      "loss_grad=0.8995228681590537\n",
      "grad=1.3290942186339834\n",
      "unweighted_step=1.3290942186339834\n",
      "learning_rate=5.390015554804712e-06\n",
      "key='dif_a', 0.283685931078295, current_value=-0.033771849299862336, updated=-0.03377901313837457, learning_rate=5.390015554804712e-06, grad=1.3290942186339834\n",
      "loss_grad=0.8995228681590537\n",
      "grad=0.11673202278531114\n",
      "unweighted_step=0.11673202278531114\n",
      "learning_rate=4.824974775135279e-06\n",
      "key='dif_b', 0.283685931078295, current_value=1.0009968074471374, updated=1.000996244218072, learning_rate=4.824974775135279e-06, grad=0.11673202278531114\n",
      "loss_grad=0.8995228681590537\n",
      "grad=-0.11673202278531114\n",
      "unweighted_step=-0.11673202278531114\n",
      "learning_rate=4.824974775135279e-06\n",
      "key='add_norm_b', -0.283685931078295, current_value=0.9991012533406435, updated=0.9991018165697089, learning_rate=4.824974775135279e-06, grad=-0.11673202278531114\n",
      "loss_grad=0.8995228681590537\n",
      "grad=-0.0004285003446631494\n",
      "unweighted_step=-0.0004285003446631494\n",
      "learning_rate=1.0844783829754066e-05\n",
      "key='target_normalized_intensity_a', -0.8414744725594714, current_value=0.09926223937592182, updated=0.09926224402291543, learning_rate=1.0844783829754066e-05, grad=-0.0004285003446631494\n",
      "loss_grad=0.8995228681590537\n",
      "grad=-0.0004222530562931977\n",
      "unweighted_step=-0.0004222530562931977\n",
      "learning_rate=2.7842394927118275e-06\n",
      "key='query_normalized_intensity_a', -0.17415053209805093, current_value=0.10075000765507816, updated=0.10075000883073179, learning_rate=2.7842394927118275e-06, grad=-0.0004222530562931977\n",
      "loss_grad=0.8995228681590537\n",
      "grad=0.0001408577118949904\n",
      "unweighted_step=0.0001408577118949904\n",
      "learning_rate=2.1750391457547974e-06\n",
      "key='target_normalized_intensity_b', 0.8258456490132757, current_value=0.1003040568891421, updated=0.10030405658277106, learning_rate=2.1750391457547974e-06, grad=0.0001408577118949904\n",
      "loss_grad=0.8995228681590537\n",
      "grad=0.00011287620094373071\n",
      "unweighted_step=0.00011287620094373071\n",
      "learning_rate=3.824831486898436e-06\n",
      "key='query_normalized_intensity_b', 0.17806061354366143, current_value=0.09949661681778389, updated=0.09949661638605145, learning_rate=3.824831486898436e-06, grad=0.00011287620094373071\n",
      "pred_val=0.43183979143052675\n",
      "loss_grad=0.8636795828610535\n",
      "grad=0.017825200162750136\n",
      "unweighted_step=0.017825200162750136\n",
      "learning_rate=6.614207248404855e-06\n",
      "loss_grad=0.8636795828610535\n",
      "grad=-0.00013765348091401863\n",
      "unweighted_step=-0.00013765348091401863\n",
      "learning_rate=5.349875514243166e-06\n",
      "loss_grad=0.8636795828610535\n",
      "grad=1.7219111243749952\n",
      "unweighted_step=1.7219111243749952\n",
      "learning_rate=5.349875514243166e-06\n",
      "loss_grad=0.8636795828610535\n",
      "grad=0.15229913566108588\n",
      "unweighted_step=0.15229913566108588\n",
      "learning_rate=4.789042655605546e-06\n",
      "loss_grad=0.8636795828610535\n",
      "grad=-0.149865699041917\n",
      "unweighted_step=-0.149865699041917\n",
      "learning_rate=4.789042655605546e-06\n",
      "loss_grad=0.8636795828610535\n",
      "grad=-0.0013240164038909256\n",
      "unweighted_step=-0.0013240164038909256\n",
      "learning_rate=1.1671385951240931e-05\n",
      "loss_grad=0.8636795828610535\n",
      "grad=0.0004883583608610741\n",
      "unweighted_step=0.0004883583608610741\n",
      "learning_rate=2.57229599970461e-06\n",
      "loss_grad=0.8636795828610535\n",
      "grad=-0.012019585876932066\n",
      "unweighted_step=-0.012019585876932066\n",
      "learning_rate=1.796850196223785e-06\n",
      "loss_grad=0.8636795828610535\n",
      "grad=0.0006779456347045653\n",
      "unweighted_step=0.0006779456347045653\n",
      "learning_rate=3.7357476887422523e-06\n",
      "pred_val=0.49106333028499627\n",
      "loss_grad=-1.0178733394300075\n",
      "grad=-0.03912708777702107\n",
      "unweighted_step=-0.03912708777702107\n",
      "learning_rate=6.142770070825969e-06\n",
      "loss_grad=-1.0178733394300075\n",
      "grad=0.0003125353122646325\n",
      "unweighted_step=0.0003125353122646325\n",
      "learning_rate=4.5673147437319424e-06\n",
      "loss_grad=-1.0178733394300075\n",
      "grad=-0.27097631606142775\n",
      "unweighted_step=-0.27097631606142775\n",
      "learning_rate=4.5673147437319424e-06\n",
      "loss_grad=-1.0178733394300075\n",
      "grad=-0.026294660909873917\n",
      "unweighted_step=-0.026294660909873917\n",
      "learning_rate=4.088518521800168e-06\n",
      "loss_grad=-1.0178733394300075\n",
      "grad=0.023942939680969246\n",
      "unweighted_step=0.023942939680969246\n",
      "learning_rate=4.088518521800168e-06\n",
      "loss_grad=-1.0178733394300075\n",
      "grad=-0.172195218738819\n",
      "unweighted_step=-0.172195218738819\n",
      "learning_rate=1.269975860032384e-05\n",
      "loss_grad=-1.0178733394300075\n",
      "grad=0.1692153032611598\n",
      "unweighted_step=0.1692153032611598\n",
      "learning_rate=2.603005895917557e-06\n",
      "loss_grad=-1.0178733394300075\n",
      "grad=0.04866475054332316\n",
      "unweighted_step=0.04866475054332316\n",
      "learning_rate=1.6835379904318166e-06\n",
      "loss_grad=-1.0178733394300075\n",
      "grad=-0.045452425767019036\n",
      "unweighted_step=-0.045452425767019036\n",
      "learning_rate=3.2188900132368388e-06\n",
      "pred_val=0.45493678285062533\n",
      "loss_grad=-1.0901264342987493\n",
      "grad=-0.06802209227225964\n",
      "unweighted_step=-0.06802209227225964\n",
      "learning_rate=6.230991168550822e-06\n",
      "loss_grad=-1.0901264342987493\n",
      "grad=0.0005842845283224705\n",
      "unweighted_step=0.0005842845283224705\n",
      "learning_rate=4.461635199350173e-06\n",
      "loss_grad=-1.0901264342987493\n",
      "grad=-1.4492197571004337\n",
      "unweighted_step=-1.4492197571004337\n",
      "learning_rate=4.461635199350173e-06\n",
      "loss_grad=-1.0901264342987493\n",
      "grad=-0.1452808911253502\n",
      "unweighted_step=-0.1452808911253502\n",
      "learning_rate=3.993917470893126e-06\n",
      "loss_grad=-1.0901264342987493\n",
      "grad=0.13599093371638188\n",
      "unweighted_step=0.13599093371638188\n",
      "learning_rate=3.993917470893126e-06\n",
      "loss_grad=-1.0901264342987493\n",
      "grad=-0.18335392815522408\n",
      "unweighted_step=-0.18335392815522408\n",
      "learning_rate=1.3894238112963088e-05\n",
      "loss_grad=-1.0901264342987493\n",
      "grad=0.18051204182611524\n",
      "unweighted_step=0.18051204182611524\n",
      "learning_rate=2.7486944570938396e-06\n",
      "loss_grad=-1.0901264342987493\n",
      "grad=0.09917602604823983\n",
      "unweighted_step=0.09917602604823983\n",
      "learning_rate=1.714631610214419e-06\n",
      "loss_grad=-1.0901264342987493\n",
      "grad=-0.06727877193782959\n",
      "unweighted_step=-0.06727877193782959\n",
      "learning_rate=3.157160480670634e-06\n",
      "pred_val=0.4903838706032261\n",
      "loss_grad=-1.0192322587935478\n",
      "grad=-0.05382441514820731\n",
      "unweighted_step=-0.05382441514820731\n",
      "learning_rate=6.5872847817342244e-06\n",
      "loss_grad=-1.0192322587935478\n",
      "grad=0.0004441801551725496\n",
      "unweighted_step=0.0004441801551725496\n",
      "learning_rate=4.63309980552813e-06\n",
      "loss_grad=-1.0192322587935478\n",
      "grad=-0.29256653189373316\n",
      "unweighted_step=-0.29256653189373316\n",
      "learning_rate=4.63309980552813e-06\n",
      "loss_grad=-1.0192322587935478\n",
      "grad=-0.03259563474763542\n",
      "unweighted_step=-0.03259563474763542\n",
      "learning_rate=4.1474072690625715e-06\n",
      "loss_grad=-1.0192322587935478\n",
      "grad=0.027731364905017937\n",
      "unweighted_step=0.027731364905017937\n",
      "learning_rate=4.1474072690625715e-06\n",
      "loss_grad=-1.0192322587935478\n",
      "grad=0.08575828377303184\n",
      "unweighted_step=0.08575828377303184\n",
      "learning_rate=1.1156689079593752e-05\n",
      "loss_grad=-1.0192322587935478\n",
      "grad=-0.08514929072208371\n",
      "unweighted_step=-0.08514929072208371\n",
      "learning_rate=2.259468960538291e-06\n",
      "loss_grad=-1.0192322587935478\n",
      "grad=-0.04917825173550371\n",
      "unweighted_step=-0.04917825173550371\n",
      "learning_rate=1.4416029214656676e-06\n",
      "loss_grad=-1.0192322587935478\n",
      "grad=0.06435195599360514\n",
      "unweighted_step=0.06435195599360514\n",
      "learning_rate=2.7138592728038055e-06\n",
      "pred_val=0.43642189740593873\n",
      "loss_grad=-1.1271562051881225\n",
      "grad=-0.20271512808529882\n",
      "unweighted_step=-0.20271512808529882\n",
      "learning_rate=7.1049824201521685e-06\n",
      "loss_grad=-1.1271562051881225\n",
      "grad=0.0017150547140740977\n",
      "unweighted_step=0.0017150547140740977\n",
      "learning_rate=4.953781867499434e-06\n",
      "loss_grad=-1.1271562051881225\n",
      "grad=-2.1079670727462085\n",
      "unweighted_step=-2.1079670727462085\n",
      "learning_rate=4.953781867499434e-06\n",
      "loss_grad=-1.1271562051881225\n",
      "grad=-0.21619207200835627\n",
      "unweighted_step=-0.21619207200835627\n",
      "learning_rate=4.434471906282523e-06\n",
      "loss_grad=-1.1271562051881225\n",
      "grad=0.19842178335744273\n",
      "unweighted_step=0.19842178335744273\n",
      "learning_rate=4.434471906282523e-06\n",
      "loss_grad=-1.1271562051881225\n",
      "grad=-0.19069735193999665\n",
      "unweighted_step=-0.19069735193999665\n",
      "learning_rate=1.058227381291876e-05\n",
      "loss_grad=-1.1271562051881225\n",
      "grad=0.18607071071280507\n",
      "unweighted_step=0.18607071071280507\n",
      "learning_rate=2.121624043615048e-06\n",
      "loss_grad=-1.1271562051881225\n",
      "grad=0.06508905941982064\n",
      "unweighted_step=0.06508905941982064\n",
      "learning_rate=1.340139019791449e-06\n",
      "loss_grad=-1.1271562051881225\n",
      "grad=-0.008276395392796607\n",
      "unweighted_step=-0.008276395392796607\n",
      "learning_rate=2.497308722298634e-06\n",
      "pred_val=0.4086352178374953\n",
      "loss_grad=0.8172704356749906\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=7.73942340410406e-06\n",
      "loss_grad=0.8172704356749906\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=4.03927548067426e-06\n",
      "loss_grad=0.8172704356749906\n",
      "grad=2.1616549806758996\n",
      "unweighted_step=2.1616549806758996\n",
      "learning_rate=4.03927548067426e-06\n",
      "loss_grad=0.8172704356749906\n",
      "grad=0.19174283397036374\n",
      "unweighted_step=0.19174283397036374\n",
      "learning_rate=3.615834148512371e-06\n",
      "loss_grad=0.8172704356749906\n",
      "grad=-0.19174283397036374\n",
      "unweighted_step=-0.19174283397036374\n",
      "learning_rate=3.615834148512371e-06\n",
      "loss_grad=0.8172704356749906\n",
      "grad=-0.0006339004428860026\n",
      "unweighted_step=-0.0006339004428860026\n",
      "learning_rate=1.0838967093221199e-05\n",
      "loss_grad=0.8172704356749906\n",
      "grad=-0.0007493527330047814\n",
      "unweighted_step=-0.0007493527330047814\n",
      "learning_rate=1.8680980975368668e-06\n",
      "loss_grad=0.8172704356749906\n",
      "grad=0.00017065222184502147\n",
      "unweighted_step=0.00017065222184502147\n",
      "learning_rate=1.3599846714699543e-06\n",
      "loss_grad=0.8172704356749906\n",
      "grad=0.0002799582649971272\n",
      "unweighted_step=0.0002799582649971272\n",
      "learning_rate=2.2223479374341e-06\n",
      "pred_val=0.44144464867936417\n",
      "loss_grad=0.8828892973587283\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=8.471941340603366e-06\n",
      "loss_grad=0.8828892973587283\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=3.868398531799077e-06\n",
      "loss_grad=0.8828892973587283\n",
      "grad=1.5165116703269805\n",
      "unweighted_step=1.5165116703269805\n",
      "learning_rate=3.868398531799077e-06\n",
      "loss_grad=0.8828892973587283\n",
      "grad=0.130765891476233\n",
      "unweighted_step=0.130765891476233\n",
      "learning_rate=3.4628704029365558e-06\n",
      "loss_grad=0.8828892973587283\n",
      "grad=-0.130765891476233\n",
      "unweighted_step=-0.130765891476233\n",
      "learning_rate=3.4628704029365558e-06\n",
      "loss_grad=0.8828892973587283\n",
      "grad=-0.0005588416587696497\n",
      "unweighted_step=-0.0005588416587696497\n",
      "learning_rate=1.1512375381136515e-05\n",
      "loss_grad=0.8828892973587283\n",
      "grad=-0.0004128763387451494\n",
      "unweighted_step=-0.0004128763387451494\n",
      "learning_rate=1.8498877190721114e-06\n",
      "loss_grad=0.8828892973587283\n",
      "grad=0.00010271926906752292\n",
      "unweighted_step=0.00010271926906752292\n",
      "learning_rate=1.4380536745522523e-06\n",
      "loss_grad=0.8828892973587283\n",
      "grad=0.0001396191417002727\n",
      "unweighted_step=0.0001396191417002727\n",
      "learning_rate=2.211121923644805e-06\n",
      "pred_val=0.3924755335815962\n",
      "loss_grad=0.7849510671631924\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=9.296462912272415e-06\n",
      "loss_grad=0.7849510671631924\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=3.979994361306562e-06\n",
      "loss_grad=0.7849510671631924\n",
      "grad=2.420565644244179\n",
      "unweighted_step=2.420565644244179\n",
      "learning_rate=3.979994361306562e-06\n",
      "loss_grad=0.7849510671631924\n",
      "grad=0.22322567321560638\n",
      "unweighted_step=0.22322567321560638\n",
      "learning_rate=3.5627675288184896e-06\n",
      "loss_grad=0.7849510671631924\n",
      "grad=-0.22322567321560638\n",
      "unweighted_step=-0.22322567321560638\n",
      "learning_rate=3.5627675288184896e-06\n",
      "loss_grad=0.7849510671631924\n",
      "grad=-0.0006001698398918027\n",
      "unweighted_step=-0.0006001698398918027\n",
      "learning_rate=1.2445617202674887e-05\n",
      "loss_grad=0.7849510671631924\n",
      "grad=-0.0009458554203912737\n",
      "unweighted_step=-0.0009458554203912737\n",
      "learning_rate=1.9333656739490986e-06\n",
      "loss_grad=0.7849510671631924\n",
      "grad=0.00018901597983512407\n",
      "unweighted_step=0.00018901597983512407\n",
      "learning_rate=1.5512316090625755e-06\n",
      "loss_grad=0.7849510671631924\n",
      "grad=0.00047711813503301733\n",
      "unweighted_step=0.00047711813503301733\n",
      "learning_rate=2.3160933665901385e-06\n",
      "pred_val=0.47911784736553403\n",
      "loss_grad=-1.041764305268932\n",
      "grad=-0.17084230637213274\n",
      "unweighted_step=-0.17084230637213274\n",
      "learning_rate=1.0213669632277261e-05\n",
      "loss_grad=-1.041764305268932\n",
      "grad=0.001522564028191084\n",
      "unweighted_step=0.001522564028191084\n",
      "learning_rate=3.3255876302368277e-06\n",
      "loss_grad=-1.041764305268932\n",
      "grad=-0.6509391836996962\n",
      "unweighted_step=-0.6509391836996962\n",
      "learning_rate=3.3255876302368277e-06\n",
      "loss_grad=-1.041764305268932\n",
      "grad=-0.07594726132541733\n",
      "unweighted_step=-0.07594726132541733\n",
      "learning_rate=2.9769629169420766e-06\n",
      "loss_grad=-1.041764305268932\n",
      "grad=0.06024786446479059\n",
      "unweighted_step=0.06024786446479059\n",
      "learning_rate=2.9769629169420766e-06\n",
      "loss_grad=-1.041764305268932\n",
      "grad=-0.2212294921855637\n",
      "unweighted_step=-0.2212294921855637\n",
      "learning_rate=1.3572345237430164e-05\n",
      "loss_grad=-1.041764305268932\n",
      "grad=0.21572706955372048\n",
      "unweighted_step=0.21572706955372048\n",
      "learning_rate=1.5997383340897961e-06\n",
      "loss_grad=-1.041764305268932\n",
      "grad=0.14037820252004443\n",
      "unweighted_step=0.14037820252004443\n",
      "learning_rate=1.6898358308884233e-06\n",
      "loss_grad=-1.041764305268932\n",
      "grad=-0.09815962179308646\n",
      "unweighted_step=-0.09815962179308646\n",
      "learning_rate=1.9137019177783666e-06\n",
      "pred_val=0.2866467819020759\n",
      "loss_grad=0.5732935638041518\n",
      "grad=0.00473629278548389\n",
      "unweighted_step=0.00473629278548389\n",
      "learning_rate=8.177769147316263e-06\n",
      "key='mult_a', 0.002230162041159167, current_value=0.0015714394122253927, updated=0.001571400679916379, learning_rate=8.177769147316263e-06, grad=0.00473629278548389\n",
      "loss_grad=0.5732935638041518\n",
      "grad=-3.73112839792844e-05\n",
      "unweighted_step=-3.73112839792844e-05\n",
      "learning_rate=3.10015275369925e-06\n",
      "key='mult_b', -0.44070672454206866, current_value=0.9999957223942261, updated=0.9999957225098968, learning_rate=3.10015275369925e-06, grad=-3.73112839792844e-05\n",
      "loss_grad=0.5732935638041518\n",
      "grad=3.163187582328954\n",
      "unweighted_step=3.163187582328954\n",
      "learning_rate=3.10015275369925e-06\n",
      "key='dif_a', 0.44070672454206866, current_value=-0.033790790799326856, updated=-0.03380059716402068, learning_rate=3.10015275369925e-06, grad=3.163187582328954\n",
      "loss_grad=0.5732935638041518\n",
      "grad=0.29116412851514584\n",
      "unweighted_step=0.29116412851514584\n",
      "learning_rate=2.7751606064162847e-06\n",
      "key='dif_b', 0.44070672454206866, current_value=1.0009955811375328, updated=1.0009947731103135, learning_rate=2.7751606064162847e-06, grad=0.29116412851514584\n",
      "loss_grad=0.5732935638041518\n",
      "grad=-0.2901368010119444\n",
      "unweighted_step=-0.2901368010119444\n",
      "learning_rate=2.7751606064162847e-06\n",
      "key='add_norm_b', -0.44070672454206866, current_value=0.9991026604272715, updated=0.9991034656034921, learning_rate=2.7751606064162847e-06, grad=-0.2901368010119444\n",
      "loss_grad=0.5732935638041518\n",
      "grad=0.01937537868316656\n",
      "unweighted_step=0.01937537868316656\n",
      "learning_rate=1.0922126899022135e-05\n",
      "key='target_normalized_intensity_a', 0.015779810085391144, current_value=0.09927107848690403, updated=0.09927086686655934, learning_rate=1.0922126899022135e-05, grad=0.01937537868316656\n",
      "loss_grad=0.5732935638041518\n",
      "grad=-0.021025813632465792\n",
      "unweighted_step=-0.021025813632465792\n",
      "learning_rate=1.4978053901137865e-06\n",
      "key='query_normalized_intensity_a', -0.454271631379002, current_value=0.10074852743919521, updated=0.1007485589317722, learning_rate=1.4978053901137865e-06, grad=-0.021025813632465792\n",
      "loss_grad=0.5732935638041518\n",
      "grad=-0.0061723087187403125\n",
      "unweighted_step=-0.0061723087187403125\n",
      "learning_rate=1.3608661267864401e-06\n",
      "key='target_normalized_intensity_b', -0.017748197608385474, current_value=0.10030357197885952, updated=0.10030358037854538, learning_rate=1.3608661267864401e-06, grad=-0.0061723087187403125\n",
      "loss_grad=0.5732935638041518\n",
      "grad=0.0025385076969238617\n",
      "unweighted_step=0.0025385076969238617\n",
      "learning_rate=1.7928871837949932e-06\n",
      "key='query_normalized_intensity_b', 0.45622857481791373, current_value=0.09949700440854557, updated=0.09949699985728766, learning_rate=1.7928871837949932e-06, grad=0.0025385076969238617\n",
      "pred_val=0.0033099802348683914\n",
      "loss_grad=0.006619960469736783\n",
      "grad=3.16384114771901e-05\n",
      "unweighted_step=3.16384114771901e-05\n",
      "learning_rate=7.771616352500507e-06\n",
      "loss_grad=0.006619960469736783\n",
      "grad=-2.8750727040587183e-07\n",
      "unweighted_step=-2.8750727040587183e-07\n",
      "learning_rate=3.150083840863718e-06\n",
      "loss_grad=0.006619960469736783\n",
      "grad=0.003689227438011111\n",
      "unweighted_step=0.003689227438011111\n",
      "learning_rate=3.150083840863718e-06\n",
      "loss_grad=0.006619960469736783\n",
      "grad=0.00037183632027147867\n",
      "unweighted_step=0.00037183632027147867\n",
      "learning_rate=2.8198573672352557e-06\n",
      "loss_grad=0.006619960469736783\n",
      "grad=-0.00036414423978775324\n",
      "unweighted_step=-0.00036414423978775324\n",
      "learning_rate=2.8198573672352557e-06\n",
      "loss_grad=0.006619960469736783\n",
      "grad=-1.0502453694034248e-05\n",
      "unweighted_step=-1.0502453694034248e-05\n",
      "learning_rate=1.0350168190841761e-05\n",
      "loss_grad=0.006619960469736783\n",
      "grad=8.501491170335041e-06\n",
      "unweighted_step=8.501491170335041e-06\n",
      "learning_rate=1.3208535458998093e-06\n",
      "loss_grad=0.006619960469736783\n",
      "grad=-1.2576417252325294e-05\n",
      "unweighted_step=-1.2576417252325294e-05\n",
      "learning_rate=1.2964457585876329e-06\n",
      "loss_grad=0.006619960469736783\n",
      "grad=-2.1494596193272876e-05\n",
      "unweighted_step=-2.1494596193272876e-05\n",
      "learning_rate=1.5805478699044296e-06\n",
      "pred_val=0.3559082663292374\n",
      "loss_grad=-1.2881834673415251\n",
      "grad=-0.4058273051448144\n",
      "unweighted_step=-0.4058273051448144\n",
      "learning_rate=6.7988644111538585e-06\n",
      "loss_grad=-1.2881834673415251\n",
      "grad=0.00354840193161548\n",
      "unweighted_step=0.00354840193161548\n",
      "learning_rate=2.6522036258902576e-06\n",
      "loss_grad=-1.2881834673415251\n",
      "grad=-5.2010975204677035\n",
      "unweighted_step=-5.2010975204677035\n",
      "learning_rate=2.6522036258902576e-06\n",
      "loss_grad=-1.2881834673415251\n",
      "grad=-0.5517992268861632\n",
      "unweighted_step=-0.5517992268861632\n",
      "learning_rate=2.374170438531594e-06\n",
      "loss_grad=-1.2881834673415251\n",
      "grad=0.5018326092441724\n",
      "unweighted_step=0.5018326092441724\n",
      "learning_rate=2.374170438531594e-06\n",
      "loss_grad=-1.2881834673415251\n",
      "grad=0.5438653557564176\n",
      "unweighted_step=0.5438653557564176\n",
      "learning_rate=9.068646443616791e-06\n",
      "loss_grad=-1.2881834673415251\n",
      "grad=-0.5389236979129217\n",
      "unweighted_step=-0.5389236979129217\n",
      "learning_rate=1.2007488247954818e-06\n",
      "loss_grad=-1.2881834673415251\n",
      "grad=-0.2747930577673173\n",
      "unweighted_step=-0.2747930577673173\n",
      "learning_rate=1.330582620715721e-06\n",
      "loss_grad=-1.2881834673415251\n",
      "grad=0.4558638349701758\n",
      "unweighted_step=0.4558638349701758\n",
      "learning_rate=1.4370612188252251e-06\n",
      "pred_val=0.21485405787074674\n",
      "loss_grad=-1.5702918842585065\n",
      "grad=-0.696602175199818\n",
      "unweighted_step=-0.696602175199818\n",
      "learning_rate=6.7133100096644575e-06\n",
      "loss_grad=-1.5702918842585065\n",
      "grad=0.006295053875855298\n",
      "unweighted_step=0.006295053875855298\n",
      "learning_rate=2.5752194315872033e-06\n",
      "loss_grad=-1.5702918842585065\n",
      "grad=-10.19265158595399\n",
      "unweighted_step=-10.19265158595399\n",
      "learning_rate=2.5752194315872033e-06\n",
      "loss_grad=-1.5702918842585065\n",
      "grad=-1.0768272810263997\n",
      "unweighted_step=-1.0768272810263997\n",
      "learning_rate=2.305256575144829e-06\n",
      "loss_grad=-1.5702918842585065\n",
      "grad=0.9790649921598438\n",
      "unweighted_step=0.9790649921598438\n",
      "learning_rate=2.305256575144829e-06\n",
      "loss_grad=-1.5702918842585065\n",
      "grad=0.8867763051870744\n",
      "unweighted_step=0.8867763051870744\n",
      "learning_rate=8.960654670019526e-06\n",
      "loss_grad=-1.5702918842585065\n",
      "grad=-0.8781408626460996\n",
      "unweighted_step=-0.8781408626460996\n",
      "learning_rate=1.2061944442673983e-06\n",
      "loss_grad=-1.5702918842585065\n",
      "grad=-0.3093024635563471\n",
      "unweighted_step=-0.3093024635563471\n",
      "learning_rate=1.4146296136337068e-06\n",
      "loss_grad=-1.5702918842585065\n",
      "grad=0.656463333214335\n",
      "unweighted_step=0.656463333214335\n",
      "learning_rate=1.4436840182820623e-06\n",
      "pred_val=0.1578709962992963\n",
      "loss_grad=0.3157419925985926\n",
      "grad=0.039863776734539405\n",
      "unweighted_step=0.039863776734539405\n",
      "learning_rate=5.748552416446831e-06\n",
      "loss_grad=0.3157419925985926\n",
      "grad=-0.0003499416482301777\n",
      "unweighted_step=-0.0003499416482301777\n",
      "learning_rate=2.226311323059141e-06\n",
      "loss_grad=0.3157419925985926\n",
      "grad=2.0834739781455576\n",
      "unweighted_step=2.0834739781455576\n",
      "learning_rate=2.226311323059141e-06\n",
      "loss_grad=0.3157419925985926\n",
      "grad=0.20847948525958146\n",
      "unweighted_step=0.20847948525958146\n",
      "learning_rate=1.9929248563639065e-06\n",
      "loss_grad=0.3157419925985926\n",
      "grad=-0.19964259699360598\n",
      "unweighted_step=-0.19964259699360598\n",
      "learning_rate=1.9929248563639065e-06\n",
      "loss_grad=0.3157419925985926\n",
      "grad=-0.006714767050675634\n",
      "unweighted_step=-0.006714767050675634\n",
      "learning_rate=7.669909359411713e-06\n",
      "loss_grad=0.3157419925985926\n",
      "grad=0.005915039646539609\n",
      "unweighted_step=0.005915039646539609\n",
      "learning_rate=1.0225301194422542e-06\n",
      "loss_grad=0.3157419925985926\n",
      "grad=-0.020990284307376206\n",
      "unweighted_step=-0.020990284307376206\n",
      "learning_rate=1.5300390283525477e-06\n",
      "loss_grad=0.3157419925985926\n",
      "grad=-0.019025906406576476\n",
      "unweighted_step=-0.019025906406576476\n",
      "learning_rate=1.2238047549887366e-06\n",
      "pred_val=0.16913419087373857\n",
      "loss_grad=0.33826838174747714\n",
      "grad=0.024449326140257747\n",
      "unweighted_step=0.024449326140257747\n",
      "learning_rate=5.622923021743845e-06\n",
      "loss_grad=0.33826838174747714\n",
      "grad=-0.00021486140973017286\n",
      "unweighted_step=-0.00021486140973017286\n",
      "learning_rate=2.186809047595313e-06\n",
      "loss_grad=0.33826838174747714\n",
      "grad=2.242113900159083\n",
      "unweighted_step=2.242113900159083\n",
      "learning_rate=2.186809047595313e-06\n",
      "loss_grad=0.33826838174747714\n",
      "grad=0.22314401973376485\n",
      "unweighted_step=0.22314401973376485\n",
      "learning_rate=1.9575636443719457e-06\n",
      "loss_grad=0.33826838174747714\n",
      "grad=-0.21802309404078532\n",
      "unweighted_step=-0.21802309404078532\n",
      "learning_rate=1.9575636443719457e-06\n",
      "loss_grad=0.33826838174747714\n",
      "grad=-0.008877659318001063\n",
      "unweighted_step=-0.008877659318001063\n",
      "learning_rate=7.50099543861461e-06\n",
      "loss_grad=0.33826838174747714\n",
      "grad=0.007703544810874896\n",
      "unweighted_step=0.007703544810874896\n",
      "learning_rate=9.958075253404561e-07\n",
      "loss_grad=0.33826838174747714\n",
      "grad=0.0021908746137216323\n",
      "unweighted_step=0.0021908746137216323\n",
      "learning_rate=1.2381207563694959e-06\n",
      "loss_grad=0.33826838174747714\n",
      "grad=-0.02427193996709099\n",
      "unweighted_step=-0.02427193996709099\n",
      "learning_rate=1.191799638168134e-06\n",
      "pred_val=0.23530094206120242\n",
      "loss_grad=0.47060188412240483\n",
      "grad=0.002835477113980074\n",
      "unweighted_step=0.002835477113980074\n",
      "learning_rate=5.842627233668919e-06\n",
      "loss_grad=0.47060188412240483\n",
      "grad=-2.4149545761244887e-05\n",
      "unweighted_step=-2.4149545761244887e-05\n",
      "learning_rate=2.2767488140759018e-06\n",
      "loss_grad=0.47060188412240483\n",
      "grad=2.955477436487108\n",
      "unweighted_step=2.955477436487108\n",
      "learning_rate=2.2767488140759018e-06\n",
      "loss_grad=0.47060188412240483\n",
      "grad=0.27858399321567284\n",
      "unweighted_step=0.27858399321567284\n",
      "learning_rate=2.0380749342073832e-06\n",
      "loss_grad=0.47060188412240483\n",
      "grad=-0.27796783462576863\n",
      "unweighted_step=-0.27796783462576863\n",
      "learning_rate=2.0380749342073832e-06\n",
      "loss_grad=0.47060188412240483\n",
      "grad=-0.014581682232959125\n",
      "unweighted_step=-0.014581682232959125\n",
      "learning_rate=7.793448240356716e-06\n",
      "loss_grad=0.47060188412240483\n",
      "grad=0.012531243930685856\n",
      "unweighted_step=0.012531243930685856\n",
      "learning_rate=1.0325857859752744e-06\n",
      "loss_grad=0.47060188412240483\n",
      "grad=0.004324946375341261\n",
      "unweighted_step=0.004324946375341261\n",
      "learning_rate=1.18191540454097e-06\n",
      "loss_grad=0.47060188412240483\n",
      "grad=-0.0061586875613103284\n",
      "unweighted_step=-0.0061586875613103284\n",
      "learning_rate=1.235805562860374e-06\n",
      "pred_val=0.4664300878757762\n",
      "loss_grad=0.9328601757515524\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=4.852088796385648e-06\n",
      "loss_grad=0.9328601757515524\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=2.4374056734431866e-06\n",
      "loss_grad=0.9328601757515524\n",
      "grad=0.9243561998885502\n",
      "unweighted_step=0.9243561998885502\n",
      "learning_rate=2.4374056734431866e-06\n",
      "loss_grad=0.9328601757515524\n",
      "grad=0.07641273742366535\n",
      "unweighted_step=0.07641273742366535\n",
      "learning_rate=2.1818899725904568e-06\n",
      "loss_grad=0.9328601757515524\n",
      "grad=-0.07641273742366535\n",
      "unweighted_step=-0.07641273742366535\n",
      "learning_rate=2.1818899725904568e-06\n",
      "loss_grad=0.9328601757515524\n",
      "grad=-0.00029816881850227265\n",
      "unweighted_step=-0.00029816881850227265\n",
      "learning_rate=8.335048205975714e-06\n",
      "loss_grad=0.9328601757515524\n",
      "grad=-0.0002937950252268589\n",
      "unweighted_step=-0.0002937950252268589\n",
      "learning_rate=8.586296201408734e-07\n",
      "loss_grad=0.9328601757515524\n",
      "grad=5.320706357187389e-05\n",
      "unweighted_step=5.320706357187389e-05\n",
      "learning_rate=1.214184239349146e-06\n",
      "loss_grad=0.9328601757515524\n",
      "grad=3.25373882676363e-05\n",
      "unweighted_step=3.25373882676363e-05\n",
      "learning_rate=1.0276193302755196e-06\n",
      "pred_val=0.11725314522602945\n",
      "loss_grad=0.2345062904520589\n",
      "grad=0.0007194102530312732\n",
      "unweighted_step=0.0007194102530312732\n",
      "learning_rate=4.535578497102976e-06\n",
      "loss_grad=0.2345062904520589\n",
      "grad=-6.405462574377117e-06\n",
      "unweighted_step=-6.405462574377117e-06\n",
      "learning_rate=2.645272694673398e-06\n",
      "loss_grad=0.2345062904520589\n",
      "grad=1.450606879667741\n",
      "unweighted_step=1.450606879667741\n",
      "learning_rate=2.645272694673398e-06\n",
      "loss_grad=0.2345062904520589\n",
      "grad=0.1368082993641958\n",
      "unweighted_step=0.1368082993641958\n",
      "learning_rate=2.367966083841052e-06\n",
      "loss_grad=0.2345062904520589\n",
      "grad=-0.13664103122683346\n",
      "unweighted_step=-0.13664103122683346\n",
      "learning_rate=2.367966083841052e-06\n",
      "loss_grad=0.2345062904520589\n",
      "grad=-0.004185335153213906\n",
      "unweighted_step=-0.004185335153213906\n",
      "learning_rate=9.041419645141145e-06\n",
      "loss_grad=0.2345062904520589\n",
      "grad=0.003212898856386118\n",
      "unweighted_step=0.003212898856386118\n",
      "learning_rate=8.02160362407123e-07\n",
      "loss_grad=0.2345062904520589\n",
      "grad=0.0017688374768493537\n",
      "unweighted_step=0.0017688374768493537\n",
      "learning_rate=1.2914683730477678e-06\n",
      "loss_grad=0.2345062904520589\n",
      "grad=-0.0020446792238637515\n",
      "unweighted_step=-0.0020446792238637515\n",
      "learning_rate=9.600338161882022e-07\n",
      "pred_val=0.16758857676946365\n",
      "loss_grad=0.3351771535389273\n",
      "grad=0.05965627960211788\n",
      "unweighted_step=0.05965627960211788\n",
      "learning_rate=4.6144255344989065e-06\n",
      "loss_grad=0.3351771535389273\n",
      "grad=-0.0005385657439332975\n",
      "unweighted_step=-0.0005385657439332975\n",
      "learning_rate=2.890333505801306e-06\n",
      "loss_grad=0.3351771535389273\n",
      "grad=2.2212032649693585\n",
      "unweighted_step=2.2212032649693585\n",
      "learning_rate=2.890333505801306e-06\n",
      "loss_grad=0.3351771535389273\n",
      "grad=0.22402485076426337\n",
      "unweighted_step=0.22402485076426337\n",
      "learning_rate=2.587336922393147e-06\n",
      "loss_grad=0.3351771535389273\n",
      "grad=-0.21349226524755754\n",
      "unweighted_step=-0.21349226524755754\n",
      "learning_rate=2.587336922393147e-06\n",
      "loss_grad=0.3351771535389273\n",
      "grad=-0.0238312155039127\n",
      "unweighted_step=-0.0238312155039127\n",
      "learning_rate=9.876607824144476e-06\n",
      "loss_grad=0.3351771535389273\n",
      "grad=0.023002290658337834\n",
      "unweighted_step=0.023002290658337834\n",
      "learning_rate=8.15890650720572e-07\n",
      "loss_grad=0.3351771535389273\n",
      "grad=-0.00637379566497331\n",
      "unweighted_step=-0.00637379566497331\n",
      "learning_rate=1.0566464410946815e-06\n",
      "loss_grad=0.3351771535389273\n",
      "grad=-0.03663484086080962\n",
      "unweighted_step=-0.03663484086080962\n",
      "learning_rate=9.764652663955344e-07\n",
      "pred_val=0.16615449314707947\n",
      "loss_grad=0.33230898629415895\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=3.882152842476374e-06\n",
      "key='mult_a', -0.1376931346073817, current_value=0.0015781743604860634, updated=0.0015781743604860634, learning_rate=3.882152842476374e-06, grad=0.0\n",
      "loss_grad=0.33230898629415895\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=3.1687319307476745e-06\n",
      "key='mult_b', -0.9877350651606855, current_value=0.9999956997660742, updated=0.9999956997660742, learning_rate=3.1687319307476745e-06, grad=0.0\n",
      "loss_grad=0.33230898629415895\n",
      "grad=2.198008396492224\n",
      "unweighted_step=2.198008396492224\n",
      "learning_rate=3.1687319307476745e-06\n",
      "key='dif_a', 0.9877350651606855, current_value=-0.033789346817725795, updated=-0.033796311717115814, learning_rate=3.1687319307476745e-06, grad=2.198008396492224\n",
      "loss_grad=0.33230898629415895\n",
      "grad=0.21237097292770427\n",
      "unweighted_step=0.21237097292770427\n",
      "learning_rate=2.8365505590043105e-06\n",
      "key='dif_b', 0.9877350651606855, current_value=1.0009960741033817, updated=1.0009954717023797, learning_rate=2.8365505590043105e-06, grad=0.21237097292770427\n",
      "loss_grad=0.33230898629415895\n",
      "grad=-0.21237097292770427\n",
      "unweighted_step=-0.21237097292770427\n",
      "learning_rate=2.8365505590043105e-06\n",
      "key='add_norm_b', -0.9877350651606855, current_value=0.9991024520461567, updated=0.9991030544471586, learning_rate=2.8365505590043105e-06, grad=-0.21237097292770427\n",
      "loss_grad=0.33230898629415895\n",
      "grad=-0.00011810516187204488\n",
      "unweighted_step=-0.00011810516187204488\n",
      "learning_rate=1.0826606960257273e-05\n",
      "key='target_normalized_intensity_a', -0.9872892775292135, current_value=0.09925849618911176, updated=0.09925849746778992, learning_rate=1.0826606960257273e-05, grad=-0.00011810516187204488\n",
      "loss_grad=0.33230898629415895\n",
      "grad=-0.0012795644863587996\n",
      "unweighted_step=-0.0012795644863587996\n",
      "learning_rate=6.86524400770363e-07\n",
      "key='query_normalized_intensity_a', -0.13813893714001857, current_value=0.10075021748967834, updated=0.10075021836813058, learning_rate=6.86524400770363e-07, grad=-0.0012795644863587996\n",
      "loss_grad=0.33230898629415895\n",
      "grad=4.909692032389852e-05\n",
      "unweighted_step=4.909692032389852e-05\n",
      "learning_rate=9.942121288318106e-07\n",
      "key='target_normalized_intensity_b', 0.46970923027577305, current_value=0.10030441225565528, updated=0.10030441220684252, learning_rate=9.942121288318106e-07, grad=4.909692032389852e-05\n",
      "loss_grad=0.33230898629415895\n",
      "grad=0.0007464857249380525\n",
      "unweighted_step=0.0007464857249380525\n",
      "learning_rate=8.216391351628974e-07\n",
      "key='query_normalized_intensity_b', 0.13814084821759565, current_value=0.099495494585799, updated=0.0994954939724571, learning_rate=8.216391351628974e-07, grad=0.0007464857249380525\n",
      "pred_val=0.4906819305963518\n",
      "loss_grad=-1.0186361388072964\n",
      "grad=-0.06917563629623874\n",
      "unweighted_step=-0.06917563629623874\n",
      "learning_rate=3.7682270694383853e-06\n",
      "loss_grad=-1.0186361388072964\n",
      "grad=0.0005565868261474024\n",
      "unweighted_step=0.0005565868261474024\n",
      "learning_rate=2.540815188196221e-06\n",
      "loss_grad=-1.0186361388072964\n",
      "grad=-0.2840165188428176\n",
      "unweighted_step=-0.2840165188428176\n",
      "learning_rate=2.540815188196221e-06\n",
      "loss_grad=-1.0186361388072964\n",
      "grad=-0.029284337525151468\n",
      "unweighted_step=-0.029284337525151468\n",
      "learning_rate=2.27445896336964e-06\n",
      "loss_grad=-1.0186361388072964\n",
      "grad=0.025266156672738807\n",
      "unweighted_step=0.025266156672738807\n",
      "learning_rate=2.27445896336964e-06\n",
      "loss_grad=-1.0186361388072964\n",
      "grad=0.04241412805874692\n",
      "unweighted_step=0.04241412805874692\n",
      "learning_rate=8.681927667661636e-06\n",
      "loss_grad=-1.0186361388072964\n",
      "grad=-0.04269036705076967\n",
      "unweighted_step=-0.04269036705076967\n",
      "learning_rate=6.664235433883109e-07\n",
      "loss_grad=-1.0186361388072964\n",
      "grad=-0.02158042158161601\n",
      "unweighted_step=-0.02158042158161601\n",
      "learning_rate=8.74452930325556e-07\n",
      "loss_grad=-1.0186361388072964\n",
      "grad=0.03304108691229\n",
      "unweighted_step=0.03304108691229\n",
      "learning_rate=7.975824674637787e-07\n",
      "pred_val=0.0911008283188793\n",
      "loss_grad=-1.8177983433622413\n",
      "grad=-0.4228231662591752\n",
      "unweighted_step=-0.4228231662591752\n",
      "learning_rate=3.9013471709570964e-06\n",
      "loss_grad=-1.8177983433622413\n",
      "grad=0.003815147687126665\n",
      "unweighted_step=0.003815147687126665\n",
      "learning_rate=2.416111648740558e-06\n",
      "loss_grad=-1.8177983433622413\n",
      "grad=-10.264512054932329\n",
      "unweighted_step=-10.264512054932329\n",
      "learning_rate=2.416111648740558e-06\n",
      "loss_grad=-1.8177983433622413\n",
      "grad=-1.0700801907242659\n",
      "unweighted_step=-1.0700801907242659\n",
      "learning_rate=2.1628282220246897e-06\n",
      "loss_grad=-1.8177983433622413\n",
      "grad=1.004901493395858\n",
      "unweighted_step=1.004901493395858\n",
      "learning_rate=2.1628282220246897e-06\n",
      "loss_grad=-1.8177983433622413\n",
      "grad=-0.8419760662989211\n",
      "unweighted_step=-0.8419760662989211\n",
      "learning_rate=8.239554766296424e-06\n",
      "loss_grad=-1.8177983433622413\n",
      "grad=0.8293921733663662\n",
      "unweighted_step=0.8293921733663662\n",
      "learning_rate=5.762161724671161e-07\n",
      "loss_grad=-1.8177983433622413\n",
      "grad=0.5842949991259485\n",
      "unweighted_step=0.5842949991259485\n",
      "learning_rate=7.959517099960323e-07\n",
      "loss_grad=-1.8177983433622413\n",
      "grad=-0.37035468022825985\n",
      "unweighted_step=-0.37035468022825985\n",
      "learning_rate=6.896212551373867e-07\n",
      "pred_val=0.4730057244881584\n",
      "loss_grad=-1.053988551023683\n",
      "grad=-0.11761903499415934\n",
      "unweighted_step=-0.11761903499415934\n",
      "learning_rate=4.165325946185442e-06\n",
      "loss_grad=-1.053988551023683\n",
      "grad=0.0009879402860068832\n",
      "unweighted_step=0.0009879402860068832\n",
      "learning_rate=2.477625694406684e-06\n",
      "loss_grad=-1.053988551023683\n",
      "grad=-0.8463576146956517\n",
      "unweighted_step=-0.8463576146956517\n",
      "learning_rate=2.477625694406684e-06\n",
      "loss_grad=-1.053988551023683\n",
      "grad=-0.08622371821105013\n",
      "unweighted_step=-0.08622371821105013\n",
      "learning_rate=2.217893688095748e-06\n",
      "loss_grad=-1.053988551023683\n",
      "grad=0.07864882525165465\n",
      "unweighted_step=0.07864882525165465\n",
      "learning_rate=2.217893688095748e-06\n",
      "loss_grad=-1.053988551023683\n",
      "grad=-0.22225584567153953\n",
      "unweighted_step=-0.22225584567153953\n",
      "learning_rate=8.441616234431938e-06\n",
      "loss_grad=-1.053988551023683\n",
      "grad=0.217672576087821\n",
      "unweighted_step=0.217672576087821\n",
      "learning_rate=5.66028549450246e-07\n",
      "loss_grad=-1.053988551023683\n",
      "grad=0.07323057113143887\n",
      "unweighted_step=0.07323057113143887\n",
      "learning_rate=8.000222835600415e-07\n",
      "loss_grad=-1.053988551023683\n",
      "grad=-0.05556021087781565\n",
      "unweighted_step=-0.05556021087781565\n",
      "learning_rate=6.774285570056608e-07\n",
      "pred_val=0.4005474706983613\n",
      "loss_grad=0.8010949413967225\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=4.514512494054105e-06\n",
      "loss_grad=0.8010949413967225\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=2.0744417456087585e-06\n",
      "loss_grad=0.8010949413967225\n",
      "grad=2.296584257624559\n",
      "unweighted_step=2.296584257624559\n",
      "learning_rate=2.0744417456087585e-06\n",
      "loss_grad=0.8010949413967225\n",
      "grad=0.20626102891299675\n",
      "unweighted_step=0.20626102891299675\n",
      "learning_rate=1.856975920250845e-06\n",
      "loss_grad=0.8010949413967225\n",
      "grad=-0.20626102891299675\n",
      "unweighted_step=-0.20626102891299675\n",
      "learning_rate=1.856975920250845e-06\n",
      "loss_grad=0.8010949413967225\n",
      "grad=-0.0004942785040584969\n",
      "unweighted_step=-0.0004942785040584969\n",
      "learning_rate=8.96720539206217e-06\n",
      "loss_grad=0.8010949413967225\n",
      "grad=-0.0009736062407152971\n",
      "unweighted_step=-0.0009736062407152971\n",
      "learning_rate=4.861280188832475e-07\n",
      "loss_grad=0.8010949413967225\n",
      "grad=0.00010451481397927059\n",
      "unweighted_step=0.00010451481397927059\n",
      "learning_rate=8.420690931723144e-07\n",
      "loss_grad=0.8010949413967225\n",
      "grad=0.00042643880469120924\n",
      "unweighted_step=0.00042643880469120924\n",
      "learning_rate=5.818028373511894e-07\n",
      "pred_val=0.4719469850888952\n",
      "loss_grad=-1.0561060298222096\n",
      "grad=-0.12812117841417783\n",
      "unweighted_step=-0.12812117841417783\n",
      "learning_rate=4.929467851732731e-06\n",
      "loss_grad=-1.0561060298222096\n",
      "grad=0.0010748314520620942\n",
      "unweighted_step=0.0010748314520620942\n",
      "learning_rate=1.9320624027185112e-06\n",
      "loss_grad=-1.0561060298222096\n",
      "grad=-0.8813800207407386\n",
      "unweighted_step=-0.8813800207407386\n",
      "learning_rate=1.9320624027185112e-06\n",
      "loss_grad=-1.0561060298222096\n",
      "grad=-0.09540469740137351\n",
      "unweighted_step=-0.09540469740137351\n",
      "learning_rate=1.7295223478147874e-06\n",
      "loss_grad=-1.0561060298222096\n",
      "grad=0.08407813105383594\n",
      "unweighted_step=0.08407813105383594\n",
      "learning_rate=1.7295223478147874e-06\n",
      "loss_grad=-1.0561060298222096\n",
      "grad=-0.08968830755205354\n",
      "unweighted_step=-0.08968830755205354\n",
      "learning_rate=9.694722270863373e-06\n",
      "loss_grad=-1.0561060298222096\n",
      "grad=0.08691700269436324\n",
      "unweighted_step=0.08691700269436324\n",
      "learning_rate=4.475197060844702e-07\n",
      "loss_grad=-1.0561060298222096\n",
      "grad=0.06547675414873136\n",
      "unweighted_step=0.06547675414873136\n",
      "learning_rate=9.063008807340089e-07\n",
      "loss_grad=-1.0561060298222096\n",
      "grad=-0.034315595529892606\n",
      "unweighted_step=-0.034315595529892606\n",
      "learning_rate=5.355960150195612e-07\n",
      "pred_val=0.14175347902849464\n",
      "loss_grad=0.2835069580569893\n",
      "grad=0.004504400384583967\n",
      "unweighted_step=0.004504400384583967\n",
      "learning_rate=3.963499503126567e-06\n",
      "loss_grad=0.2835069580569893\n",
      "grad=-3.854669269805005e-05\n",
      "unweighted_step=-3.854669269805005e-05\n",
      "learning_rate=1.7085566095003952e-06\n",
      "loss_grad=0.2835069580569893\n",
      "grad=1.8393806943242235\n",
      "unweighted_step=1.8393806943242235\n",
      "learning_rate=1.7085566095003952e-06\n",
      "loss_grad=0.2835069580569893\n",
      "grad=0.1771013113429641\n",
      "unweighted_step=0.1771013113429641\n",
      "learning_rate=1.5294468928538632e-06\n",
      "loss_grad=0.2835069580569893\n",
      "grad=-0.17585813583407417\n",
      "unweighted_step=-0.17585813583407417\n",
      "learning_rate=1.5294468928538632e-06\n",
      "loss_grad=0.2835069580569893\n",
      "grad=-0.022687973595655296\n",
      "unweighted_step=-0.022687973595655296\n",
      "learning_rate=1.0572728850453101e-05\n",
      "loss_grad=0.2835069580569893\n",
      "grad=0.021255475653618437\n",
      "unweighted_step=0.021255475653618437\n",
      "learning_rate=4.521246721047017e-07\n",
      "loss_grad=0.2835069580569893\n",
      "grad=0.005708907572862981\n",
      "unweighted_step=0.005708907572862981\n",
      "learning_rate=9.86181571527187e-07\n",
      "loss_grad=0.2835069580569893\n",
      "grad=-0.012483077113235177\n",
      "unweighted_step=-0.012483077113235177\n",
      "learning_rate=5.411072791471966e-07\n",
      "pred_val=0.07068400439510068\n",
      "loss_grad=0.14136800879020137\n",
      "grad=0.01899459896837749\n",
      "unweighted_step=0.01899459896837749\n",
      "learning_rate=3.773334886171472e-06\n",
      "loss_grad=0.14136800879020137\n",
      "grad=-0.00017155527314761398\n",
      "unweighted_step=-0.00017155527314761398\n",
      "learning_rate=1.695159396921131e-06\n",
      "loss_grad=0.14136800879020137\n",
      "grad=0.7092038532045878\n",
      "unweighted_step=0.7092038532045878\n",
      "learning_rate=1.695159396921131e-06\n",
      "loss_grad=0.14136800879020137\n",
      "grad=0.07203159434167303\n",
      "unweighted_step=0.07203159434167303\n",
      "learning_rate=1.5174541236132525e-06\n",
      "loss_grad=0.14136800879020137\n",
      "grad=-0.06828395013000121\n",
      "unweighted_step=-0.06828395013000121\n",
      "learning_rate=1.5174541236132525e-06\n",
      "loss_grad=0.14136800879020137\n",
      "grad=-0.034362844992041036\n",
      "unweighted_step=-0.034362844992041036\n",
      "learning_rate=1.1580127099428686e-05\n",
      "loss_grad=0.14136800879020137\n",
      "grad=0.03370504842671939\n",
      "unweighted_step=0.03370504842671939\n",
      "learning_rate=4.770570812097453e-07\n",
      "loss_grad=0.14136800879020137\n",
      "grad=0.008430589905016696\n",
      "unweighted_step=0.008430589905016696\n",
      "learning_rate=1.0789513080680919e-06\n",
      "loss_grad=0.14136800879020137\n",
      "grad=-0.02382867498844996\n",
      "unweighted_step=-0.02382867498844996\n",
      "learning_rate=5.709466305252409e-07\n",
      "pred_val=0.4053745443914084\n",
      "loss_grad=0.8107490887828168\n",
      "grad=0.14616139461156089\n",
      "unweighted_step=0.14616139461156089\n",
      "learning_rate=3.8714812730653345e-06\n",
      "loss_grad=0.8107490887828168\n",
      "grad=-0.0012386486679812245\n",
      "unweighted_step=-0.0012386486679812245\n",
      "learning_rate=1.7732712858969344e-06\n",
      "loss_grad=0.8107490887828168\n",
      "grad=2.223568363976214\n",
      "unweighted_step=2.223568363976214\n",
      "learning_rate=1.7732712858969344e-06\n",
      "loss_grad=0.8107490887828168\n",
      "grad=0.2164209991231712\n",
      "unweighted_step=0.2164209991231712\n",
      "learning_rate=1.5873774643001745e-06\n",
      "loss_grad=0.8107490887828168\n",
      "grad=-0.20544598634486722\n",
      "unweighted_step=-0.20544598634486722\n",
      "learning_rate=1.5873774643001745e-06\n",
      "loss_grad=0.8107490887828168\n",
      "grad=0.0004313230459476651\n",
      "unweighted_step=0.0004313230459476651\n",
      "learning_rate=9.291415092801458e-06\n",
      "loss_grad=0.8107490887828168\n",
      "grad=0.00045747908929586985\n",
      "unweighted_step=0.00045747908929586985\n",
      "learning_rate=5.14063588509073e-07\n",
      "loss_grad=0.8107490887828168\n",
      "grad=-0.009810262333615191\n",
      "unweighted_step=-0.009810262333615191\n",
      "learning_rate=8.663603361448483e-07\n",
      "loss_grad=0.8107490887828168\n",
      "grad=-0.019226633777814927\n",
      "unweighted_step=-0.019226633777814927\n",
      "learning_rate=6.152363829936157e-07\n",
      "pred_val=0.4825642854677451\n",
      "loss_grad=-1.0348714290645098\n",
      "grad=-0.02991767911167466\n",
      "unweighted_step=-0.02991767911167466\n",
      "learning_rate=3.240409469582255e-06\n",
      "loss_grad=-1.0348714290645098\n",
      "grad=0.00024000774570313412\n",
      "unweighted_step=0.00024000774570313412\n",
      "learning_rate=1.4664249749331919e-06\n",
      "loss_grad=-1.0348714290645098\n",
      "grad=-0.5351206315618234\n",
      "unweighted_step=-0.5351206315618234\n",
      "learning_rate=1.4664249749331919e-06\n",
      "loss_grad=-1.0348714290645098\n",
      "grad=-0.05521617269703835\n",
      "unweighted_step=-0.05521617269703835\n",
      "learning_rate=1.3126981623223505e-06\n",
      "loss_grad=-1.0348714290645098\n",
      "grad=0.05293353349346721\n",
      "unweighted_step=0.05293353349346721\n",
      "learning_rate=1.3126981623223505e-06\n",
      "loss_grad=-1.0348714290645098\n",
      "grad=-0.043780585972185086\n",
      "unweighted_step=-0.043780585972185086\n",
      "learning_rate=8.815886761530014e-06\n",
      "loss_grad=-1.0348714290645098\n",
      "grad=0.04300726909516745\n",
      "unweighted_step=0.04300726909516745\n",
      "learning_rate=5.597053651064189e-07\n",
      "loss_grad=-1.0348714290645098\n",
      "grad=0.009615482155275145\n",
      "unweighted_step=0.009615482155275145\n",
      "learning_rate=8.217578602193623e-07\n",
      "loss_grad=-1.0348714290645098\n",
      "grad=-0.0032624116833607597\n",
      "unweighted_step=-0.0032624116833607597\n",
      "learning_rate=6.698609115941415e-07\n",
      "pred_val=0.4310160245363317\n",
      "loss_grad=0.8620320490726634\n",
      "grad=0.1305203789950096\n",
      "unweighted_step=0.1305203789950096\n",
      "learning_rate=3.018449939837236e-06\n",
      "key='mult_a', 0.43834209654823497, current_value=0.0015806476590601414, updated=0.0015802536898300165, learning_rate=3.018449939837236e-06, grad=0.1305203789950096\n",
      "loss_grad=0.8620320490726634\n",
      "grad=-0.0011006560963066092\n",
      "unweighted_step=-0.0011006560963066092\n",
      "learning_rate=1.3733360891395525e-06\n",
      "key='mult_b', -0.455066147524571, current_value=0.999995686810859, updated=0.9999956883224298, learning_rate=1.3733360891395525e-06, grad=-0.0011006560963066092\n",
      "loss_grad=0.8620320490726634\n",
      "grad=1.7441229174443056\n",
      "unweighted_step=1.7441229174443056\n",
      "learning_rate=1.3733360891395525e-06\n",
      "key='dif_a', 0.455066147524571, current_value=-0.03377925734346151, updated=-0.03378165261040793, learning_rate=1.3733360891395525e-06, grad=1.7441229174443056\n",
      "loss_grad=0.8620320490726634\n",
      "grad=0.17730931888256146\n",
      "unweighted_step=0.17730931888256146\n",
      "learning_rate=1.2293678785350652e-06\n",
      "key='dif_b', 0.455066147524571, current_value=1.0009971746945343, updated=1.0009969567161532, learning_rate=1.2293678785350652e-06, grad=0.17730931888256146\n",
      "loss_grad=0.8620320490726634\n",
      "grad=-0.1690927621787435\n",
      "unweighted_step=-0.1690927621787435\n",
      "learning_rate=1.2293678785350652e-06\n",
      "key='add_norm_b', -0.455066147524571, current_value=0.9991015159410539, updated=0.9991017238182642, learning_rate=1.2293678785350652e-06, grad=-0.1690927621787435\n",
      "loss_grad=0.8620320490726634\n",
      "grad=0.07181549892931853\n",
      "unweighted_step=0.07181549892931853\n",
      "learning_rate=7.719099303979295e-06\n",
      "key='target_normalized_intensity_a', 0.2519655378149129, current_value=0.09926883663038508, updated=0.09926828227941728, learning_rate=7.719099303979295e-06, grad=0.07181549892931853\n",
      "loss_grad=0.8620320490726634\n",
      "grad=-0.06981900826075892\n",
      "unweighted_step=-0.06981900826075892\n",
      "learning_rate=4.50902491023007e-07\n",
      "key='query_normalized_intensity_a', -0.018689588805800794, current_value=0.10074955728021635, updated=0.1007495887617811, learning_rate=4.50902491023007e-07, grad=-0.06981900826075892\n",
      "loss_grad=0.8620320490726634\n",
      "grad=-0.0342652489832292\n",
      "unweighted_step=-0.0342652489832292\n",
      "learning_rate=7.19647293608542e-07\n",
      "key='target_normalized_intensity_b', -0.2524709870798088, current_value=0.10030383386297302, updated=0.10030385852186671, learning_rate=7.19647293608542e-07, grad=-0.0342652489832292\n",
      "loss_grad=0.8620320490726634\n",
      "grad=0.014393688776929595\n",
      "unweighted_step=0.014393688776929595\n",
      "learning_rate=5.39644557148791e-07\n",
      "key='query_normalized_intensity_b', 0.01868959067208753, current_value=0.09949581316706357, updated=0.09949580539958777, learning_rate=5.39644557148791e-07, grad=0.014393688776929595\n",
      "pred_val=0.44648376671711154\n",
      "loss_grad=-1.107032466565777\n",
      "grad=-0.16775756903327657\n",
      "unweighted_step=-0.16775756903327657\n",
      "learning_rate=2.6690603916022523e-06\n",
      "loss_grad=-1.107032466565777\n",
      "grad=0.0014177684394662364\n",
      "unweighted_step=0.0014177684394662364\n",
      "learning_rate=1.2109254701813953e-06\n",
      "loss_grad=-1.107032466565777\n",
      "grad=-1.7481621735195558\n",
      "unweighted_step=-1.7481621735195558\n",
      "learning_rate=1.2109254701813953e-06\n",
      "loss_grad=-1.107032466565777\n",
      "grad=-0.17818487752689247\n",
      "unweighted_step=-0.17818487752689247\n",
      "learning_rate=1.083982928952001e-06\n",
      "loss_grad=-1.107032466565777\n",
      "grad=0.16428481091156197\n",
      "unweighted_step=0.16428481091156197\n",
      "learning_rate=1.083982928952001e-06\n",
      "loss_grad=-1.107032466565777\n",
      "grad=-0.09375633204783229\n",
      "unweighted_step=-0.09375633204783229\n",
      "learning_rate=7.041402287644251e-06\n",
      "loss_grad=-1.107032466565777\n",
      "grad=0.09085520269771136\n",
      "unweighted_step=0.09085520269771136\n",
      "learning_rate=4.2709328914954696e-07\n",
      "loss_grad=-1.107032466565777\n",
      "grad=0.019643265108362916\n",
      "unweighted_step=0.019643265108362916\n",
      "learning_rate=6.564114195431157e-07\n",
      "loss_grad=-1.107032466565777\n",
      "grad=0.01841129087440542\n",
      "unweighted_step=0.01841129087440542\n",
      "learning_rate=5.141751896735811e-07\n",
      "pred_val=0.4745919091112609\n",
      "loss_grad=0.9491838182225218\n",
      "grad=0.01149489739551178\n",
      "unweighted_step=0.01149489739551178\n",
      "learning_rate=2.4231749572421295e-06\n",
      "loss_grad=0.9491838182225218\n",
      "grad=-8.466572063625621e-05\n",
      "unweighted_step=-8.466572063625621e-05\n",
      "learning_rate=1.1008886255578329e-06\n",
      "loss_grad=0.9491838182225218\n",
      "grad=0.7132588913907895\n",
      "unweighted_step=0.7132588913907895\n",
      "learning_rate=1.1008886255578329e-06\n",
      "loss_grad=0.9491838182225218\n",
      "grad=0.062359785989719904\n",
      "unweighted_step=0.062359785989719904\n",
      "learning_rate=9.85481357992545e-07\n",
      "loss_grad=0.9491838182225218\n",
      "grad=-0.06152898607158651\n",
      "unweighted_step=-0.06152898607158651\n",
      "learning_rate=9.85481357992545e-07\n",
      "loss_grad=0.9491838182225218\n",
      "grad=0.03995040738252398\n",
      "unweighted_step=0.03995040738252398\n",
      "learning_rate=6.294291305267028e-06\n",
      "loss_grad=0.9491838182225218\n",
      "grad=-0.039636182294199386\n",
      "unweighted_step=-0.039636182294199386\n",
      "learning_rate=3.7430529285254776e-07\n",
      "loss_grad=0.9491838182225218\n",
      "grad=-0.001898369471924275\n",
      "unweighted_step=-0.001898369471924275\n",
      "learning_rate=5.867893550269144e-07\n",
      "loss_grad=0.9491838182225218\n",
      "grad=0.0003603582444191844\n",
      "unweighted_step=0.0003603582444191844\n",
      "learning_rate=5.277502987025764e-07\n",
      "pred_val=0.22102395707071\n",
      "loss_grad=0.44204791414142\n",
      "grad=0.079081766459085\n",
      "unweighted_step=0.079081766459085\n",
      "learning_rate=2.432717004941379e-06\n",
      "loss_grad=0.44204791414142\n",
      "grad=-0.0007016668577670644\n",
      "unweighted_step=-0.0007016668577670644\n",
      "learning_rate=1.1059141607015934e-06\n",
      "loss_grad=0.44204791414142\n",
      "grad=2.841876234077786\n",
      "unweighted_step=2.841876234077786\n",
      "learning_rate=1.1059141607015934e-06\n",
      "loss_grad=0.44204791414142\n",
      "grad=0.2855169603200004\n",
      "unweighted_step=0.2855169603200004\n",
      "learning_rate=9.899800612065988e-07\n",
      "loss_grad=0.44204791414142\n",
      "grad=-0.2662435965198815\n",
      "unweighted_step=-0.2662435965198815\n",
      "learning_rate=9.899800612065988e-07\n",
      "loss_grad=0.44204791414142\n",
      "grad=0.06500556612387759\n",
      "unweighted_step=0.06500556612387759\n",
      "learning_rate=6.275085582472266e-06\n",
      "loss_grad=0.44204791414142\n",
      "grad=-0.06461441816594392\n",
      "unweighted_step=-0.06461441816594392\n",
      "learning_rate=3.698888121423127e-07\n",
      "loss_grad=0.44204791414142\n",
      "grad=-0.08205987838736331\n",
      "unweighted_step=-0.08205987838736331\n",
      "learning_rate=5.850100113767619e-07\n",
      "loss_grad=0.44204791414142\n",
      "grad=-0.005678654153201555\n",
      "unweighted_step=-0.005678654153201555\n",
      "learning_rate=4.416209962736644e-07\n",
      "pred_val=0.05235421095565149\n",
      "loss_grad=0.10470842191130297\n",
      "grad=0.0001988915022320855\n",
      "unweighted_step=0.0001988915022320855\n",
      "learning_rate=2.5591426665123374e-06\n",
      "loss_grad=0.10470842191130297\n",
      "grad=-1.6037136362503386e-06\n",
      "unweighted_step=-1.6037136362503386e-06\n",
      "learning_rate=1.1637341070436007e-06\n",
      "loss_grad=0.10470842191130297\n",
      "grad=0.4453227973906631\n",
      "unweighted_step=0.4453227973906631\n",
      "learning_rate=1.1637341070436007e-06\n",
      "loss_grad=0.10470842191130297\n",
      "grad=0.04253631578649484\n",
      "unweighted_step=0.04253631578649484\n",
      "learning_rate=1.0417386841202516e-06\n",
      "loss_grad=0.10470842191130297\n",
      "grad=-0.04248148692011623\n",
      "unweighted_step=-0.04248148692011623\n",
      "learning_rate=1.0417386841202516e-06\n",
      "loss_grad=0.10470842191130297\n",
      "grad=0.0005792396690892537\n",
      "unweighted_step=0.0005792396690892537\n",
      "learning_rate=6.579266301335845e-06\n",
      "loss_grad=0.10470842191130297\n",
      "grad=-0.000850546909357462\n",
      "unweighted_step=-0.000850546909357462\n",
      "learning_rate=3.8620106773234157e-07\n",
      "loss_grad=0.10470842191130297\n",
      "grad=-0.00021752211783641006\n",
      "unweighted_step=-0.00021752211783641006\n",
      "learning_rate=6.133735379061927e-07\n",
      "loss_grad=0.10470842191130297\n",
      "grad=6.474544988498807e-05\n",
      "unweighted_step=6.474544988498807e-05\n",
      "learning_rate=4.114143099483354e-07\n",
      "pred_val=0.48894774332989965\n",
      "loss_grad=0.9778954866597993\n",
      "grad=0.10214626027372889\n",
      "unweighted_step=0.10214626027372889\n",
      "learning_rate=2.7535977328098342e-06\n",
      "loss_grad=0.9778954866597993\n",
      "grad=-0.0008557880913392336\n",
      "unweighted_step=-0.0008557880913392336\n",
      "learning_rate=1.2523422709576905e-06\n",
      "loss_grad=0.9778954866597993\n",
      "grad=0.3245880929067309\n",
      "unweighted_step=0.3245880929067309\n",
      "learning_rate=1.2523422709576905e-06\n",
      "loss_grad=0.9778954866597993\n",
      "grad=0.03741256565471977\n",
      "unweighted_step=0.03741256565471977\n",
      "learning_rate=1.1210579646324254e-06\n",
      "loss_grad=0.9778954866597993\n",
      "grad=-0.02809067635673299\n",
      "unweighted_step=-0.02809067635673299\n",
      "learning_rate=1.1210579646324254e-06\n",
      "loss_grad=0.9778954866597993\n",
      "grad=0.16563638794963145\n",
      "unweighted_step=0.16563638794963145\n",
      "learning_rate=7.067692457732268e-06\n",
      "loss_grad=0.9778954866597993\n",
      "grad=-0.1618034213715807\n",
      "unweighted_step=-0.1618034213715807\n",
      "learning_rate=4.1402693765515623e-07\n",
      "loss_grad=0.9778954866597993\n",
      "grad=-0.07793681231113135\n",
      "unweighted_step=-0.07793681231113135\n",
      "learning_rate=6.589115642797361e-07\n",
      "loss_grad=0.9778954866597993\n",
      "grad=0.05286634543979217\n",
      "unweighted_step=0.05286634543979217\n",
      "learning_rate=4.1791474446361794e-07\n",
      "pred_val=0.21235973704134964\n",
      "loss_grad=0.4247194740826993\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=2.2359427584127697e-06\n",
      "loss_grad=0.4247194740826993\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=1.3626368344273058e-06\n",
      "loss_grad=0.4247194740826993\n",
      "grad=2.7561901128997484\n",
      "unweighted_step=2.7561901128997484\n",
      "learning_rate=1.3626368344273058e-06\n",
      "loss_grad=0.4247194740826993\n",
      "grad=0.25546562955937846\n",
      "unweighted_step=0.25546562955937846\n",
      "learning_rate=1.2197902375107607e-06\n",
      "loss_grad=0.4247194740826993\n",
      "grad=-0.25546561262214956\n",
      "unweighted_step=-0.25546561262214956\n",
      "learning_rate=1.2197902375107607e-06\n",
      "loss_grad=0.4247194740826993\n",
      "grad=-9.118473934005265e-05\n",
      "unweighted_step=-9.118473934005265e-05\n",
      "learning_rate=5.745195821382567e-06\n",
      "loss_grad=0.4247194740826993\n",
      "grad=-0.0016613657714899662\n",
      "unweighted_step=-0.0016613657714899662\n",
      "learning_rate=4.4964364945311164e-07\n",
      "loss_grad=0.4247194740826993\n",
      "grad=1.865366572537681e-05\n",
      "unweighted_step=1.865366572537681e-05\n",
      "learning_rate=5.356154012978852e-07\n",
      "loss_grad=0.4247194740826993\n",
      "grad=0.0007749815665568454\n",
      "unweighted_step=0.0007749815665568454\n",
      "learning_rate=4.4211205307562333e-07\n",
      "pred_val=0.41978917281654765\n",
      "loss_grad=-1.1604216543669046\n",
      "grad=-0.1869489870092562\n",
      "unweighted_step=-0.1869489870092562\n",
      "learning_rate=2.1375699652704605e-06\n",
      "loss_grad=-1.1604216543669046\n",
      "grad=0.0016521306618423324\n",
      "unweighted_step=0.0016521306618423324\n",
      "learning_rate=1.0982371720931665e-06\n",
      "loss_grad=-1.1604216543669046\n",
      "grad=-2.715998298958029\n",
      "unweighted_step=-2.715998298958029\n",
      "learning_rate=1.0982371720931665e-06\n",
      "loss_grad=-1.1604216543669046\n",
      "grad=-0.2827120275642404\n",
      "unweighted_step=-0.2827120275642404\n",
      "learning_rate=9.831078590750778e-07\n",
      "loss_grad=-1.1604216543669046\n",
      "grad=0.26262682044169355\n",
      "unweighted_step=0.26262682044169355\n",
      "learning_rate=9.831078590750778e-07\n",
      "loss_grad=-1.1604216543669046\n",
      "grad=0.0941699728553624\n",
      "unweighted_step=0.0941699728553624\n",
      "learning_rate=5.420932914432593e-06\n",
      "loss_grad=-1.1604216543669046\n",
      "grad=-0.09399387505110336\n",
      "unweighted_step=-0.09399387505110336\n",
      "learning_rate=4.914661535544183e-07\n",
      "loss_grad=-1.1604216543669046\n",
      "grad=-0.05904218822216838\n",
      "unweighted_step=-0.05904218822216838\n",
      "learning_rate=5.053855245419765e-07\n",
      "loss_grad=-1.1604216543669046\n",
      "grad=0.12448912754193538\n",
      "unweighted_step=0.12448912754193538\n",
      "learning_rate=4.77016823419801e-07\n",
      "pred_val=0.3919134145303374\n",
      "loss_grad=-1.2161731709393253\n",
      "grad=-0.3454236337074937\n",
      "unweighted_step=-0.3454236337074937\n",
      "learning_rate=2.197426077287827e-06\n",
      "loss_grad=-1.2161731709393253\n",
      "grad=0.0030613691430720585\n",
      "unweighted_step=0.0030613691430720585\n",
      "learning_rate=1.0466006349228522e-06\n",
      "loss_grad=-1.2161731709393253\n",
      "grad=-3.784610990783414\n",
      "unweighted_step=-3.784610990783414\n",
      "learning_rate=1.0466006349228522e-06\n",
      "loss_grad=-1.2161731709393253\n",
      "grad=-0.4116225888468686\n",
      "unweighted_step=-0.4116225888468686\n",
      "learning_rate=9.368844322985054e-07\n",
      "loss_grad=-1.2161731709393253\n",
      "grad=0.37308626395557243\n",
      "unweighted_step=0.37308626395557243\n",
      "learning_rate=9.368844322985054e-07\n",
      "loss_grad=-1.2161731709393253\n",
      "grad=-1.5684323791693395\n",
      "unweighted_step=-1.5684323791693395\n",
      "learning_rate=4.760773617080926e-06\n",
      "loss_grad=-1.2161731709393253\n",
      "grad=1.5423247873841426\n",
      "unweighted_step=1.5423247873841426\n",
      "learning_rate=3.948899695158097e-07\n",
      "loss_grad=-1.2161731709393253\n",
      "grad=0.8609668928167893\n",
      "unweighted_step=0.8609668928167893\n",
      "learning_rate=4.4383955435476345e-07\n",
      "loss_grad=-1.2161731709393253\n",
      "grad=-0.779291284958742\n",
      "unweighted_step=-0.779291284958742\n",
      "learning_rate=3.866340479877215e-07\n",
      "pred_val=0.49947873513856283\n",
      "loss_grad=-1.0010425297228744\n",
      "grad=-0.04386019257425514\n",
      "unweighted_step=-0.04386019257425514\n",
      "learning_rate=2.338063480875036e-06\n",
      "loss_grad=-1.0010425297228744\n",
      "grad=0.00036472485558370233\n",
      "unweighted_step=0.00036472485558370233\n",
      "learning_rate=1.0743263125454375e-06\n",
      "loss_grad=-1.0010425297228744\n",
      "grad=-0.017499808798807893\n",
      "unweighted_step=-0.017499808798807893\n",
      "learning_rate=1.0743263125454375e-06\n",
      "loss_grad=-1.0010425297228744\n",
      "grad=-0.0036717179124810414\n",
      "unweighted_step=-0.0036717179124810414\n",
      "learning_rate=9.617035991065227e-07\n",
      "loss_grad=-1.0010425297228744\n",
      "grad=0.0010581321681135641\n",
      "unweighted_step=0.0010581321681135641\n",
      "learning_rate=9.617035991065227e-07\n",
      "loss_grad=-1.0010425297228744\n",
      "grad=-0.04263227386115703\n",
      "unweighted_step=-0.04263227386115703\n",
      "learning_rate=4.708929632394602e-06\n",
      "loss_grad=-1.0010425297228744\n",
      "grad=0.041330189415671625\n",
      "unweighted_step=0.041330189415671625\n",
      "learning_rate=3.7583528915177334e-07\n",
      "loss_grad=-1.0010425297228744\n",
      "grad=-0.0015178598137742094\n",
      "unweighted_step=-0.0015178598137742094\n",
      "learning_rate=4.042890648022346e-07\n",
      "loss_grad=-1.0010425297228744\n",
      "grad=0.00707779119894087\n",
      "unweighted_step=0.00707779119894087\n",
      "learning_rate=3.652676891040819e-07\n",
      "pred_val=0.3751290474711346\n",
      "loss_grad=0.7502580949422692\n",
      "grad=0.0069531218929690655\n",
      "unweighted_step=0.0069531218929690655\n",
      "learning_rate=1.9125347917258437e-06\n",
      "key='mult_a', 0.05999838095366039, current_value=0.001581460635859541, updated=0.0015814473377720097, learning_rate=1.9125347917258437e-06, grad=0.0069531218929690655\n",
      "loss_grad=0.7502580949422692\n",
      "grad=-5.538523107639709e-05\n",
      "unweighted_step=-5.538523107639709e-05\n",
      "learning_rate=8.98947284002487e-07\n",
      "key='mult_b', -0.12251471303469197, current_value=0.9999956831381189, updated=0.9999956831879073, learning_rate=8.98947284002487e-07, grad=-5.538523107639709e-05\n",
      "loss_grad=0.7502580949422692\n",
      "grad=2.6568345564312565\n",
      "unweighted_step=2.6568345564312565\n",
      "learning_rate=8.98947284002487e-07\n",
      "key='dif_a', 0.12251471303469197, current_value=-0.03378118163790254, updated=-0.03378356999211109, learning_rate=8.98947284002487e-07, grad=2.6568345564312565\n",
      "loss_grad=0.7502580949422692\n",
      "grad=0.2436273988614196\n",
      "unweighted_step=0.2436273988614196\n",
      "learning_rate=8.04709731425908e-07\n",
      "key='dif_b', 0.12251471303469197, current_value=1.0009970749974688, updated=1.00099687894813, learning_rate=8.04709731425908e-07, grad=0.2436273988614196\n",
      "loss_grad=0.7502580949422692\n",
      "grad=-0.24176316220676236\n",
      "unweighted_step=-0.24176316220676236\n",
      "learning_rate=8.04709731425908e-07\n",
      "key='add_norm_b', -0.12251471303469197, current_value=0.9991016485613862, updated=0.9991018431105555, learning_rate=8.04709731425908e-07, grad=-0.24176316220676236\n",
      "loss_grad=0.7502580949422692\n",
      "grad=-0.02897425489849772\n",
      "unweighted_step=-0.02897425489849772\n",
      "learning_rate=4.918736407594042e-06\n",
      "key='target_normalized_intensity_a', -0.8151836274044777, current_value=0.09927426634109116, updated=0.0992744088578136, learning_rate=4.918736407594042e-06, grad=-0.02897425489849772\n",
      "loss_grad=0.7502580949422692\n",
      "grad=0.0269708849971902\n",
      "unweighted_step=0.0269708849971902\n",
      "learning_rate=3.8555943749202544e-07\n",
      "key='query_normalized_intensity_a', 0.7529114359484319, current_value=0.10074907837367297, updated=0.10074906797479373, learning_rate=3.8555943749202544e-07, grad=0.0269708849971902\n",
      "loss_grad=0.7502580949422692\n",
      "grad=0.0006702455710480646\n",
      "unweighted_step=0.0006702455710480646\n",
      "learning_rate=3.616587801636233e-07\n",
      "key='target_normalized_intensity_b', 0.31518313380167984, current_value=0.10030359454605396, updated=0.10030359430365376, learning_rate=3.616587801636233e-07, grad=0.0006702455710480646\n",
      "loss_grad=0.7502580949422692\n",
      "grad=-0.011172268413622961\n",
      "unweighted_step=-0.011172268413622961\n",
      "learning_rate=3.205703361198827e-07\n",
      "key='query_normalized_intensity_b', -0.25877081094660925, current_value=0.09949601511954882, updated=0.09949601870104666, learning_rate=3.205703361198827e-07, grad=-0.011172268413622961\n",
      "pred_val=0.2669820994284107\n",
      "loss_grad=-1.4660358011431787\n",
      "grad=-0.6135390672223889\n",
      "unweighted_step=-0.6135390672223889\n",
      "learning_rate=1.799695703486387e-06\n",
      "loss_grad=-1.4660358011431787\n",
      "grad=0.0055840443232709975\n",
      "unweighted_step=0.0055840443232709975\n",
      "learning_rate=8.374797795224306e-07\n",
      "loss_grad=-1.4660358011431787\n",
      "grad=-8.606051317525829\n",
      "unweighted_step=-8.606051317525829\n",
      "learning_rate=8.374797795224306e-07\n",
      "loss_grad=-1.4660358011431787\n",
      "grad=-0.9157356919080599\n",
      "unweighted_step=-0.9157356919080599\n",
      "learning_rate=7.496859275813322e-07\n",
      "loss_grad=-1.4660358011431787\n",
      "grad=0.8480370215477473\n",
      "unweighted_step=0.8480370215477473\n",
      "learning_rate=7.496859275813322e-07\n",
      "loss_grad=-1.4660358011431787\n",
      "grad=-0.34314749045789383\n",
      "unweighted_step=-0.34314749045789383\n",
      "learning_rate=5.274250595262687e-06\n",
      "loss_grad=-1.4660358011431787\n",
      "grad=0.33395399503011197\n",
      "unweighted_step=0.33395399503011197\n",
      "learning_rate=4.0982528207626277e-07\n",
      "loss_grad=-1.4660358011431787\n",
      "grad=0.3374413669341237\n",
      "unweighted_step=0.3374413669341237\n",
      "learning_rate=3.606741533102717e-07\n",
      "loss_grad=-1.4660358011431787\n",
      "grad=-0.13424801698418484\n",
      "unweighted_step=-0.13424801698418484\n",
      "learning_rate=3.16984956190364e-07\n",
      "pred_val=0.0720237014641972\n",
      "loss_grad=0.1440474029283944\n",
      "grad=0.009290336718843136\n",
      "unweighted_step=0.009290336718843136\n",
      "learning_rate=1.5828321526819717e-06\n",
      "loss_grad=0.1440474029283944\n",
      "grad=-8.316998782053335e-05\n",
      "unweighted_step=-8.316998782053335e-05\n",
      "learning_rate=7.404900766966679e-07\n",
      "loss_grad=0.1440474029283944\n",
      "grad=0.7289990556359358\n",
      "unweighted_step=0.7289990556359358\n",
      "learning_rate=7.404900766966679e-07\n",
      "loss_grad=0.1440474029283944\n",
      "grad=0.07119534818754036\n",
      "unweighted_step=0.07119534818754036\n",
      "learning_rate=6.628637533549488e-07\n",
      "loss_grad=0.1440474029283944\n",
      "grad=-0.06951812899223742\n",
      "unweighted_step=-0.06951812899223742\n",
      "learning_rate=6.628637533549488e-07\n",
      "loss_grad=0.1440474029283944\n",
      "grad=-0.01774761870127877\n",
      "unweighted_step=-0.01774761870127877\n",
      "learning_rate=5.728568065050739e-06\n",
      "loss_grad=0.1440474029283944\n",
      "grad=0.017170788825940954\n",
      "unweighted_step=0.017170788825940954\n",
      "learning_rate=4.432130747493701e-07\n",
      "loss_grad=0.1440474029283944\n",
      "grad=0.005518505595285982\n",
      "unweighted_step=0.005518505595285982\n",
      "learning_rate=3.782168878871484e-07\n",
      "loss_grad=0.1440474029283944\n",
      "grad=-0.01249658885647872\n",
      "unweighted_step=-0.01249658885647872\n",
      "learning_rate=3.310615641579673e-07\n",
      "pred_val=0.2655183566301313\n",
      "loss_grad=0.5310367132602626\n",
      "grad=0.009698186743278228\n",
      "unweighted_step=0.009698186743278228\n",
      "learning_rate=1.5666080270165338e-06\n",
      "loss_grad=0.5310367132602626\n",
      "grad=-8.467556158827481e-05\n",
      "unweighted_step=-8.467556158827481e-05\n",
      "learning_rate=7.346359855848919e-07\n",
      "loss_grad=0.5310367132602626\n",
      "grad=3.120109250988596\n",
      "unweighted_step=3.120109250988596\n",
      "learning_rate=7.346359855848919e-07\n",
      "loss_grad=0.5310367132602626\n",
      "grad=0.2885396013164755\n",
      "unweighted_step=0.2885396013164755\n",
      "learning_rate=6.576233525326387e-07\n",
      "loss_grad=0.5310367132602626\n",
      "grad=-0.28703759335315093\n",
      "unweighted_step=-0.28703759335315093\n",
      "learning_rate=6.576233525326387e-07\n",
      "loss_grad=0.5310367132602626\n",
      "grad=0.01592863741576924\n",
      "unweighted_step=0.01592863741576924\n",
      "learning_rate=4.622556945913687e-06\n",
      "loss_grad=0.5310367132602626\n",
      "grad=-0.017526869616075552\n",
      "unweighted_step=-0.017526869616075552\n",
      "learning_rate=3.586771928823224e-07\n",
      "loss_grad=0.5310367132602626\n",
      "grad=-0.010600323301019106\n",
      "unweighted_step=-0.010600323301019106\n",
      "learning_rate=3.122863592061997e-07\n",
      "loss_grad=0.5310367132602626\n",
      "grad=0.0022279373785661807\n",
      "unweighted_step=0.0022279373785661807\n",
      "learning_rate=2.7405146987865727e-07\n",
      "pred_val=0.4076069651424609\n",
      "loss_grad=0.8152139302849218\n",
      "grad=0.030159083620345906\n",
      "unweighted_step=0.030159083620345906\n",
      "learning_rate=1.6369095146711948e-06\n",
      "loss_grad=0.8152139302849218\n",
      "grad=-0.0002439399779682942\n",
      "unweighted_step=-0.0002439399779682942\n",
      "learning_rate=7.684638796472101e-07\n",
      "loss_grad=0.8152139302849218\n",
      "grad=2.1800803818359356\n",
      "unweighted_step=2.1800803818359356\n",
      "learning_rate=7.684638796472101e-07\n",
      "loss_grad=0.8152139302849218\n",
      "grad=0.20150847045979092\n",
      "unweighted_step=0.20150847045979092\n",
      "learning_rate=6.879050342619501e-07\n",
      "loss_grad=0.8152139302849218\n",
      "grad=-0.19864335728120122\n",
      "unweighted_step=-0.19864335728120122\n",
      "learning_rate=6.879050342619501e-07\n",
      "loss_grad=0.8152139302849218\n",
      "grad=-0.13293323326207787\n",
      "unweighted_step=-0.13293323326207787\n",
      "learning_rate=4.375410519739378e-06\n",
      "loss_grad=0.8152139302849218\n",
      "grad=0.13007843325194293\n",
      "unweighted_step=0.13007843325194293\n",
      "learning_rate=3.390816138779437e-07\n",
      "loss_grad=0.8152139302849218\n",
      "grad=0.030959685853200858\n",
      "unweighted_step=0.030959685853200858\n",
      "learning_rate=2.926621856358634e-07\n",
      "loss_grad=0.8152139302849218\n",
      "grad=-0.04149504501059859\n",
      "unweighted_step=-0.04149504501059859\n",
      "learning_rate=2.5654011609515477e-07\n",
      "pred_val=0.465799299585914\n",
      "loss_grad=-1.0684014008281721\n",
      "grad=-0.17017591647767108\n",
      "unweighted_step=-0.17017591647767108\n",
      "learning_rate=1.354644955081009e-06\n",
      "loss_grad=-1.0684014008281721\n",
      "grad=0.0014695986952064726\n",
      "unweighted_step=0.0014695986952064726\n",
      "learning_rate=6.355015115982287e-07\n",
      "loss_grad=-1.0684014008281721\n",
      "grad=-1.0862741297973026\n",
      "unweighted_step=-1.0862741297973026\n",
      "learning_rate=6.355015115982287e-07\n",
      "loss_grad=-1.0684014008281721\n",
      "grad=-0.11379242362301413\n",
      "unweighted_step=-0.11379242362301413\n",
      "learning_rate=5.688812456744176e-07\n",
      "loss_grad=-1.0684014008281721\n",
      "grad=0.08946088135867666\n",
      "unweighted_step=0.08946088135867666\n",
      "learning_rate=5.688812456744176e-07\n",
      "loss_grad=-1.0684014008281721\n",
      "grad=-0.1834502770611313\n",
      "unweighted_step=-0.1834502770611313\n",
      "learning_rate=4.477214712412164e-06\n",
      "loss_grad=-1.0684014008281721\n",
      "grad=0.17878494892496405\n",
      "unweighted_step=0.17878494892496405\n",
      "learning_rate=3.467731868273668e-07\n",
      "loss_grad=-1.0684014008281721\n",
      "grad=0.11316502764328688\n",
      "unweighted_step=0.11316502764328688\n",
      "learning_rate=2.9809980276905464e-07\n",
      "loss_grad=-1.0684014008281721\n",
      "grad=-0.01759401957758987\n",
      "unweighted_step=-0.01759401957758987\n",
      "learning_rate=2.6117091566428796e-07\n",
      "pred_val=0.499784711354437\n",
      "loss_grad=-1.0004305772911262\n",
      "grad=-0.0422340307412801\n",
      "unweighted_step=-0.0422340307412801\n",
      "learning_rate=1.3055814183949484e-06\n",
      "loss_grad=-1.0004305772911262\n",
      "grad=0.0003295930793708315\n",
      "unweighted_step=0.0003295930793708315\n",
      "learning_rate=6.122982153126991e-07\n",
      "loss_grad=-1.0004305772911262\n",
      "grad=-0.008354642677539624\n",
      "unweighted_step=-0.008354642677539624\n",
      "learning_rate=6.122982153126991e-07\n",
      "loss_grad=-1.0004305772911262\n",
      "grad=-0.0017111073605063777\n",
      "unweighted_step=-0.0017111073605063777\n",
      "learning_rate=5.481103743959716e-07\n",
      "loss_grad=-1.0004305772911262\n",
      "grad=0.000379794307489813\n",
      "unweighted_step=0.000379794307489813\n",
      "learning_rate=5.481103743959716e-07\n",
      "loss_grad=-1.0004305772911262\n",
      "grad=-0.0004952519873032755\n",
      "unweighted_step=-0.0004952519873032755\n",
      "learning_rate=4.753161901082553e-06\n",
      "loss_grad=-1.0004305772911262\n",
      "grad=-0.0001707178166428453\n",
      "unweighted_step=-0.0001707178166428453\n",
      "learning_rate=2.9082418625909827e-07\n",
      "loss_grad=-1.0004305772911262\n",
      "grad=-0.0065766628493199066\n",
      "unweighted_step=-0.0065766628493199066\n",
      "learning_rate=2.506155087538332e-07\n",
      "loss_grad=-1.0004305772911262\n",
      "grad=0.009575137916850277\n",
      "unweighted_step=0.009575137916850277\n",
      "learning_rate=2.196380833033201e-07\n",
      "pred_val=0.355176742876646\n",
      "loss_grad=-1.289646514246708\n",
      "grad=-0.43478073226546293\n",
      "unweighted_step=-0.43478073226546293\n",
      "learning_rate=1.3472172307701611e-06\n",
      "loss_grad=-1.289646514246708\n",
      "grad=0.0038248172554054757\n",
      "unweighted_step=0.0038248172554054757\n",
      "learning_rate=6.317350748497293e-07\n",
      "loss_grad=-1.289646514246708\n",
      "grad=-5.234755484245219\n",
      "unweighted_step=-5.234755484245219\n",
      "learning_rate=6.317350748497293e-07\n",
      "loss_grad=-1.289646514246708\n",
      "grad=-0.5590124502265559\n",
      "unweighted_step=-0.5590124502265559\n",
      "learning_rate=5.655096482979587e-07\n",
      "loss_grad=-1.289646514246708\n",
      "grad=0.5037639492745976\n",
      "unweighted_step=0.5037639492745976\n",
      "learning_rate=5.655096482979587e-07\n",
      "loss_grad=-1.289646514246708\n",
      "grad=-0.20135062755021937\n",
      "unweighted_step=-0.20135062755021937\n",
      "learning_rate=5.137297409902736e-06\n",
      "loss_grad=-1.289646514246708\n",
      "grad=0.19490154669400936\n",
      "unweighted_step=0.19490154669400936\n",
      "learning_rate=2.7066160314775194e-07\n",
      "loss_grad=-1.289646514246708\n",
      "grad=0.16820384731791885\n",
      "unweighted_step=0.16820384731791885\n",
      "learning_rate=2.3298344472858805e-07\n",
      "loss_grad=-1.289646514246708\n",
      "grad=0.010040879169452418\n",
      "unweighted_step=0.010040879169452418\n",
      "learning_rate=2.131559607587761e-07\n",
      "pred_val=0.22783536833946944\n",
      "loss_grad=0.45567073667893887\n",
      "grad=0.005062278525158066\n",
      "unweighted_step=0.005062278525158066\n",
      "learning_rate=1.1236528438485202e-06\n",
      "loss_grad=0.45567073667893887\n",
      "grad=-4.396873464066349e-05\n",
      "unweighted_step=-4.396873464066349e-05\n",
      "learning_rate=5.26947880987656e-07\n",
      "loss_grad=0.45567073667893887\n",
      "grad=2.897094574623623\n",
      "unweighted_step=2.897094574623623\n",
      "learning_rate=5.26947880987656e-07\n",
      "loss_grad=0.45567073667893887\n",
      "grad=0.2725537890808683\n",
      "unweighted_step=0.2725537890808683\n",
      "learning_rate=4.717074019035081e-07\n",
      "loss_grad=0.45567073667893887\n",
      "grad=-0.27094067630991053\n",
      "unweighted_step=-0.27094067630991053\n",
      "learning_rate=4.717074019035081e-07\n",
      "loss_grad=0.45567073667893887\n",
      "grad=-0.0003830730713203301\n",
      "unweighted_step=-0.0003830730713203301\n",
      "learning_rate=5.601752343137387e-06\n",
      "loss_grad=0.45567073667893887\n",
      "grad=-0.0013848030815510309\n",
      "unweighted_step=-0.0013848030815510309\n",
      "learning_rate=2.3944472722152635e-07\n",
      "loss_grad=0.45567073667893887\n",
      "grad=-0.0019980096129557086\n",
      "unweighted_step=-0.0019980096129557086\n",
      "learning_rate=2.062317077511753e-07\n",
      "loss_grad=0.45567073667893887\n",
      "grad=-0.005910462394228723\n",
      "unweighted_step=-0.005910462394228723\n",
      "learning_rate=1.843279753053291e-07\n",
      "pred_val=0.366780090041746\n",
      "loss_grad=-1.2664398199165081\n",
      "grad=-0.3575249116965631\n",
      "unweighted_step=-0.3575249116965631\n",
      "learning_rate=1.048337379494185e-06\n",
      "loss_grad=-1.2664398199165081\n",
      "grad=0.003128007244559086\n",
      "unweighted_step=0.003128007244559086\n",
      "learning_rate=4.916086622798365e-07\n",
      "loss_grad=-1.2664398199165081\n",
      "grad=-4.771670119191173\n",
      "unweighted_step=-4.771670119191173\n",
      "learning_rate=4.916086622798365e-07\n",
      "loss_grad=-1.2664398199165081\n",
      "grad=-0.49204572824835163\n",
      "unweighted_step=-0.49204572824835163\n",
      "learning_rate=4.400728292191634e-07\n",
      "loss_grad=-1.2664398199165081\n",
      "grad=0.4531778647386843\n",
      "unweighted_step=0.4531778647386843\n",
      "learning_rate=4.400728292191634e-07\n",
      "loss_grad=-1.2664398199165081\n",
      "grad=-0.7496670819214467\n",
      "unweighted_step=-0.7496670819214467\n",
      "learning_rate=6.135062744857375e-06\n",
      "loss_grad=-1.2664398199165081\n",
      "grad=0.7361376291994024\n",
      "unweighted_step=0.7361376291994024\n",
      "learning_rate=2.173362499930074e-07\n",
      "loss_grad=-1.2664398199165081\n",
      "grad=0.3867964978143348\n",
      "unweighted_step=0.3867964978143348\n",
      "learning_rate=1.8713696968203597e-07\n",
      "loss_grad=-1.2664398199165081\n",
      "grad=-0.27087448866993613\n",
      "unweighted_step=-0.27087448866993613\n",
      "learning_rate=1.810797820135644e-07\n",
      "pred_val=0.35273528319966413\n",
      "loss_grad=0.7054705663993283\n",
      "grad=0.08537092793448954\n",
      "unweighted_step=0.08537092793448954\n",
      "learning_rate=9.26220407023465e-07\n",
      "key='mult_a', 0.27837890466890003, current_value=0.0015837127858953897, updated=0.0015836337135997703, learning_rate=9.26220407023465e-07, grad=0.08537092793448954\n",
      "loss_grad=0.7054705663993283\n",
      "grad=-0.0007288669017407174\n",
      "unweighted_step=-0.0007288669017407174\n",
      "learning_rate=4.343519780789292e-07\n",
      "key='mult_b', -0.2784399557744479, current_value=0.9999956737560356, updated=0.9999956740726204, learning_rate=4.343519780789292e-07, grad=-0.0007288669017407174\n",
      "loss_grad=0.7054705663993283\n",
      "grad=2.89878874279267\n",
      "unweighted_step=2.89878874279267\n",
      "learning_rate=4.343519780789292e-07\n",
      "key='dif_a', 0.2784399557744479, current_value=-0.03377604827273167, updated=-0.03377730736735613, learning_rate=4.343519780789292e-07, grad=2.89878874279267\n",
      "loss_grad=0.7054705663993283\n",
      "grad=0.28925192517897613\n",
      "unweighted_step=0.28925192517897613\n",
      "learning_rate=3.888184211069268e-07\n",
      "key='dif_b', 0.2784399557744479, current_value=1.0009976596699066, updated=1.0009975472034298, learning_rate=3.888184211069268e-07, grad=0.28925192517897613\n",
      "loss_grad=0.7054705663993283\n",
      "grad=-0.27480230771607606\n",
      "unweighted_step=-0.27480230771607606\n",
      "learning_rate=3.888184211069268e-07\n",
      "key='add_norm_b', -0.2784399557744479, current_value=0.999101171229869, updated=0.9991012780780684, learning_rate=3.888184211069268e-07, grad=-0.27480230771607606\n",
      "loss_grad=0.7054705663993283\n",
      "grad=0.05591374590464115\n",
      "unweighted_step=0.05591374590464115\n",
      "learning_rate=4.922761434753383e-06\n",
      "key='target_normalized_intensity_a', 0.007992984738862785, current_value=0.09928328787736038, updated=0.09928301262732836, learning_rate=4.922761434753383e-06, grad=0.05591374590464115\n",
      "loss_grad=0.7054705663993283\n",
      "grad=-0.05560037407889672\n",
      "unweighted_step=-0.05560037407889672\n",
      "learning_rate=1.9476938807047043e-07\n",
      "key='query_normalized_intensity_a', -0.3205537974258316, current_value=0.100748611322618, updated=0.10074862215186883, learning_rate=1.9476938807047043e-07, grad=-0.05560037407889672\n",
      "loss_grad=0.7054705663993283\n",
      "grad=-0.030406797114072015\n",
      "unweighted_step=-0.030406797114072015\n",
      "learning_rate=1.677298142052485e-07\n",
      "key='target_normalized_intensity_b', -0.3209812664708968, current_value=0.10030332151284745, updated=0.10030332661297388, learning_rate=1.677298142052485e-07, grad=-0.030406797114072015\n",
      "loss_grad=0.7054705663993283\n",
      "grad=-0.027058644069628664\n",
      "unweighted_step=-0.027058644069628664\n",
      "learning_rate=1.8853829399733624e-07\n",
      "key='query_normalized_intensity_b', -0.803963643370065, current_value=0.09949612591842778, updated=0.09949613102001838, learning_rate=1.8853829399733624e-07, grad=-0.027058644069628664\n",
      "pred_val=0.49995839941945136\n",
      "loss_grad=-1.0000832011610972\n",
      "grad=-0.012484525459875807\n",
      "unweighted_step=-0.012484525459875807\n",
      "learning_rate=8.412333533139154e-07\n",
      "loss_grad=-1.0000832011610972\n",
      "grad=9.104324209022334e-05\n",
      "unweighted_step=9.104324209022334e-05\n",
      "learning_rate=3.9449323733995654e-07\n",
      "loss_grad=-1.0000832011610972\n",
      "grad=-0.0018173603617776925\n",
      "unweighted_step=-0.0018173603617776925\n",
      "learning_rate=3.9449323733995654e-07\n",
      "loss_grad=-1.0000832011610972\n",
      "grad=-0.00040057027793392126\n",
      "unweighted_step=-0.00040057027793392126\n",
      "learning_rate=3.5313811245498494e-07\n",
      "loss_grad=-1.0000832011610972\n",
      "grad=6.69695631580956e-05\n",
      "unweighted_step=6.69695631580956e-05\n",
      "learning_rate=3.5313811245498494e-07\n",
      "loss_grad=-1.0000832011610972\n",
      "grad=-0.042332695078449716\n",
      "unweighted_step=-0.042332695078449716\n",
      "learning_rate=4.670721229462556e-06\n",
      "loss_grad=-1.0000832011610972\n",
      "grad=0.04152161700790448\n",
      "unweighted_step=0.04152161700790448\n",
      "learning_rate=1.756658086217027e-07\n",
      "loss_grad=-1.0000832011610972\n",
      "grad=-0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=1.674190427232654e-07\n",
      "loss_grad=-1.0000832011610972\n",
      "grad=-0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=2.0184806936100066e-07\n",
      "pred_val=0.4430667014770302\n",
      "loss_grad=-1.1138665970459396\n",
      "grad=-0.19945755202337148\n",
      "unweighted_step=-0.19945755202337148\n",
      "learning_rate=8.447005656867779e-07\n",
      "loss_grad=-1.1138665970459396\n",
      "grad=0.0017082853280093468\n",
      "unweighted_step=0.0017082853280093468\n",
      "learning_rate=3.9611736730658623e-07\n",
      "loss_grad=-1.1138665970459396\n",
      "grad=-1.8705572068309546\n",
      "unweighted_step=-1.8705572068309546\n",
      "learning_rate=3.9611736730658623e-07\n",
      "loss_grad=-1.1138665970459396\n",
      "grad=-0.19696076242233784\n",
      "unweighted_step=-0.19696076242233784\n",
      "learning_rate=3.545919832352917e-07\n",
      "loss_grad=-1.1138665970459396\n",
      "grad=0.1726494851832243\n",
      "unweighted_step=0.1726494851832243\n",
      "learning_rate=3.545919832352917e-07\n",
      "loss_grad=-1.1138665970459396\n",
      "grad=-0.187421032043674\n",
      "unweighted_step=-0.187421032043674\n",
      "learning_rate=4.784689284936127e-06\n",
      "loss_grad=-1.1138665970459396\n",
      "grad=0.18274619472815637\n",
      "unweighted_step=0.18274619472815637\n",
      "learning_rate=1.7583417818487783e-07\n",
      "loss_grad=-1.1138665970459396\n",
      "grad=0.12788703977421545\n",
      "unweighted_step=0.12788703977421545\n",
      "learning_rate=1.4246128415550783e-07\n",
      "loss_grad=-1.1138665970459396\n",
      "grad=-0.05296559289220847\n",
      "unweighted_step=-0.05296559289220847\n",
      "learning_rate=2.1906515928882694e-07\n",
      "pred_val=0.4802005751112816\n",
      "loss_grad=-1.0395988497774367\n",
      "grad=-0.08861082661432049\n",
      "unweighted_step=-0.08861082661432049\n",
      "learning_rate=8.886763453578607e-07\n",
      "loss_grad=-1.0395988497774367\n",
      "grad=0.0007483766510219981\n",
      "unweighted_step=0.0007483766510219981\n",
      "learning_rate=4.167386439294603e-07\n",
      "loss_grad=-1.0395988497774367\n",
      "grad=-0.612918279552528\n",
      "unweighted_step=-0.612918279552528\n",
      "learning_rate=4.167386439294603e-07\n",
      "loss_grad=-1.0395988497774367\n",
      "grad=-0.06200452515786014\n",
      "unweighted_step=-0.06200452515786014\n",
      "learning_rate=3.730515105826222e-07\n",
      "loss_grad=-1.0395988497774367\n",
      "grad=0.05288555062692264\n",
      "unweighted_step=0.05288555062692264\n",
      "learning_rate=3.730515105826222e-07\n",
      "loss_grad=-1.0395988497774367\n",
      "grad=0.2611496089405777\n",
      "unweighted_step=0.2611496089405777\n",
      "learning_rate=4.008611424200307e-06\n",
      "loss_grad=-1.0395988497774367\n",
      "grad=-0.2583580894393286\n",
      "unweighted_step=-0.2583580894393286\n",
      "learning_rate=1.4937478598737568e-07\n",
      "loss_grad=-1.0395988497774367\n",
      "grad=-0.11629168977171336\n",
      "unweighted_step=-0.11629168977171336\n",
      "learning_rate=1.3171069941982484e-07\n",
      "loss_grad=-1.0395988497774367\n",
      "grad=0.15386311116509804\n",
      "unweighted_step=0.15386311116509804\n",
      "learning_rate=1.768625550194942e-07\n",
      "pred_val=0.23637887942665425\n",
      "loss_grad=-1.5272422411466915\n",
      "grad=-0.5276207455419001\n",
      "unweighted_step=-0.5276207455419001\n",
      "learning_rate=9.562427594002017e-07\n",
      "loss_grad=-1.5272422411466915\n",
      "grad=0.004787467281776512\n",
      "unweighted_step=0.004787467281776512\n",
      "learning_rate=4.4842297081900597e-07\n",
      "loss_grad=-1.5272422411466915\n",
      "grad=-9.595481580452962\n",
      "unweighted_step=-9.595481580452962\n",
      "learning_rate=4.4842297081900597e-07\n",
      "loss_grad=-1.5272422411466915\n",
      "grad=-1.0016197124083135\n",
      "unweighted_step=-1.0016197124083135\n",
      "learning_rate=4.014143374529312e-07\n",
      "loss_grad=-1.5272422411466915\n",
      "grad=0.9399332042336237\n",
      "unweighted_step=0.9399332042336237\n",
      "learning_rate=4.014143374529312e-07\n",
      "loss_grad=-1.5272422411466915\n",
      "grad=-1.2140381135418277\n",
      "unweighted_step=-1.2140381135418277\n",
      "learning_rate=3.7324186243502048e-06\n",
      "loss_grad=-1.5272422411466915\n",
      "grad=1.1941760971629198\n",
      "unweighted_step=1.1941760971629198\n",
      "learning_rate=1.3820746967159503e-07\n",
      "loss_grad=-1.5272422411466915\n",
      "grad=0.8375792378980591\n",
      "unweighted_step=0.8375792378980591\n",
      "learning_rate=1.1692375009820202e-07\n",
      "loss_grad=-1.5272422411466915\n",
      "grad=-0.6752579481633247\n",
      "unweighted_step=-0.6752579481633247\n",
      "learning_rate=1.673693368139517e-07\n",
      "pred_val=0.2544479204879368\n",
      "loss_grad=0.5088958409758736\n",
      "grad=0.0024423197589168366\n",
      "unweighted_step=0.0024423197589168366\n",
      "learning_rate=7.764545878766576e-07\n",
      "loss_grad=0.5088958409758736\n",
      "grad=-2.307693635314392e-05\n",
      "unweighted_step=-2.307693635314392e-05\n",
      "learning_rate=3.6411289393316197e-07\n",
      "loss_grad=0.5088958409758736\n",
      "grad=3.073160472965328\n",
      "unweighted_step=3.073160472965328\n",
      "learning_rate=3.6411289393316197e-07\n",
      "loss_grad=0.5088958409758736\n",
      "grad=0.29031942468843946\n",
      "unweighted_step=0.29031942468843946\n",
      "learning_rate=3.259425711606627e-07\n",
      "loss_grad=0.5088958409758736\n",
      "grad=-0.2902620206643697\n",
      "unweighted_step=-0.2902620206643697\n",
      "learning_rate=3.259425711606627e-07\n",
      "loss_grad=0.5088958409758736\n",
      "grad=0.01622371304256858\n",
      "unweighted_step=0.01622371304256858\n",
      "learning_rate=3.3011374068325576e-06\n",
      "loss_grad=0.5088958409758736\n",
      "grad=-0.01790451276457861\n",
      "unweighted_step=-0.01790451276457861\n",
      "learning_rate=1.2264257095240292e-07\n",
      "loss_grad=0.5088958409758736\n",
      "grad=-0.010875157403187447\n",
      "unweighted_step=-0.010875157403187447\n",
      "learning_rate=1.0594860868294227e-07\n",
      "loss_grad=0.5088958409758736\n",
      "grad=0.01208443896963846\n",
      "unweighted_step=0.01208443896963846\n",
      "learning_rate=1.4675576791533813e-07\n",
      "pred_val=0.09066463993788387\n",
      "loss_grad=0.18132927987576775\n",
      "grad=0.015058843720912998\n",
      "unweighted_step=0.015058843720912998\n",
      "learning_rate=7.422846858214572e-07\n",
      "loss_grad=0.18132927987576775\n",
      "grad=-0.0001313913447441274\n",
      "unweighted_step=-0.0001313913447441274\n",
      "learning_rate=3.480892639490827e-07\n",
      "loss_grad=0.18132927987576775\n",
      "grad=1.021252411833711\n",
      "unweighted_step=1.021252411833711\n",
      "learning_rate=3.480892639490827e-07\n",
      "loss_grad=0.18132927987576775\n",
      "grad=0.10063493479027755\n",
      "unweighted_step=0.10063493479027755\n",
      "learning_rate=3.1159871450697167e-07\n",
      "loss_grad=0.18132927987576775\n",
      "grad=-0.09747570525190292\n",
      "unweighted_step=-0.09747570525190292\n",
      "learning_rate=3.1159871450697167e-07\n",
      "loss_grad=0.18132927987576775\n",
      "grad=-0.03514991713932143\n",
      "unweighted_step=-0.03514991713932143\n",
      "learning_rate=2.9966901180421483e-06\n",
      "loss_grad=0.18132927987576775\n",
      "grad=0.03422789154214558\n",
      "unweighted_step=0.03422789154214558\n",
      "learning_rate=1.111521767153305e-07\n",
      "loss_grad=0.18132927987576775\n",
      "grad=0.013809846659558478\n",
      "unweighted_step=0.013809846659558478\n",
      "learning_rate=9.502879287016277e-08\n",
      "loss_grad=0.18132927987576775\n",
      "grad=-0.027617969996725195\n",
      "unweighted_step=-0.027617969996725195\n",
      "learning_rate=1.3377978135060597e-07\n",
      "pred_val=0.4249975276137176\n",
      "loss_grad=0.8499950552274352\n",
      "grad=0.010288463164899422\n",
      "unweighted_step=0.010288463164899422\n",
      "learning_rate=7.630658367567115e-07\n",
      "loss_grad=0.8499950552274352\n",
      "grad=-7.53286816563205e-05\n",
      "unweighted_step=-7.53286816563205e-05\n",
      "learning_rate=3.57834490602205e-07\n",
      "loss_grad=0.8499950552274352\n",
      "grad=1.8596646088781263\n",
      "unweighted_step=1.8596646088781263\n",
      "learning_rate=3.57834490602205e-07\n",
      "loss_grad=0.8499950552274352\n",
      "grad=0.17695464798292798\n",
      "unweighted_step=0.17695464798292798\n",
      "learning_rate=3.203223391980686e-07\n",
      "loss_grad=0.8499950552274352\n",
      "grad=-0.17688702344435528\n",
      "unweighted_step=-0.17688702344435528\n",
      "learning_rate=3.203223391980686e-07\n",
      "loss_grad=0.8499950552274352\n",
      "grad=-0.034924852040582585\n",
      "unweighted_step=-0.034924852040582585\n",
      "learning_rate=3.008339800147671e-06\n",
      "loss_grad=0.8499950552274352\n",
      "grad=0.033397768750487555\n",
      "unweighted_step=0.033397768750487555\n",
      "learning_rate=1.1150285650170701e-07\n",
      "loss_grad=0.8499950552274352\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=8.567164497356068e-08\n",
      "loss_grad=0.8499950552274352\n",
      "grad=5.457915661826026e-05\n",
      "unweighted_step=5.457915661826026e-05\n",
      "learning_rate=1.1962714606865108e-07\n",
      "pred_val=0.37451495279190083\n",
      "loss_grad=0.7490299055838017\n",
      "grad=0.10284335023863406\n",
      "unweighted_step=0.10284335023863406\n",
      "learning_rate=8.11900600696877e-07\n",
      "loss_grad=0.7490299055838017\n",
      "grad=-0.0008676952605142276\n",
      "unweighted_step=-0.0008676952605142276\n",
      "learning_rate=3.807352438160226e-07\n",
      "loss_grad=0.7490299055838017\n",
      "grad=2.6694113771364183\n",
      "unweighted_step=2.6694113771364183\n",
      "learning_rate=3.807352438160226e-07\n",
      "loss_grad=0.7490299055838017\n",
      "grad=0.2674237386092371\n",
      "unweighted_step=0.2674237386092371\n",
      "learning_rate=3.408223833008674e-07\n",
      "loss_grad=0.7490299055838017\n",
      "grad=-0.24863632695631008\n",
      "unweighted_step=-0.24863632695631008\n",
      "learning_rate=3.408223833008674e-07\n",
      "loss_grad=0.7490299055838017\n",
      "grad=-0.20602730973265002\n",
      "unweighted_step=-0.20602730973265002\n",
      "learning_rate=3.1646042753731804e-06\n",
      "loss_grad=0.7490299055838017\n",
      "grad=0.20296140809810945\n",
      "unweighted_step=0.20296140809810945\n",
      "learning_rate=1.1725389240893384e-07\n",
      "loss_grad=0.7490299055838017\n",
      "grad=0.059061297664257176\n",
      "unweighted_step=0.059061297664257176\n",
      "learning_rate=7.703878960734277e-08\n",
      "loss_grad=0.7490299055838017\n",
      "grad=-0.13851028899455267\n",
      "unweighted_step=-0.13851028899455267\n",
      "learning_rate=1.0801078433637238e-07\n",
      "pred_val=0.41854294247192964\n",
      "loss_grad=-1.1629141150561408\n",
      "grad=-0.24367230892997846\n",
      "unweighted_step=-0.24367230892997846\n",
      "learning_rate=6.641354625623929e-07\n",
      "loss_grad=-1.1629141150561408\n",
      "grad=0.0021390233849030095\n",
      "unweighted_step=0.0021390233849030095\n",
      "learning_rate=3.114417774671815e-07\n",
      "loss_grad=-1.1629141150561408\n",
      "grad=-2.7661892607825145\n",
      "unweighted_step=-2.7661892607825145\n",
      "learning_rate=3.114417774671815e-07\n",
      "loss_grad=-1.1629141150561408\n",
      "grad=-0.3000817859991878\n",
      "unweighted_step=-0.3000817859991878\n",
      "learning_rate=2.787930210819012e-07\n",
      "loss_grad=-1.1629141150561408\n",
      "grad=0.27417277111017835\n",
      "unweighted_step=0.27417277111017835\n",
      "learning_rate=2.787930210819012e-07\n",
      "loss_grad=-1.1629141150561408\n",
      "grad=0.285735438371453\n",
      "unweighted_step=0.285735438371453\n",
      "learning_rate=2.6077229143946254e-06\n",
      "loss_grad=-1.1629141150561408\n",
      "grad=-0.28367092758584894\n",
      "unweighted_step=-0.28367092758584894\n",
      "learning_rate=9.664197863568667e-08\n",
      "loss_grad=-1.1629141150561408\n",
      "grad=-0.17088093725880488\n",
      "unweighted_step=-0.17088093725880488\n",
      "learning_rate=6.936444635254092e-08\n",
      "loss_grad=-1.1629141150561408\n",
      "grad=0.2562436995416163\n",
      "unweighted_step=0.2562436995416163\n",
      "learning_rate=9.705334571664592e-08\n",
      "pred_val=0.49060648497568055\n",
      "loss_grad=-1.0187870300486388\n",
      "grad=-0.05314600927266368\n",
      "unweighted_step=-0.05314600927266368\n",
      "learning_rate=6.369062240153712e-07\n",
      "key='mult_a', -0.5300015831009093, current_value=0.0015844533855387533, updated=0.0015844872345628406, learning_rate=6.369062240153712e-07, grad=-0.05314600927266368\n",
      "loss_grad=-1.0187870300486388\n",
      "grad=0.0004440131168675504\n",
      "unweighted_step=0.0004440131168675504\n",
      "learning_rate=2.986728069336872e-07\n",
      "key='mult_b', 0.530001523480689, current_value=0.9999956706466099, updated=0.9999956705139952, learning_rate=2.986728069336872e-07, grad=0.0004440131168675504\n",
      "loss_grad=-1.0187870300486388\n",
      "grad=-0.2857779767375543\n",
      "unweighted_step=-0.2857779767375543\n",
      "learning_rate=2.986728069336872e-07\n",
      "key='dif_a', -0.530001523480689, current_value=-0.03377430217771721, updated=-0.033774216823606734, learning_rate=2.986728069336872e-07, grad=-0.2857779767375543\n",
      "loss_grad=-1.0187870300486388\n",
      "grad=-0.030167005790232695\n",
      "unweighted_step=-0.030167005790232695\n",
      "learning_rate=2.673626346382784e-07\n",
      "key='dif_b', -0.530001523480689, current_value=1.000997852229998, updated=1.0009978602955283, learning_rate=2.673626346382784e-07, grad=-0.030167005790232695\n",
      "loss_grad=-1.0187870300486388\n",
      "grad=0.026200525299471537\n",
      "unweighted_step=0.026200525299471537\n",
      "learning_rate=2.673626346382784e-07\n",
      "key='add_norm_b', 0.530001523480689, current_value=0.9991010097488858, updated=0.9991010027438444, learning_rate=2.673626346382784e-07, grad=0.026200525299471537\n",
      "loss_grad=-1.0187870300486388\n",
      "grad=0.08535237239204192\n",
      "unweighted_step=0.08535237239204192\n",
      "learning_rate=2.508666109391585e-06\n",
      "key='target_normalized_intensity_a', 0.540046868149159, current_value=0.09928765527269046, updated=0.09928744115208649, learning_rate=2.508666109391585e-06, grad=0.08535237239204192\n",
      "loss_grad=-1.0187870300486388\n",
      "grad=-0.0847696235326623\n",
      "unweighted_step=-0.0847696235326623\n",
      "learning_rate=9.297979183592546e-08\n",
      "key='query_normalized_intensity_a', -0.5403521033177987, current_value=0.10074845455694985, updated=0.1007484624388118, learning_rate=9.297979183592546e-08, grad=-0.0847696235326623\n",
      "loss_grad=-1.0187870300486388\n",
      "grad=-0.05738937324735682\n",
      "unweighted_step=-0.05738937324735682\n",
      "learning_rate=6.937774308202017e-08\n",
      "key='target_normalized_intensity_b', -0.667305645768038, current_value=0.10030323292090172, updated=0.10030323690244691, learning_rate=6.937774308202017e-08, grad=-0.05738937324735682\n",
      "loss_grad=-1.0187870300486388\n",
      "grad=0.07032680008300034\n",
      "unweighted_step=0.07032680008300034\n",
      "learning_rate=9.698309680914066e-08\n",
      "key='query_normalized_intensity_b', 0.6642539417545215, current_value=0.09949622043381873, updated=0.09949621361330786, learning_rate=9.698309680914066e-08, grad=0.07032680008300034\n",
      "pred_val=0.4743184343605115\n",
      "loss_grad=-1.0513631312789768\n",
      "grad=-0.021297316251602454\n",
      "unweighted_step=-0.021297316251602454\n",
      "learning_rate=6.556951088668481e-07\n",
      "loss_grad=-1.0513631312789768\n",
      "grad=0.00016750980936408615\n",
      "unweighted_step=0.00016750980936408615\n",
      "learning_rate=3.07483722991569e-07\n",
      "loss_grad=-1.0513631312789768\n",
      "grad=-0.7990375030875478\n",
      "unweighted_step=-0.7990375030875478\n",
      "learning_rate=3.07483722991569e-07\n",
      "loss_grad=-1.0513631312789768\n",
      "grad=-0.07986686858549596\n",
      "unweighted_step=-0.07986686858549596\n",
      "learning_rate=2.7524989345837925e-07\n",
      "loss_grad=-1.0513631312789768\n",
      "grad=0.07805124372526517\n",
      "unweighted_step=0.07805124372526517\n",
      "learning_rate=2.7524989345837925e-07\n",
      "loss_grad=-1.0513631312789768\n",
      "grad=-0.08845407252475923\n",
      "unweighted_step=-0.08845407252475923\n",
      "learning_rate=2.1800132125806766e-06\n",
      "loss_grad=-1.0513631312789768\n",
      "grad=0.08734425359628732\n",
      "unweighted_step=0.08734425359628732\n",
      "learning_rate=8.079452833144018e-08\n",
      "loss_grad=-1.0513631312789768\n",
      "grad=0.022439602393562078\n",
      "unweighted_step=0.022439602393562078\n",
      "learning_rate=5.896443198052769e-08\n",
      "loss_grad=-1.0513631312789768\n",
      "grad=-0.018422177795907598\n",
      "unweighted_step=-0.018422177795907598\n",
      "learning_rate=8.247073131782883e-08\n",
      "pred_val=0.4984414435548254\n",
      "loss_grad=-1.003117112890349\n",
      "grad=-0.10720752815807981\n",
      "unweighted_step=-0.10720752815807981\n",
      "learning_rate=6.981514450183408e-07\n",
      "loss_grad=-1.003117112890349\n",
      "grad=0.000893921265057083\n",
      "unweighted_step=0.000893921265057083\n",
      "learning_rate=3.2739332918868673e-07\n",
      "loss_grad=-1.003117112890349\n",
      "grad=-0.05131939164980301\n",
      "unweighted_step=-0.05131939164980301\n",
      "learning_rate=3.2739332918868673e-07\n",
      "loss_grad=-1.003117112890349\n",
      "grad=-0.009896749916069746\n",
      "unweighted_step=-0.009896749916069746\n",
      "learning_rate=2.930723555101517e-07\n",
      "loss_grad=-1.003117112890349\n",
      "grad=0.003100725870565591\n",
      "unweighted_step=0.003100725870565591\n",
      "learning_rate=2.930723555101517e-07\n",
      "loss_grad=-1.003117112890349\n",
      "grad=-0.2548360049230941\n",
      "unweighted_step=-0.2548360049230941\n",
      "learning_rate=2.146215344796845e-06\n",
      "loss_grad=-1.003117112890349\n",
      "grad=0.24948600840826626\n",
      "unweighted_step=0.24948600840826626\n",
      "learning_rate=7.954007954069144e-08\n",
      "loss_grad=-1.003117112890349\n",
      "grad=0.10871226699400109\n",
      "unweighted_step=0.10871226699400109\n",
      "learning_rate=5.748749540303251e-08\n",
      "loss_grad=-1.003117112890349\n",
      "grad=-0.09532112497147584\n",
      "unweighted_step=-0.09532112497147584\n",
      "learning_rate=8.042388647398111e-08\n",
      "pred_val=0.41063983309297863\n",
      "loss_grad=0.8212796661859573\n",
      "grad=0.11165823218410767\n",
      "unweighted_step=0.11165823218410767\n",
      "learning_rate=5.708260337864638e-07\n",
      "loss_grad=0.8212796661859573\n",
      "grad=-0.0009547760365446684\n",
      "unweighted_step=-0.0009547760365446684\n",
      "learning_rate=2.6768495207374697e-07\n",
      "loss_grad=0.8212796661859573\n",
      "grad=2.1316354796367603\n",
      "unweighted_step=2.1316354796367603\n",
      "learning_rate=2.6768495207374697e-07\n",
      "loss_grad=0.8212796661859573\n",
      "grad=0.22121066131116474\n",
      "unweighted_step=0.22121066131116474\n",
      "learning_rate=2.3962326793060997e-07\n",
      "loss_grad=0.8212796661859573\n",
      "grad=-0.20717160305419396\n",
      "unweighted_step=-0.20717160305419396\n",
      "learning_rate=2.3962326793060997e-07\n",
      "loss_grad=0.8212796661859573\n",
      "grad=-0.09824887239962808\n",
      "unweighted_step=-0.09824887239962808\n",
      "learning_rate=2.236889171021728e-06\n",
      "loss_grad=0.8212796661859573\n",
      "grad=0.09722706957840051\n",
      "unweighted_step=0.09722706957840051\n",
      "learning_rate=8.289959766406425e-08\n",
      "loss_grad=0.8212796661859573\n",
      "grad=0.009361566851514552\n",
      "unweighted_step=0.009361566851514552\n",
      "learning_rate=5.964189898159051e-08\n",
      "loss_grad=0.8212796661859573\n",
      "grad=-0.06307881711948551\n",
      "unweighted_step=-0.06307881711948551\n",
      "learning_rate=8.344705874354644e-08\n",
      "pred_val=0.44976842733310785\n",
      "loss_grad=0.8995368546662157\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=5.372543446183077e-07\n",
      "loss_grad=0.8995368546662157\n",
      "grad=0.0\n",
      "unweighted_step=0.0\n",
      "learning_rate=2.566596704637185e-07\n",
      "loss_grad=0.8995368546662157\n",
      "grad=1.328827273857393\n",
      "unweighted_step=1.328827273857393\n",
      "learning_rate=2.566596704637185e-07\n",
      "loss_grad=0.8995368546662157\n",
      "grad=0.12182962292595266\n",
      "unweighted_step=0.12182962292595266\n",
      "learning_rate=2.2975377773781633e-07\n",
      "loss_grad=0.8995368546662157\n",
      "grad=-0.12182962292595266\n",
      "unweighted_step=-0.12182962292595266\n",
      "learning_rate=2.2975377773781633e-07\n",
      "loss_grad=0.8995368546662157\n",
      "grad=-0.0002858429167101678\n",
      "unweighted_step=-0.0002858429167101678\n",
      "learning_rate=2.3959859475821115e-06\n",
      "loss_grad=0.8995368546662157\n",
      "grad=-0.00056320542654495\n",
      "unweighted_step=-0.00056320542654495\n",
      "learning_rate=6.871395131173966e-08\n",
      "loss_grad=0.8995368546662157\n",
      "grad=8.683392539480072e-05\n",
      "unweighted_step=8.683392539480072e-05\n",
      "learning_rate=6.374156497544331e-08\n",
      "loss_grad=0.8995368546662157\n",
      "grad=0.00026710858535981786\n",
      "unweighted_step=0.00026710858535981786\n",
      "learning_rate=6.936159255312031e-08\n",
      "pred_val=0.29684436404607617\n",
      "loss_grad=0.5936887280921523\n",
      "grad=0.017507952716004197\n",
      "unweighted_step=0.017507952716004197\n",
      "learning_rate=4.724648205232952e-07\n",
      "loss_grad=0.5936887280921523\n",
      "grad=-0.00014555257532996224\n",
      "unweighted_step=-0.00014555257532996224\n",
      "learning_rate=2.6420706523251674e-07\n",
      "loss_grad=0.5936887280921523\n",
      "grad=3.1648413057236895\n",
      "unweighted_step=3.1648413057236895\n",
      "learning_rate=2.6420706523251674e-07\n",
      "loss_grad=0.5936887280921523\n",
      "grad=0.30081746568759554\n",
      "unweighted_step=0.30081746568759554\n",
      "learning_rate=2.3650997148293045e-07\n",
      "loss_grad=0.5936887280921523\n",
      "grad=-0.2966232450025032\n",
      "unweighted_step=-0.2966232450025032\n",
      "learning_rate=2.3650997148293045e-07\n",
      "loss_grad=0.5936887280921523\n",
      "grad=0.0027125794039532165\n",
      "unweighted_step=0.0027125794039532165\n",
      "learning_rate=1.951381857953532e-06\n",
      "loss_grad=0.5936887280921523\n",
      "grad=-0.004410097600005459\n",
      "unweighted_step=-0.004410097600005459\n",
      "learning_rate=6.627053824081188e-08\n",
      "loss_grad=0.5936887280921523\n",
      "grad=-0.009954302507842126\n",
      "unweighted_step=-0.009954302507842126\n",
      "learning_rate=5.198959577242009e-08\n",
      "loss_grad=0.5936887280921523\n",
      "grad=-0.016050881522474695\n",
      "unweighted_step=-0.016050881522474695\n",
      "learning_rate=6.481130695221834e-08\n",
      "pred_val=0.48104202160632764\n",
      "loss_grad=-1.0379159567873448\n",
      "grad=-0.09174799569148684\n",
      "unweighted_step=-0.09174799569148684\n",
      "learning_rate=4.300832531758516e-07\n",
      "loss_grad=-1.0379159567873448\n",
      "grad=0.000791313084544647\n",
      "unweighted_step=0.000791313084544647\n",
      "learning_rate=2.2069133782842848e-07\n",
      "loss_grad=-1.0379159567873448\n",
      "grad=-0.5863183593446737\n",
      "unweighted_step=-0.5863183593446737\n",
      "learning_rate=2.2069133782842848e-07\n",
      "loss_grad=-1.0379159567873448\n",
      "grad=-0.06377261139707743\n",
      "unweighted_step=-0.06377261139707743\n",
      "learning_rate=1.9755604177502333e-07\n",
      "loss_grad=-1.0379159567873448\n",
      "grad=0.05560926494031695\n",
      "unweighted_step=0.05560926494031695\n",
      "learning_rate=1.9755604177502333e-07\n",
      "loss_grad=-1.0379159567873448\n",
      "grad=-0.39406047699837815\n",
      "unweighted_step=-0.39406047699837815\n",
      "learning_rate=1.839725798560991e-06\n",
      "loss_grad=-1.0379159567873448\n",
      "grad=0.3872785272784813\n",
      "unweighted_step=0.3872785272784813\n",
      "learning_rate=5.750822113430147e-08\n",
      "loss_grad=-1.0379159567873448\n",
      "grad=0.17198604044420243\n",
      "unweighted_step=0.17198604044420243\n",
      "learning_rate=4.898379154683399e-08\n",
      "loss_grad=-1.0379159567873448\n",
      "grad=-0.15464630369468466\n",
      "unweighted_step=-0.15464630369468466\n",
      "learning_rate=6.592598428352931e-08\n",
      "pred_val=0.1904297507258189\n",
      "loss_grad=0.3808595014516378\n",
      "grad=0.029715301280932707\n",
      "unweighted_step=0.029715301280932707\n",
      "learning_rate=3.848606695137161e-07\n",
      "loss_grad=0.3808595014516378\n",
      "grad=-0.00025293241958319255\n",
      "unweighted_step=-0.00025293241958319255\n",
      "learning_rate=2.057619128665374e-07\n",
      "loss_grad=0.3808595014516378\n",
      "grad=2.5172515618888878\n",
      "unweighted_step=2.5172515618888878\n",
      "learning_rate=2.057619128665374e-07\n",
      "loss_grad=0.3808595014516378\n",
      "grad=0.24192205532164757\n",
      "unweighted_step=0.24192205532164757\n",
      "learning_rate=1.8419168352485325e-07\n",
      "loss_grad=0.3808595014516378\n",
      "grad=-0.23651472426105496\n",
      "unweighted_step=-0.23651472426105496\n",
      "learning_rate=1.8419168352485325e-07\n",
      "loss_grad=0.3808595014516378\n",
      "grad=0.00942181429889753\n",
      "unweighted_step=0.00942181429889753\n",
      "learning_rate=1.6164005361338212e-06\n",
      "loss_grad=0.3808595014516378\n",
      "grad=-0.010418316225150772\n",
      "unweighted_step=-0.010418316225150772\n",
      "learning_rate=5.268386797891483e-08\n",
      "loss_grad=0.3808595014516378\n",
      "grad=-0.016876034517237997\n",
      "unweighted_step=-0.016876034517237997\n",
      "learning_rate=4.3052233897691836e-08\n",
      "loss_grad=0.3808595014516378\n",
      "grad=-0.003918912678096326\n",
      "unweighted_step=-0.003918912678096326\n",
      "learning_rate=6.978920772498135e-08\n",
      "pred_val=0.09347394146985298\n",
      "loss_grad=-1.813052117060294\n",
      "grad=-0.38420178220834833\n",
      "unweighted_step=-0.38420178220834833\n",
      "learning_rate=3.473653188279364e-07\n",
      "loss_grad=-1.813052117060294\n",
      "grad=0.003501138351144841\n",
      "unweighted_step=0.003501138351144841\n",
      "learning_rate=1.818573622551257e-07\n",
      "loss_grad=-1.813052117060294\n",
      "grad=-10.35201543536307\n",
      "unweighted_step=-10.35201543536307\n",
      "learning_rate=1.818573622551257e-07\n",
      "loss_grad=-1.813052117060294\n",
      "grad=-1.0669463285251821\n",
      "unweighted_step=-1.0669463285251821\n",
      "learning_rate=1.6279307112044341e-07\n",
      "loss_grad=-1.813052117060294\n",
      "grad=1.0212496365291173\n",
      "unweighted_step=1.0212496365291173\n",
      "learning_rate=1.6279307112044341e-07\n",
      "loss_grad=-1.813052117060294\n",
      "grad=0.41054739765352205\n",
      "unweighted_step=0.41054739765352205\n",
      "learning_rate=1.599112716053259e-06\n",
      "loss_grad=-1.813052117060294\n",
      "grad=-0.4040673451557722\n",
      "unweighted_step=-0.4040673451557722\n",
      "learning_rate=5.310824181025052e-08\n",
      "loss_grad=-1.813052117060294\n",
      "grad=-0.1592074601088808\n",
      "unweighted_step=-0.1592074601088808\n",
      "learning_rate=4.259819960597133e-08\n",
      "loss_grad=-1.813052117060294\n",
      "grad=0.3085624654326155\n",
      "unweighted_step=0.3085624654326155\n",
      "learning_rate=5.7276023548322504e-08\n",
      "pred_val=0.43625512418302353\n",
      "loss_grad=0.8725102483660471\n",
      "grad=0.07555780929302643\n",
      "unweighted_step=0.07555780929302643\n",
      "learning_rate=3.1218168946451405e-07\n",
      "loss_grad=0.8725102483660471\n",
      "grad=-0.0006546064836599579\n",
      "unweighted_step=-0.0006546064836599579\n",
      "learning_rate=1.6514246832254598e-07\n",
      "loss_grad=0.8725102483660471\n",
      "grad=1.632404296546415\n",
      "unweighted_step=1.632404296546415\n",
      "learning_rate=1.6514246832254598e-07\n",
      "loss_grad=0.8725102483660471\n",
      "grad=0.15883459473969638\n",
      "unweighted_step=0.15883459473969638\n",
      "learning_rate=1.4783041641680948e-07\n",
      "loss_grad=0.8725102483660471\n",
      "grad=-0.14812322224785454\n",
      "unweighted_step=-0.14812322224785454\n",
      "learning_rate=1.4783041641680948e-07\n",
      "loss_grad=0.8725102483660471\n",
      "grad=0.037690816060844366\n",
      "unweighted_step=0.037690816060844366\n",
      "learning_rate=1.6705168906599886e-06\n",
      "loss_grad=0.8725102483660471\n",
      "grad=-0.03699319308843564\n",
      "unweighted_step=-0.03699319308843564\n",
      "learning_rate=5.597755000328098e-08\n",
      "loss_grad=0.8725102483660471\n",
      "grad=-0.06407562890196224\n",
      "unweighted_step=-0.06407562890196224\n",
      "learning_rate=4.4503486591878626e-08\n",
      "loss_grad=0.8725102483660471\n",
      "grad=0.01875585178128743\n",
      "unweighted_step=0.01875585178128743\n",
      "learning_rate=5.500503773695998e-08\n",
      "pred_val=0.3076741833049192\n",
      "loss_grad=0.6153483666098384\n",
      "grad=0.059123374664976754\n",
      "unweighted_step=0.059123374664976754\n",
      "learning_rate=3.119807833301179e-07\n",
      "key='mult_a', 0.664521482829003, current_value=0.0015846419305178114, updated=0.0015846234851610702, learning_rate=3.119807833301179e-07, grad=0.059123374664976754\n",
      "loss_grad=0.6153483666098384\n",
      "grad=-0.0005108467360728313\n",
      "unweighted_step=-0.0005108467360728313\n",
      "learning_rate=1.658102953595052e-07\n",
      "key='mult_b', -0.6801464828872259, current_value=0.9999956698126633, updated=0.999995669897367, learning_rate=1.658102953595052e-07, grad=-0.0005108467360728313\n",
      "loss_grad=0.6153483666098384\n",
      "grad=3.150254436237319\n",
      "unweighted_step=3.150254436237319\n",
      "learning_rate=1.658102953595052e-07\n",
      "key='dif_a', 0.6801464828872259, current_value=-0.03377447771566587, updated=-0.0337750000602844, learning_rate=1.658102953595052e-07, grad=3.150254436237319\n",
      "loss_grad=0.6153483666098384\n",
      "grad=0.303380815365712\n",
      "unweighted_step=0.303380815365712\n",
      "learning_rate=1.4842823447034168e-07\n",
      "key='dif_b', 0.6801464828872259, current_value=1.000997851284491, updated=1.0009978062542122, learning_rate=1.4842823447034168e-07, grad=0.303380815365712\n",
      "loss_grad=0.6153483666098384\n",
      "grad=-0.2920314519980605\n",
      "unweighted_step=-0.2920314519980605\n",
      "learning_rate=1.4842823447034168e-07\n",
      "key='add_norm_b', -0.6801464828872259, current_value=0.9991010163626706, updated=0.9991010597083834, learning_rate=1.4842823447034168e-07, grad=-0.2920314519980605\n",
      "loss_grad=0.6153483666098384\n",
      "grad=0.1309187607773992\n",
      "unweighted_step=0.1309187607773992\n",
      "learning_rate=1.791339005351659e-06\n",
      "key='target_normalized_intensity_a', 0.9077539520196769, current_value=0.09928838633860466, updated=0.09928815181872196, learning_rate=1.791339005351659e-06, grad=0.1309187607773992\n",
      "loss_grad=0.6153483666098384\n",
      "grad=-0.1300940178928587\n",
      "unweighted_step=-0.1300940178928587\n",
      "learning_rate=6.028859244597928e-08\n",
      "key='query_normalized_intensity_a', -0.9233792501008963, current_value=0.10074842961588433, updated=0.10074843745906956, learning_rate=6.028859244597928e-08, grad=-0.1300940178928587\n",
      "loss_grad=0.6153483666098384\n",
      "grad=-0.08278804763561913\n",
      "unweighted_step=-0.08278804763561913\n",
      "learning_rate=4.7723913239825474e-08\n",
      "key='target_normalized_intensity_b', -0.9078782281696953, current_value=0.10030323121891886, updated=0.10030323516988847, learning_rate=4.7723913239825474e-08, grad=-0.08278804763561913\n",
      "loss_grad=0.6153483666098384\n",
      "grad=0.03517359569499659\n",
      "unweighted_step=0.03517359569499659\n",
      "learning_rate=5.666481884288245e-08\n",
      "key='query_normalized_intensity_b', 0.7672502479899946, current_value=0.09949622084799718, updated=0.09949621885489175, learning_rate=5.666481884288245e-08, grad=0.03517359569499659\n",
      "pred_val=0.41949076472104335\n",
      "loss_grad=-1.1610184705579134\n",
      "grad=-0.27402308413863596\n",
      "unweighted_step=-0.27402308413863596\n",
      "learning_rate=2.6528405425070946e-07\n",
      "loss_grad=-1.1610184705579134\n",
      "grad=0.002371260428615845\n",
      "unweighted_step=0.002371260428615845\n",
      "learning_rate=1.40603487209241e-07\n",
      "loss_grad=-1.1610184705579134\n",
      "grad=-2.7322819339536872\n",
      "unweighted_step=-2.7322819339536872\n",
      "learning_rate=1.40603487209241e-07\n",
      "loss_grad=-1.1610184705579134\n",
      "grad=-0.2868163104615073\n",
      "unweighted_step=-0.2868163104615073\n",
      "learning_rate=1.258638815014001e-07\n",
      "loss_grad=-1.1610184705579134\n",
      "grad=0.261997128174283\n",
      "unweighted_step=0.261997128174283\n",
      "learning_rate=1.258638815014001e-07\n",
      "loss_grad=-1.1610184705579134\n",
      "grad=0.6673397053855123\n",
      "unweighted_step=0.6673397053855123\n",
      "learning_rate=1.9456863143113206e-06\n",
      "loss_grad=-1.1610184705579134\n",
      "grad=-0.6602436089294094\n",
      "unweighted_step=-0.6602436089294094\n",
      "learning_rate=6.562454811604135e-08\n",
      "loss_grad=-1.1610184705579134\n",
      "grad=-0.2834088076933733\n",
      "unweighted_step=-0.2834088076933733\n",
      "learning_rate=5.183684284685875e-08\n",
      "loss_grad=-1.1610184705579134\n",
      "grad=0.36474620648577016\n",
      "unweighted_step=0.36474620648577016\n",
      "learning_rate=6.035299234716477e-08\n",
      "pred_val=0.336821073962964\n",
      "loss_grad=0.673642147925928\n",
      "grad=0.026479856625064895\n",
      "unweighted_step=0.026479856625064895\n",
      "learning_rate=2.453450689519886e-07\n",
      "loss_grad=0.673642147925928\n",
      "grad=-0.00022082371228738655\n",
      "unweighted_step=-0.00022082371228738655\n",
      "learning_rate=1.3020037385611422e-07\n",
      "loss_grad=0.673642147925928\n",
      "grad=3.0196204296202342\n",
      "unweighted_step=3.0196204296202342\n",
      "learning_rate=1.3020037385611422e-07\n",
      "loss_grad=0.673642147925928\n",
      "grad=0.28799689408194346\n",
      "unweighted_step=0.28799689408194346\n",
      "learning_rate=1.1655133703815349e-07\n",
      "loss_grad=0.673642147925928\n",
      "grad=-0.2855108038461268\n",
      "unweighted_step=-0.2855108038461268\n",
      "learning_rate=1.1655133703815349e-07\n",
      "loss_grad=0.673642147925928\n",
      "grad=-0.07638945671786329\n",
      "unweighted_step=-0.07638945671786329\n",
      "learning_rate=1.5700101919319031e-06\n",
      "loss_grad=0.673642147925928\n",
      "grad=0.07377782426163526\n",
      "unweighted_step=0.07377782426163526\n",
      "learning_rate=5.2876753649466154e-08\n",
      "loss_grad=0.673642147925928\n",
      "grad=0.0275455641074608\n",
      "unweighted_step=0.0275455641074608\n",
      "learning_rate=4.1827621913172626e-08\n",
      "loss_grad=0.673642147925928\n",
      "grad=-0.03685406345802545\n",
      "unweighted_step=-0.03685406345802545\n",
      "learning_rate=4.9335929677871646e-08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m model_\u001b[38;5;241m.\u001b[39mmax_iter \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m     41\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 42\u001b[0m model_\u001b[38;5;241m.\u001b[39mfit(demo_matches)\n\u001b[1;32m     43\u001b[0m accumulated_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     45\u001b[0m demo_matches[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [model_\u001b[38;5;241m.\u001b[39msim_func\u001b[38;5;241m.\u001b[39mpredict(demo_matches\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m], demo_matches\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m],demo_matches\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecquery\u001b[39m\u001b[38;5;124m'\u001b[39m], demo_matches\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprectarget\u001b[39m\u001b[38;5;124m'\u001b[39m], grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(demo_matches))]\n",
      "File \u001b[0;32m~/projects/TunaSim/funcOb.py:143\u001b[0m, in \u001b[0;36mfunc_ob.fit\u001b[0;34m(self, train_data, verbose)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_ones \u001b[38;5;241m=\u001b[39m counts[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstoch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstoch_descent(verbose)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscipy_solver_estimate(train_data)\n",
      "File \u001b[0;32m~/projects/TunaSim/funcOb.py:229\u001b[0m, in \u001b[0;36mfunc_ob.stoch_descent\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    226\u001b[0m     score, pred_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingle_match_grad()\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     score, pred_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouped_match_grad()\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m#update with the score of choice and funcOb's loss function\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred_val\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/TunaSim/funcOb.py:193\u001b[0m, in \u001b[0;36mfunc_ob.grouped_match_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzeros \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m#select only what we are interested in grouping\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m sub \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroupby_column] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data\u001b[38;5;241m.\u001b[39miloc[index][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroupby_column]]\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m#in the first round, we want to pick the index with the highest similarity scores\u001b[39;00m\n\u001b[1;32m    196\u001b[0m sims \u001b[38;5;241m=\u001b[39m sub\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim_func\u001b[38;5;241m.\u001b[39mpredict(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m], x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m], x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecquery\u001b[39m\u001b[38;5;124m'\u001b[39m], x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprectarget\u001b[39m\u001b[38;5;124m'\u001b[39m], grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m), \n\u001b[1;32m    197\u001b[0m           axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m    198\u001b[0m           result_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cmp_method(other, operator\u001b[38;5;241m.\u001b[39meq)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:5803\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5800\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   5801\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 5803\u001b[0m res_values \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcomparison_op(lvalues, rvalues, op)\n\u001b[1;32m   5805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:346\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 346\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:131\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    129\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mscalar_compare(x\u001b[38;5;241m.\u001b[39mravel(), y, op)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/inputs/demo_matches_no_prec.pkl')\n",
    "demo_matches_val = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/inputs/demo_matches_val_no_prec.pkl')\n",
    "\n",
    "demo_matches['score'] = 1 * demo_matches['InchiCoreMatch']\n",
    "demo_matches_val['score'] = 1 * demo_matches_val['InchiCoreMatch']\n",
    "demo_matches['queryID_target_base'] = [str(demo_matches.iloc[i]['queryID']) + '_' + demo_matches.iloc[i]['target_base'] for i in range(len(demo_matches))]\n",
    "\n",
    "train_auc_top = {i.name: list() for i in func_obs}\n",
    "val_auc_top = {i.name: list() for i in func_obs}\n",
    "\n",
    "train_auc_all = {i.name: list() for i in func_obs}\n",
    "val_auc_all = {i.name: list() for i in func_obs}\n",
    "\n",
    "train_times = {i.name: list() for i in func_obs}\n",
    "\n",
    "absolutes = [0, 1e4, 5e4]\n",
    "offsets = [absolutes[i+1] - absolutes[i] for i in range(len(absolutes)-1)]\n",
    "\n",
    "reps = 1\n",
    "\n",
    "trained_obs = []\n",
    "\n",
    "for model in func_obs:\n",
    "\n",
    "    for _ in range(reps):\n",
    "\n",
    "        model_ = copy.deepcopy(model)\n",
    "\n",
    "        accumulated = 0\n",
    "        accumulated_time = 0\n",
    "        train_aucs_top = list()\n",
    "        val_aucs_top = list()\n",
    "        train_aucs_all = list()\n",
    "        val_aucs_all = list()\n",
    "        trained_obs_sub = list()\n",
    "\n",
    "        for i in offsets:\n",
    "            \n",
    "            model_.max_iter = i\n",
    "            \n",
    "            start = time.time()\n",
    "            model_.fit(demo_matches)\n",
    "            accumulated_time += time.time() - start\n",
    "\n",
    "            demo_matches['preds'] = [model_.sim_func.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'],demo_matches.iloc[i]['precquery'], demo_matches.iloc[i]['prectarget'], grads = False) for i in range(len(demo_matches))]\n",
    "            demo_matches_val['preds'] = [model_.sim_func.predict(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], demo_matches_val.iloc[i]['precquery'], demo_matches_val.iloc[i]['prectarget'], grads = False) for i in range(len(demo_matches_val))]\n",
    "\n",
    "            train_aucs_all.append(round(roc_auc_score(demo_matches['score'] , demo_matches['preds']), 4)) \n",
    "            val_aucs_all.append(round(roc_auc_score(demo_matches_val['score'] , demo_matches_val['preds']),4))\n",
    "\n",
    "            temp = demo_matches.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "            temp_val = demo_matches_val.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "\n",
    "            train_aucs_top.append(round(roc_auc_score(temp['score'] , temp['preds']), 4)) \n",
    "            val_aucs_top.append(round(roc_auc_score(temp_val['score'] , temp_val['preds']),4))\n",
    "\n",
    "            accumulated += model_.max_iter\n",
    "\n",
    "        trained_obs_sub.append(copy.deepcopy(model_))\n",
    "        \n",
    "    trained_obs.append(trained_obs_sub)\n",
    "    train_times[model.name].append(round(accumulated_time/60, 4))\n",
    "    train_auc_all[model.name].append(train_aucs_all)\n",
    "    train_auc_top[model.name].append(train_aucs_top)\n",
    "    val_auc_all[model.name].append(val_aucs_all)\n",
    "    val_auc_top[model.name].append(val_aucs_top)\n",
    "\n",
    "    print(model.name)\n",
    "\n",
    "    model_1 = model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1722b9fd0>]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pos = 1\n",
    "neg = -2\n",
    "pos_prob = 0.5\n",
    "neg_prob = 0.5\n",
    "\n",
    "a_ = list(np.random.choice([neg, pos], 50000, p = [0.5,0.5]))\n",
    "beta = 0.5\n",
    "\n",
    "running = 0\n",
    "accum_beta = 0.5\n",
    "running_scale = 0.1\n",
    "step = 1\n",
    "steps_ = list()\n",
    "runnings_ = list()\n",
    "for i in a_:\n",
    "\n",
    "    running = running * beta + i * (1 - beta)\n",
    "    runnings_.append(running)\n",
    "    running_scale = running_scale * accum_beta + abs(i) * (1 - accum_beta) \n",
    "    #step *=  abs(running) / running_scale\n",
    "    steps_.append(abs(running) / running_scale)\n",
    "    #print(f'{round(running,2)}, {round(running_std,2)}, {round(abs(running) / running_std,2)}, {round(step,2)}')\n",
    "\n",
    "plt.plot(range(len(steps_)), steps_, c = 'b')\n",
    "\n",
    "a = list(np.random.choice([-1,1], 50000, p = [-neg/ (pos - neg), pos / (pos - neg)]))\n",
    "beta = 0.5\n",
    "\n",
    "running = 0\n",
    "accum_beta = 0.5\n",
    "running_std = 0.1\n",
    "step = 1\n",
    "steps = list()\n",
    "runnings = list()\n",
    "for i in a:\n",
    "\n",
    "    running = running * beta + i * (1 - beta)\n",
    "    runnings.append(running)\n",
    "    running_std = running_std * accum_beta + abs(i - running) * (1 - accum_beta) \n",
    "    step *=  abs(running) / running_std\n",
    "    steps.append(abs(running) / running_std)\n",
    "    #print(f'{round(running,2)}, {round(running_std,2)}, {round(abs(running) / running_std,2)}, {round(step,2)}')\n",
    "\n",
    "plt.plot(range(len(steps)), steps, c = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5273525569041206 122.28572328095206\n",
      "-0.506672194865761 -0.32914025910411066\n",
      "-0.3372741675653366 -0.32916\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(steps_) , np.mean(steps))\n",
    "print(np.mean(runnings_) , np.mean(runnings))\n",
    "print(np.mean(a_)/ np.mean(np.abs(a_)) , np.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_.sim_func.grads1_score_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_auc_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(temp['preds'], bins = 100)\n",
    "plt.title('Preds Train')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(temp_val['preds'], bins = 100)\n",
    "plt.title('Preds Val')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['residual'] = abs(temp['score'] - temp['preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model_1.init_vals:\n",
    "    print(i, getattr(model_1.sim_func,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab only inchicores where performance was bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['residual'] = np.abs(temp['score'] - temp['preds'])\n",
    "median_residual = np.median(temp['residual'])\n",
    "\n",
    "bad_ids = list()\n",
    "for i in range(len(temp)):\n",
    "\n",
    "    if temp.iloc[i]['residual'] >= median_residual:\n",
    "        bad_ids.append(temp.iloc[i]['queryID_target_base'])\n",
    "\n",
    "bad_ids = set(bad_ids)\n",
    "\n",
    "residual_inds = list()\n",
    "for i in range(len(demo_matches)):\n",
    "\n",
    "    if demo_matches.iloc[i]['queryID_target_base'] in bad_ids:\n",
    "\n",
    "        residual_inds.append(i)\n",
    "\n",
    "demo_matches = demo_matches.iloc[residual_inds]\n",
    "print(len(demo_matches))\n",
    "print(len(bad_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_auc_top = {i.name: list() for i in func_obs}\n",
    "val_auc_top = {i.name: list() for i in func_obs}\n",
    "\n",
    "train_auc_all = {i.name: list() for i in func_obs}\n",
    "val_auc_all = {i.name: list() for i in func_obs}\n",
    "\n",
    "train_times = {i.name: list() for i in func_obs}\n",
    "\n",
    "absolutes = [0, 1e5]\n",
    "offsets = [absolutes[i+1] - absolutes[i] for i in range(len(absolutes)-1)]\n",
    "\n",
    "reps = 1\n",
    "\n",
    "trained_obs = []\n",
    "\n",
    "for model in func_obs[:1]:\n",
    "\n",
    "    for _ in range(reps):\n",
    "\n",
    "        model_ = copy.deepcopy(model)\n",
    "\n",
    "        accumulated = 0\n",
    "        accumulated_time = 0\n",
    "        train_aucs_top = list()\n",
    "        val_aucs_top = list()\n",
    "        train_aucs_all = list()\n",
    "        val_aucs_all = list()\n",
    "        trained_obs_sub = list()\n",
    "\n",
    "        for i in offsets:\n",
    "            \n",
    "            model_.max_iter = i\n",
    "            \n",
    "            start = time.time()\n",
    "            model_.fit(demo_matches)\n",
    "            accumulated_time += time.time() - start\n",
    "\n",
    "            demo_matches['preds'] = [model_.sim_func.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'],demo_matches.iloc[i]['precquery'], demo_matches.iloc[i]['prectarget'], grads = False) for i in range(len(demo_matches))]\n",
    "            demo_matches_val['preds'] = [model_.sim_func.predict(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], demo_matches_val.iloc[i]['precquery'], demo_matches_val.iloc[i]['prectarget'], grads = False) for i in range(len(demo_matches_val))]\n",
    "\n",
    "            train_aucs_all.append(round(roc_auc_score(demo_matches['score'] , demo_matches['preds']), 4)) \n",
    "            val_aucs_all.append(round(roc_auc_score(demo_matches_val['score'] , demo_matches_val['preds']),4))\n",
    "\n",
    "            temp = demo_matches.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "            temp_val = demo_matches_val.groupby(by=['queryID','target_base']).apply(lambda x: x[x['preds'] == max(x['preds'])].iloc[0])\n",
    "\n",
    "            train_aucs_top.append(round(roc_auc_score(temp['score'] , temp['preds']), 4)) \n",
    "            val_aucs_top.append(round(roc_auc_score(temp_val['score'] , temp_val['preds']),4))\n",
    "\n",
    "            accumulated += model_.max_iter\n",
    "\n",
    "        trained_obs_sub.append(copy.deepcopy(model_))\n",
    "        \n",
    "    trained_obs.append(trained_obs_sub)\n",
    "    train_times[model.name].append(round(accumulated_time/60, 4))\n",
    "    train_auc_all[model.name].append(train_aucs_all)\n",
    "    train_auc_top[model.name].append(train_aucs_top)\n",
    "    val_auc_all[model.name].append(val_aucs_all)\n",
    "    val_auc_top[model.name].append(val_aucs_top)\n",
    "\n",
    "    print(model.name)\n",
    "\n",
    "    model_2 = model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_auc_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model_1.init_vals:\n",
    "    print(i, getattr(model_1.sim_func,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model_2.init_vals:\n",
    "    print(i, getattr(model_2.sim_func,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#maintain mapping to 0 1 interval\n",
    "demo_matches['score'] = (demo_matches['score'] - preds_combined + 1) / 2\n",
    "plt.hist(demo_matches['score'], bins = 100)\n",
    "plt.title('train_residuals')\n",
    "plt.show()\n",
    "\n",
    "testerooni = func_ob('teesterooni',\n",
    "                     sim_func = TunaSims.ExpandedTuna,\n",
    "                     init_vals = init_vals,\n",
    "                     fixed_vals = fixed_vals,\n",
    "                     regularization_grad = regularization_grad,\n",
    "                     bounds = bounds,\n",
    "                     max_iter = 1000000,\n",
    "                     lambdas = 0.001,\n",
    "                     tol = 0,\n",
    "                     balance_classes = False)\n",
    "\n",
    "testerooni.fit(demo_matches, verbose = 100000)\n",
    "print(testerooni.converged)\n",
    "\n",
    "\n",
    "preds_3 = np.array([testerooni.sim_func.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'],demo_matches.iloc[i]['precquery'], demo_matches.iloc[i]['prectarget']) for i in range(len(demo_matches))])\n",
    "preds_val_3 = np.array([testerooni.sim_func.predict(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], demo_matches_val.iloc[i]['precquery'], demo_matches_val.iloc[i]['prectarget']) for i in range(len(demo_matches_val))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_train = dict()\n",
    "all_scores_val = dict()\n",
    "\n",
    "for sim in sim_names:\n",
    "\n",
    "    all_scores_train[sim] = np.load(f'{sims_output_dir}/train_{sim}.npy')\n",
    "    all_scores_val[sim] = np.load(f'{sims_output_dir}/val_{sim}.npy')\n",
    "\n",
    "all_scores_train['preds'] = preds\n",
    "all_scores_val['preds'] = preds_val\n",
    "\n",
    "all_scores_train['preds2'] = preds_2\n",
    "all_scores_val['preds2'] = preds_val_2\n",
    "\n",
    "all_scores_train['preds3'] = preds_3\n",
    "all_scores_val['preds3'] = preds_val_3\n",
    "\n",
    "all_scores_train['score'] = original_labels_train\n",
    "all_scores_val['score'] = original_labels_val\n",
    "\n",
    "train_data = pd.DataFrame(all_scores_train)\n",
    "val_data = pd.DataFrame(all_scores_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Val Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Train Models with Each Pair/Triplet of Sim Scores Old and New"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create column groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_sim_combos = list()\n",
    "for sim1 in sim_names:\n",
    "    for sim2 in sim_names:\n",
    "        for sim3 in sim_names:\n",
    "\n",
    "            old_sim_combos.append(list(set([sim1, sim2, sim3])))\n",
    "\n",
    "new_sim_combos = list()\n",
    "new_sims = ['preds', 'preds2', 'preds3']\n",
    "for sim1 in new_sims:\n",
    "    for sim2 in new_sims:\n",
    "        for sim3 in new_sims:  \n",
    "\n",
    "            new_sim_combos.append(list(set([sim1, sim2, sim3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Models for each Column Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_performance = dict()\n",
    "\n",
    "for combo in old_sim_combos:\n",
    "\n",
    "    print(combo)\n",
    "\n",
    "    model = hgbc()\n",
    "    model.fit(train_data[combo], train_data['score'])\n",
    "    train_auc = roc_auc_score(original_labels_train, model.predict_proba(train_data[combo])[:,1])\n",
    "    val_auc = roc_auc_score(original_labels_val, model.predict_proba(val_data[combo])[:,1])\n",
    "\n",
    "    sim_performance['_'.join(combo)] = (train_auc, val_auc)\n",
    "\n",
    "sim_performance_new = dict()\n",
    "for combo in new_sim_combos:\n",
    "\n",
    "    print(combo)\n",
    "\n",
    "    model = hgbc()\n",
    "    model.fit(train_data[combo], train_data['score'])\n",
    "    train_auc = roc_auc_score(original_labels_train, model.predict_proba(train_data[combo])[:,1])\n",
    "    val_auc = roc_auc_score(original_labels_val, model.predict_proba(val_data[combo])[:,1])\n",
    "\n",
    "    sim_performance_new['_'.join(combo)] = (train_auc, val_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([val[1] for key, val in sim_performance.items() if len(key.split('_'))==1]))\n",
    "print(np.mean([val[1] for key, val in sim_performance.items() if len(key.split('_'))==2]))\n",
    "print(np.mean([val[1] for key, val in sim_performance.items() if len(key.split('_'))==3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([val[1] for key, val in sim_performance_new.items() if len(key.split('_'))==1 and 'preds' in key]))\n",
    "print(np.mean([val[1] for key, val in sim_performance_new.items() if len(key.split('_'))==2 and 'preds' in key]))\n",
    "print(np.mean([val[1] for key, val in sim_performance_new.items() if len(key.split('_'))==3 and 'preds' in key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_performance_2 = dict()\n",
    "for combo in new_sim_combos:\n",
    "\n",
    "    if 'preds' not in combo:\n",
    "        continue\n",
    "\n",
    "    for sim in sim_names:\n",
    "\n",
    "        combo_new = combo + [sim]\n",
    "\n",
    "        model = hgbc()\n",
    "        model.fit(train_data[combo_new], train_data['score'])\n",
    "        train_auc = roc_auc_score(original_labels_train, model.predict_proba(train_data[combo_new])[:,1])\n",
    "        val_auc = roc_auc_score(original_labels_val, model.predict_proba(val_data[combo_new])[:,1])\n",
    "\n",
    "        sim_performance_2['_'.join(combo_new)] = (train_auc, val_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([val[1] for key, val in sim_performance_2.items() if len(key.split('_'))==2]))\n",
    "print(np.mean([val[1] for key, val in sim_performance_2.items() if len(key.split('_'))==3]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
