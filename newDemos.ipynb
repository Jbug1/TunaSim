{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TunaSims\n",
    "import numpy as np\n",
    "from funcOb import func_ob\n",
    "import pandas as pd\n",
    "import tools_fast\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier as hgbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _weight_intensity_by_entropy(x):\n",
    "    WEIGHT_START = 0.25\n",
    "    ENTROPY_CUTOFF = 3\n",
    "    weight_slope = (1 - WEIGHT_START) / ENTROPY_CUTOFF\n",
    "\n",
    "    if np.sum(x) > 0:\n",
    "        entropy_x = scipy.stats.entropy(x)\n",
    "        if entropy_x < ENTROPY_CUTOFF:\n",
    "            weight = WEIGHT_START + weight_slope * entropy_x\n",
    "            x = np.power(x, weight)\n",
    "            x_sum = np.sum(x)\n",
    "            x = x / x_sum\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_mean_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Harmonic mean distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        1-2\\sum(\\frac{P_{i}Q_{i}}{P_{i}+Q_{i}})\n",
    "    \"\"\"\n",
    "\n",
    "    return 2 * np.sum(p * q / (p + q))\n",
    "\n",
    "def lorentzian_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Lorentzian distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\sum{\\ln(1+|P_i-Q_i|)}\n",
    "    \"\"\"\n",
    "\n",
    "    return 1 - np.sum(np.log(1 + np.abs(p - q)))\n",
    "\n",
    "def matusita_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Matusita distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\sqrt{\\sum(\\sqrt{P_{i}}-\\sqrt{Q_{i}})^2}\n",
    "    \"\"\"\n",
    "\n",
    "    return 1- np.sum(np.power(np.sqrt(p) - np.sqrt(q), 2))\n",
    "\n",
    "def probabilistic_symmetric_chi_squared_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Probabilistic symmetric Ï‡2 distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\frac{1}{2} \\times \\sum\\frac{(P_{i}-Q_{i}\\ )^2}{P_{i}+Q_{i}\\ }\n",
    "    \"\"\"\n",
    "\n",
    "    return 1- (1 / 2 * np.sum(np.power(p - q, 2) / (p + q)))\n",
    "\n",
    "def entropy_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Unweighted entropy distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        -\\frac{2\\times S_{PQ}-S_P-S_Q} {ln(4)}, S_I=\\sum_{i} {I_i ln(I_i)}\n",
    "    \"\"\"\n",
    "\n",
    "    merged = p + q\n",
    "    entropy_increase = 2 * \\\n",
    "                       scipy.stats.entropy(merged) - scipy.stats.entropy(p) - \\\n",
    "                       scipy.stats.entropy(q)\n",
    "    \n",
    "    return 1 - entropy_increase\n",
    "\n",
    "def dot_product_distance(p, q):\n",
    "    r\"\"\"\n",
    "    Dot product distance:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        1 - \\sqrt{\\frac{(\\sum{Q_iP_i})^2}{\\sum{Q_i^2\\sum P_i^2}}}\n",
    "    \"\"\"\n",
    "    \n",
    "    score = np.power(np.sum(q * p), 2) / (\n",
    "        np.sum(np.power(q, 2)) * np.sum(np.power(p, 2))\n",
    "    )\n",
    "    return np.sqrt(score)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \n",
    "        return 1/(1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create old similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_output_dir = '/Users/jonahpoczobutt/projects/TunaRes/oldSimResUW'\n",
    "\n",
    "demo_matches = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/train/10_ppm/chunk_1.pkl')\n",
    "demo_matches_val = pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/metlinGnps_NIST20_matchedPol/intermediateOutputs/splitMatches/train/10_ppm/chunk_2.pkl')\n",
    "demo_matches['score'] = 1 * demo_matches['InchiCoreMatch']\n",
    "demo_matches_val['score'] = 1 * demo_matches_val['InchiCoreMatch']\n",
    "\n",
    "sim_names = ['prob','matusita','entropy','dot','lorentzian','harmonic']\n",
    "distances = [probabilistic_symmetric_chi_squared_distance,\n",
    "             matusita_distance,\n",
    "             entropy_distance,\n",
    "             dot_product_distance,\n",
    "             lorentzian_distance,\n",
    "             harmonic_mean_distance]\n",
    "\n",
    "# for _ in range(len(sim_names)):\n",
    "\n",
    "#      matched_scores_val = list()\n",
    "#      for i in range(len(demo_matches_val)):\n",
    "     \n",
    "#           matched = tools_fast.match_spectrum(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], ms2_da = 0.05)\n",
    "#           matched_scores_val.append(sigmoid(distances[_](matched[:,1]/sum(matched[:,1]), matched[:,2]/sum(matched[:,2]))))\n",
    "\n",
    "#      np.save(f'{sims_output_dir}/val_{sim_names[_]}.npy', np.array(matched_scores_val))\n",
    "\n",
    "#      matched_scores = list()\n",
    "#      for i in range(len(demo_matches)):\n",
    "\n",
    "#           matched = tools_fast.match_spectrum(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'], ms2_da = 0.05)\n",
    "#           matched_scores.append(sigmoid(distances[_](matched[:,1]/sum(matched[:,1]), matched[:,2]/sum(matched[:,2]))))\n",
    "\n",
    "#      np.save(f'{sims_output_dir}/train_{sim_names[_]}.npy', np.array(matched_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_vals = {\n",
    "    'mult_a' : 0.001,\n",
    "    'mult_b': 1,\n",
    "    'dif_a': 0.001,\n",
    "    'dif_b':1,\n",
    "    'add_norm_b' : 1,\n",
    "    # 'target_normalized_intensity_int': 0,\n",
    "    # 'query_normalized_intensity_int': 0,\n",
    "    'target_normalized_intensity_a': 0.1,\n",
    "    'query_normalized_intensity_a': 0.1,\n",
    "    'target_normalized_intensity_b': 0.1,\n",
    "    'query_normalized_intensity_b': 0.1,\n",
    "    'target_normalized_intensity_c': 0.1,\n",
    "    'query_normalized_intensity_c': 0.1,\n",
    "    # 'target_mz_int': 1e-10,\n",
    "    # 'query_mz_int': 1e-10,\n",
    "    # 'target_mz_a': 0.001,\n",
    "    # 'query_mz_a': 0.001,\n",
    "    # 'target_mz_b': 0.001,\n",
    "    # 'query_mz_b': 0.001,\n",
    "    # 'target_mz_c': 0.001,\n",
    "    # 'query_mz_c': 0.001,\n",
    "    # 'target_intensity_int': 0,\n",
    "    # 'query_intensity_int': 0,\n",
    "    # 'target_intensity_a': 1,\n",
    "    # 'query_intensity_a': 1,\n",
    "    # 'target_intensity_b': 1,\n",
    "    # 'query_intensity_b': 1,\n",
    "    # 'target_intensity_c': 1,\n",
    "    # 'query_intensity_c': 1,\n",
    "    }\n",
    "\n",
    "regularization_grad = lambda x: 0.\n",
    "\n",
    "fixed_vals = {'sigmoid_score' : True, \n",
    "              'weight_combine': 'multiply'\n",
    "    }\n",
    "\n",
    "bounds = {'add_norm_b': (0, 2),\n",
    "          'mult_b': (1e-10, 2),\n",
    "          'add_norm_a': (1e-10, 3),\n",
    "          'dif_b': (1e-10, 2),\n",
    "          'dif_a':(-1.5,1.5),\n",
    "          'mult_a': (-1.5,1.5),\n",
    "          'target_normalized_intensity_int': (-0.2,1),\n",
    "          'query_normalized_intensity_int': (-0.2,1),\n",
    "          'target_normalized_intensity_a': (-2,2),\n",
    "          'query_normalized_intensity_a': (-2,2),\n",
    "          'target_normalized_intensity_b': (-2,2),\n",
    "          'query_normalized_intensity_b': (-2,2),\n",
    "          'target_normalized_intensity_c': (-2,2),\n",
    "          'query_normalized_intensity_c': (-2,2),\n",
    "          'target_normalized_intensity_a': (-2,2),\n",
    "          'query_normalized_intensity_a': (-2,2),\n",
    "          'target_mz_b': (-2,2),\n",
    "          'query_mz_b': (-2,2),\n",
    "          'target_mz_a': (-2,2),\n",
    "          'query_mz_a': (-2,2),\n",
    "          'target_mz_int': (-0.2,1),\n",
    "          'query_mz_int': (-0.2,1),\n",
    "          'target_mz_c': (-2,2),\n",
    "          'query_mz_c': (-2,2),\n",
    "           'target_intensity_int': (-0.2,1),\n",
    "           'query_intensity_int': (-0.2,1),\n",
    "          'target_intensity_a': (1e-10,2),\n",
    "          'query_intensity_a': (1e-10,2),\n",
    "          'target_intensity_b': (1e-10,2),\n",
    "          'query_intensity_b': (1e-10,2),\n",
    "          'target_intensity_c': (1e-10,2),\n",
    "          'query_intensity_c': (1e-10,2),\n",
    "          }\n",
    "\n",
    "testerooni = func_ob('teesterooni',\n",
    "                     sim_func = TunaSims.ExpandedTuna,\n",
    "                     init_vals = init_vals,\n",
    "                     fixed_vals = fixed_vals,\n",
    "                     regularization_grad = regularization_grad,\n",
    "                     bounds = bounds,\n",
    "                     max_iter = 1e5,\n",
    "                     learning_rates = 0.001,\n",
    "                     tol = 0,\n",
    "                     balance_classes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('trained values')\n",
    "for i in testerooni.init_vals.keys():\n",
    "    print(i, getattr(testerooni.sim_func,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "\n",
    "    print(f'round {i+1}')\n",
    "    testerooni.fit(demo_matches.copy(), verbose = 1e4)\n",
    "    preds = [testerooni.sim_func.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'],demo_matches.iloc[i]['precquery'], demo_matches.iloc[i]['prectarget']) for i in range(len(demo_matches))]\n",
    "    preds_val = [testerooni.sim_func.predict(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], demo_matches_val.iloc[i]['precquery'], demo_matches_val.iloc[i]['prectarget']) for i in range(len(demo_matches_val))]\n",
    "\n",
    "    print(round(roc_auc_score(demo_matches['score'] , np.array(preds)), 4), round(roc_auc_score(demo_matches_val['score'] , np.array(preds_val)), 4))\n",
    "    print('\\n') \n",
    "\n",
    "print('trained values')\n",
    "for i in testerooni.init_vals.keys():\n",
    "    print(i, getattr(testerooni.sim_func,i))\n",
    "\n",
    "print('\\n')\n",
    "print('aucs')\n",
    "print(round(roc_auc_score(demo_matches['score'] , np.array(preds)), 4), round(roc_auc_score(demo_matches_val['score'] , np.array(preds_val)), 4))\n",
    "\n",
    "for sim in sim_names:\n",
    "    print(sim, round(roc_auc_score(demo_matches['score'], np.load(f'{sims_output_dir}/train_{sim}.npy')),4), round(roc_auc_score(demo_matches_val['score'], np.load(f'{sims_output_dir}/val_{sim}.npy')),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testerooni.sim_func.grads1_score_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(preds, bins = 100)\n",
    "plt.title('Preds Train')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(preds_val, bins = 100)\n",
    "plt.title('Preds Val')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels_train = demo_matches['score']\n",
    "original_labels_val = demo_matches_val['score']\n",
    "#maintain mapping to 0 1 interval\n",
    "demo_matches['score'] = (demo_matches['score'] - preds + 1) / 2\n",
    "plt.hist(demo_matches['score'], bins = 100)\n",
    "plt.title('train_residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testerooni = func_ob('teesterooni',\n",
    "                     sim_func = TunaSims.ExpandedTuna,\n",
    "                     init_vals = init_vals,\n",
    "                     fixed_vals = fixed_vals,\n",
    "                     regularization_grad = regularization_grad,\n",
    "                     bounds = bounds,\n",
    "                     max_iter = 1000000,\n",
    "                     lambdas = 0.001,\n",
    "                     tol = 0,\n",
    "                     balance_classes = False)\n",
    "\n",
    "testerooni.fit(demo_matches, verbose = 1e7)\n",
    "print(testerooni.converged)\n",
    "\n",
    "\n",
    "preds_2 = np.array([testerooni.sim_func.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'],demo_matches.iloc[i]['precquery'], demo_matches.iloc[i]['prectarget']) for i in range(len(demo_matches))])\n",
    "preds_val_2 = np.array([testerooni.sim_func.predict(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], demo_matches_val.iloc[i]['precquery'], demo_matches_val.iloc[i]['prectarget']) for i in range(len(demo_matches_val))])\n",
    "\n",
    "plt.hist(preds_2, bins = 100)\n",
    "plt.title('Train Preds 2')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(preds_2, bins = 100)\n",
    "plt.title('Val Preds 2')\n",
    "plt.show()\n",
    "\n",
    "print('trained values')\n",
    "for i in testerooni.init_vals.keys():\n",
    "    print(i, getattr(testerooni.sim_func,i))\n",
    "\n",
    "preds_combined = preds + (2 * preds_2 - 1)\n",
    "preds_combined_val = preds_val + (2 * preds_val_2 - 1)\n",
    "\n",
    "plt.hist((1 + original_labels_train - preds_combined) / 2, bins = 100)\n",
    "plt.title('Two Stage Train Residuals')\n",
    "plt.show()\n",
    "\n",
    "plt.hist((1 + original_labels_val - preds_combined_val) / 2, bins = 100)\n",
    "plt.title('Two Stage Val Residuals')\n",
    "plt.show()\n",
    "\n",
    "print('\\n')\n",
    "print('aucs')\n",
    "print(round(roc_auc_score(original_labels_train , preds_combined), 4), round(roc_auc_score(original_labels_val, preds_combined_val), 4))\n",
    "for sim in sim_names:\n",
    "    print(sim, round(roc_auc_score(original_labels_train, np.load(f'{sims_output_dir}/train_{sim}.npy')),4), round(roc_auc_score(original_labels_val, np.load(f'{sims_output_dir}/val_{sim}.npy')),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#maintain mapping to 0 1 interval\n",
    "demo_matches['score'] = (demo_matches['score'] - preds_combined + 1) / 2\n",
    "plt.hist(demo_matches['score'], bins = 100)\n",
    "plt.title('train_residuals')\n",
    "plt.show()\n",
    "\n",
    "testerooni = func_ob('teesterooni',\n",
    "                     sim_func = TunaSims.ExpandedTuna,\n",
    "                     init_vals = init_vals,\n",
    "                     fixed_vals = fixed_vals,\n",
    "                     regularization_grad = regularization_grad,\n",
    "                     bounds = bounds,\n",
    "                     max_iter = 1000000,\n",
    "                     lambdas = 0.001,\n",
    "                     tol = 0,\n",
    "                     balance_classes = False)\n",
    "\n",
    "testerooni.fit(demo_matches, verbose = 100000)\n",
    "print(testerooni.converged)\n",
    "\n",
    "\n",
    "preds_3 = np.array([testerooni.sim_func.predict(demo_matches.iloc[i]['query'], demo_matches.iloc[i]['target'],demo_matches.iloc[i]['precquery'], demo_matches.iloc[i]['prectarget']) for i in range(len(demo_matches))])\n",
    "preds_val_3 = np.array([testerooni.sim_func.predict(demo_matches_val.iloc[i]['query'], demo_matches_val.iloc[i]['target'], demo_matches_val.iloc[i]['precquery'], demo_matches_val.iloc[i]['prectarget']) for i in range(len(demo_matches_val))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_train = dict()\n",
    "all_scores_val = dict()\n",
    "\n",
    "for sim in sim_names:\n",
    "\n",
    "    all_scores_train[sim] = np.load(f'{sims_output_dir}/train_{sim}.npy')\n",
    "    all_scores_val[sim] = np.load(f'{sims_output_dir}/val_{sim}.npy')\n",
    "\n",
    "all_scores_train['preds'] = preds\n",
    "all_scores_val['preds'] = preds_val\n",
    "\n",
    "all_scores_train['preds2'] = preds_2\n",
    "all_scores_val['preds2'] = preds_val_2\n",
    "\n",
    "all_scores_train['preds3'] = preds_3\n",
    "all_scores_val['preds3'] = preds_val_3\n",
    "\n",
    "all_scores_train['score'] = original_labels_train\n",
    "all_scores_val['score'] = original_labels_val\n",
    "\n",
    "train_data = pd.DataFrame(all_scores_train)\n",
    "val_data = pd.DataFrame(all_scores_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Val Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Train Models with Each Pair/Triplet of Sim Scores Old and New"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create column groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_sim_combos = list()\n",
    "for sim1 in sim_names:\n",
    "    for sim2 in sim_names:\n",
    "        for sim3 in sim_names:\n",
    "\n",
    "            old_sim_combos.append(list(set([sim1, sim2, sim3])))\n",
    "\n",
    "new_sim_combos = list()\n",
    "new_sims = ['preds', 'preds2', 'preds3']\n",
    "for sim1 in new_sims:\n",
    "    for sim2 in new_sims:\n",
    "        for sim3 in new_sims:  \n",
    "\n",
    "            new_sim_combos.append(list(set([sim1, sim2, sim3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Models for each Column Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_performance = dict()\n",
    "\n",
    "for combo in old_sim_combos:\n",
    "\n",
    "    print(combo)\n",
    "\n",
    "    model = hgbc()\n",
    "    model.fit(train_data[combo], train_data['score'])\n",
    "    train_auc = roc_auc_score(original_labels_train, model.predict_proba(train_data[combo])[:,1])\n",
    "    val_auc = roc_auc_score(original_labels_val, model.predict_proba(val_data[combo])[:,1])\n",
    "\n",
    "    sim_performance['_'.join(combo)] = (train_auc, val_auc)\n",
    "\n",
    "sim_performance_new = dict()\n",
    "for combo in new_sim_combos:\n",
    "\n",
    "    print(combo)\n",
    "\n",
    "    model = hgbc()\n",
    "    model.fit(train_data[combo], train_data['score'])\n",
    "    train_auc = roc_auc_score(original_labels_train, model.predict_proba(train_data[combo])[:,1])\n",
    "    val_auc = roc_auc_score(original_labels_val, model.predict_proba(val_data[combo])[:,1])\n",
    "\n",
    "    sim_performance_new['_'.join(combo)] = (train_auc, val_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([val[1] for key, val in sim_performance.items() if len(key.split('_'))==1]))\n",
    "print(np.mean([val[1] for key, val in sim_performance.items() if len(key.split('_'))==2]))\n",
    "print(np.mean([val[1] for key, val in sim_performance.items() if len(key.split('_'))==3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([val[1] for key, val in sim_performance_new.items() if len(key.split('_'))==1 and 'preds' in key]))\n",
    "print(np.mean([val[1] for key, val in sim_performance_new.items() if len(key.split('_'))==2 and 'preds' in key]))\n",
    "print(np.mean([val[1] for key, val in sim_performance_new.items() if len(key.split('_'))==3 and 'preds' in key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_performance_2 = dict()\n",
    "for combo in new_sim_combos:\n",
    "\n",
    "    if 'preds' not in combo:\n",
    "        continue\n",
    "\n",
    "    for sim in sim_names:\n",
    "\n",
    "        combo_new = combo + [sim]\n",
    "\n",
    "        model = hgbc()\n",
    "        model.fit(train_data[combo_new], train_data['score'])\n",
    "        train_auc = roc_auc_score(original_labels_train, model.predict_proba(train_data[combo_new])[:,1])\n",
    "        val_auc = roc_auc_score(original_labels_val, model.predict_proba(val_data[combo_new])[:,1])\n",
    "\n",
    "        sim_performance_2['_'.join(combo_new)] = (train_auc, val_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([val[1] for key, val in sim_performance_2.items() if len(key.split('_'))==2]))\n",
    "print(np.mean([val[1] for key, val in sim_performance_2.items() if len(key.split('_'))==3]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
