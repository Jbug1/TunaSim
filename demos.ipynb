{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from importlib import reload\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "import copy\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier as hgbc\n",
    "import pickle\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import TunaSims\n",
    "import func_ob\n",
    "import tools\n",
    "import datasetBuilder\n",
    "import testUtils\n",
    "import spectral_similarity\n",
    "import itertools\n",
    "import reweightFuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nist14='/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist14_highres.pkl'\n",
    "nist20_prot = '/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist20_prot_fiehn_.pkl'\n",
    "nist20 = '/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist20.pkl'\n",
    "nist23_prot = '/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist23_prot_deprot_only.pkl'\n",
    "nist23='/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist23_full.pkl'\n",
    "gnps='/Users/jonahpoczobutt/projects/raw_data/db_csvs/gnps_highres.pkl'\n",
    "mona='/Users/jonahpoczobutt/projects/raw_data/db_csvs/mona_highres.pkl'\n",
    "metlin='/Users/jonahpoczobutt/projects/raw_data/db_csvs/metlin.pkl'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion Points\n",
    "\n",
    "Should we look at inchiKey for match rather than inchiCore\n",
    "\n",
    "does spectral entropy relate to precursor m/z - yes\n",
    "\n",
    "does spectral entropy relate to CE - yes\n",
    "\n",
    "does peak intensity relate to m/z - meh\n",
    "\n",
    "does peak intensity relate to CE\n",
    "\n",
    "What factors should play into reweighting\n",
    "\n",
    "    -for quality measure: precursor mz\n",
    "    -for reducing corr: fragment mz\n",
    "\n",
    "does having lower correlated similarity measures produce better results\n",
    "\n",
    "can we obtain lower correlation with the same cleaning procedure in order to be memory efficient (ie thru sim measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create all Necessary Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#databases\n",
    "outputs_path='/Users/jonahpoczobutt/projects/TunaRes/test'\n",
    "\n",
    "self_search=True\n",
    "query = nist20\n",
    "target = nist20\n",
    "\n",
    "if query == target:\n",
    "    self_search = True\n",
    "    \n",
    "fullRun=True\n",
    "if fullRun:\n",
    "    os.mkdir(outputs_path)\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches/train')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches/val')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches/test')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches/port')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/datasets')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/datasets/train')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/datasets/val')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/datasets/test')\n",
    "    os.mkdir(f'{outputs_path}/gbc_res')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/train_to_func')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/train_to_error')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splt Queries into Train, Val, Test by Core or Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullRun=True\n",
    "match_category = 'inchi_base'\n",
    "if fullRun:\n",
    "\n",
    "    #This should be replaced with a function to read in all the databases\n",
    "    query_ = pd.read_pickle(query)\n",
    "\n",
    "    #jonah edit here\n",
    "    query_ = query_[:200]\n",
    "    all_bases = list(set(query_[match_category]))\n",
    "\n",
    "    if self_search:\n",
    "        query_.insert(0,'queryID', [i for i in range(len(query_))])\n",
    "    else:\n",
    "        query_.insert(0,'queryID', [\"_\" for i in range(len(query_))])\n",
    "\n",
    "    #this method is in place\n",
    "    np.random.shuffle(all_bases)\n",
    "\n",
    "    first_bases = all_bases[:int(len(all_bases)*0.5)]\n",
    "    second_bases = all_bases[int(len(all_bases)*0.5):int(len(all_bases)*0.7)]\n",
    "    third_bases = all_bases[int(len(all_bases)*0.7):]\n",
    "\n",
    "    first_query_ = query_[np.isin(query_[match_category],first_bases)]\n",
    "    first_query_.reset_index(inplace=True)\n",
    "    first_query_.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/first_query.pkl')\n",
    "    #del(first_query_)\n",
    "\n",
    "    second_query_ = query_[np.isin(query_[match_category],second_bases)]\n",
    "    second_query_.reset_index(inplace=True)\n",
    "    second_query_.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/second_query.pkl')\n",
    "    #del(second_query_)\n",
    "\n",
    "    third_query_ = query_[np.isin(query_[match_category],third_bases)]\n",
    "    third_query_.reset_index(inplace=True)\n",
    "    third_query_.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/third_query.pkl')\n",
    "    #del(third_query_)\n",
    "    #del(query_)\n",
    "\n",
    "    \n",
    "    np.save(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/first_bases.npy',first_bases)\n",
    "    np.save(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/second_bases.npy',second_bases)\n",
    "    np.save(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/third_bases.npy',third_bases)\n",
    "    del(first_bases)\n",
    "    del(second_bases)\n",
    "    del(third_bases)\n",
    "    del(all_bases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Parameters Here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1e6\n",
    "adduct_match = False\n",
    "strong_self_separation = False\n",
    "\n",
    "num_chunks=1 #number of chunks to be combined for calculating correlations and collecting testable indices\n",
    "\n",
    "label_field = 'InchiCoreMatch' # should be either inchicore or inchi\n",
    "\n",
    "comparison_metrics = ['entropy',\n",
    "                'manhattan',\n",
    "                'lorentzian',\n",
    "                'dot_product',\n",
    "                'fidelity',\n",
    "                'matusita',\n",
    "                'chi2',\n",
    "                'laplacian',\n",
    "                'laplacian_unnorm',\n",
    "                'sigmoid',\n",
    "                'sigmoid_unnorm',\n",
    "                'harmonic_mean',\n",
    "                'bhattacharya_1',\n",
    "                'squared_chord',\n",
    "                'cross_ent',\n",
    "                ]\n",
    "\n",
    "ppm_windows = [3]\n",
    "noise_threshes=[partial(reweightFuncs.noise_clip, perc_thresh = 0.0),\n",
    "                partial(reweightFuncs.noise_clip, perc_thresh = 0.5),\n",
    "                partial(reweightFuncs.noise_clip, fixed_thresh = 10)]\n",
    "\n",
    "noise_names = ['None','5%','10']\n",
    "centroid_tolerance_vals = [0.05,0.0]\n",
    "centroid_tolerance_types=['da','da']\n",
    "reweight_methods = [partial(reweightFuncs.logent,intercept = 0.25),reweightFuncs.weight_intensity_by_entropy,partial(reweightFuncs.fixed_power,power=1)]\n",
    "reweight_names = ['logent','fiehn','1']\n",
    "sim_methods=comparison_metrics\n",
    "prec_removes=[lambda x: x-1.6, lambda x: None]\n",
    "prec_remove_names = ['fiehn', 'none']\n",
    "train_size=3e6\n",
    "test_size=1e6\n",
    "test_size=2e6\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create all Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ppm_windows:\n",
    "    try:\n",
    "        os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches/train/{i}_ppm')\n",
    "        os.mkdir(f'{outputs_path}/intermediateOutputs/datasets/train/{i}_ppm')\n",
    "        os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches/val/{i}_ppm')\n",
    "        os.mkdir(f'{outputs_path}/intermediateOutputs/datasets/val/{i}_ppm')\n",
    "        os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches/test/{i}_ppm')\n",
    "        os.mkdir(f'{outputs_path}/intermediateOutputs/datasets/test/{i}_ppm')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#read in first bases and shuffle order\n",
    "query_ = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/first_query.pkl')\n",
    "query_ = query_.sample(frac=1)\n",
    "\n",
    "target_=pd.read_pickle(target)\n",
    "if self_search:\n",
    "    target_.insert(0,'queryID', [i for i in range(len(target_))])\n",
    "else:\n",
    "    target_.insert(0,'queryID', [\"*\" for i in range(len(target_))])\n",
    "\n",
    "datasetBuilder.create_matches_and_model_data(query_,\n",
    "                              target_,\n",
    "                            matchesOutputPath = f'{outputs_path}/intermediateOutputs/splitMatches/train',\n",
    "                            modelDataOutputPath = f'{outputs_path}/intermediateOutputs/datasets/train',\n",
    "                            chunk_size = chunk_size,\n",
    "                            max_size = train_size,\n",
    "                            ppm_windows = ppm_windows,\n",
    "                            noise_threshes = noise_threshes,\n",
    "                            noise_names = noise_names,\n",
    "                            centroid_tolerance_vals = centroid_tolerance_vals,\n",
    "                            centroid_tolerance_types = centroid_tolerance_types,\n",
    "                            reweight_methods = reweight_methods,\n",
    "                            reweight_names = reweight_names,\n",
    "                            sim_methods = comparison_metrics,\n",
    "                            prec_removes = prec_removes,\n",
    "                            prec_remove_names = prec_remove_names\n",
    "                            )\n",
    "\n",
    "del(query_)\n",
    "del(target_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_kernels as pk\n",
    "(1-pk([[.25,.75]], [[.75,.25]], metric=\"sigmoid\")[0][0])/(1-pk([[0,1]], [[1,0]], metric=\"sigmoid\")[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('/Users/jonahpoczobutt/projects/TunaRes/test/intermediateOutputs/splitMatches/train/3_ppm/chunk_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create all Val Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in second bases and shuffle order\n",
    "query_ = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/second_query.pkl')\n",
    "query_ = query_.sample(frac=1)\n",
    "\n",
    "target_=pd.read_pickle(target)\n",
    "if self_search:\n",
    "    target_.insert(0,'queryID', [i for i in range(len(target_))])\n",
    "else:\n",
    "    target_.insert(0,'queryID', [\"*\" for i in range(len(target_))])\n",
    "\n",
    "datasetBuilder.create_matches_and_model_data(query_,\n",
    "                              target_,\n",
    "                            matchesOutputPath = f'{outputs_path}/intermediateOutputs/splitMatches/val',\n",
    "                            modelDataOutputPath = f'{outputs_path}/intermediateOutputs/datasets/val',\n",
    "                            chunk_size = chunk_size,\n",
    "                            max_size = train_size,\n",
    "                            ppm_windows = ppm_windows,\n",
    "                            noise_threshes = noise_threshes,\n",
    "                            noise_names = noise_names,\n",
    "                            centroid_tolerance_vals = centroid_tolerance_vals,\n",
    "                            centroid_tolerance_types = centroid_tolerance_types,\n",
    "                            reweight_methods = reweight_methods,\n",
    "                            reweight_names = reweight_names,\n",
    "                            sim_methods = comparison_metrics,\n",
    "                            prec_removes = prec_removes,\n",
    "                            prec_remove_names = prec_remove_names\n",
    "                            )\n",
    "\n",
    "del(query_)\n",
    "del(target_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in second bases and shuffle order\n",
    "query_ = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/third_query.pkl')\n",
    "query_ = query_.sample(frac=1)\n",
    "\n",
    "target_=pd.read_pickle(target)\n",
    "if self_search:\n",
    "    target_.insert(0,'queryID', [i for i in range(len(target_))])\n",
    "else:\n",
    "    target_.insert(0,'queryID', [\"*\" for i in range(len(target_))])\n",
    "\n",
    "datasetBuilder.create_matches_and_model_data(query_,\n",
    "                              target_,\n",
    "                            matchesOutputPath = f'{outputs_path}/intermediateOutputs/splitMatches/test',\n",
    "                            modelDataOutputPath = f'{outputs_path}/intermediateOutputs/datasets/test',\n",
    "                            chunk_size = chunk_size,\n",
    "                            max_size = train_size,\n",
    "                            ppm_windows = ppm_windows,\n",
    "                            noise_threshes = noise_threshes,\n",
    "                            noise_names = noise_names,\n",
    "                            centroid_tolerance_vals = centroid_tolerance_vals,\n",
    "                            centroid_tolerance_types = centroid_tolerance_types,\n",
    "                            reweight_methods = reweight_methods,\n",
    "                            reweight_names = reweight_names,\n",
    "                            sim_methods = comparison_metrics,\n",
    "                            prec_removes = prec_removes,\n",
    "                            prec_remove_names = prec_remove_names\n",
    "                            )\n",
    "\n",
    "del(query_)\n",
    "del(target_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create indices to pull for interesting metric combos, instantiate GBC models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_indices = datasetBuilder.generate_keep_indices(noise_threshes=[True for i in range(len(noise_threshes))],\n",
    "                                                centroid_tolerance_vals = [True for i in range(len(centroid_tolerance_vals))],\n",
    "                                                reweight_methods = [True for i in range(len(reweight_methods))],\n",
    "                                                sim_methods = [True for i in range(len(sim_methods))],\n",
    "                                                prec_removes = [True for i in range(len(prec_removes))],\n",
    "                                                spec_features=[False for i in range(6)])\n",
    "\n",
    "\n",
    "#get all unique pairs and unique triplets by metric\n",
    "unique_pairs = list()\n",
    "for j in range(len(comparison_metrics)):\n",
    "    for k in range(len(comparison_metrics)):\n",
    "\n",
    "        if j>k:\n",
    "            unique_pairs.append([j,k])\n",
    "\n",
    "unique_triplets = list()\n",
    "for j in range(len(comparison_metrics)):\n",
    "    for k in unique_pairs:\n",
    "\n",
    "        if j not in k:\n",
    "            unique_triplets.append([j]+k)                  \n",
    "\n",
    "for i in ppm_windows:\n",
    "\n",
    "    chunks=list()\n",
    "    #catch case where we run out of chunks to combine\n",
    "    for j in range(num_chunks):\n",
    "        try:\n",
    "            chunk = pd.read_pickle(f'{outputs_path}/intermediateOutputs/datasets/train/{i}_ppm/chunk_{j+1}.pkl')\n",
    "            chunk = chunk.iloc[:,sim_indices]\n",
    "            chunks.append(chunk)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    train = pd.concat(chunks)\n",
    "    del(chunk)\n",
    "    del(chunks)\n",
    "\n",
    "    models = [\n",
    "            hgbc(),\n",
    "            hgbc(learning_rate=0.5),\n",
    "            hgbc(max_iter=200),\n",
    "            hgbc(learning_rate=0.01,min_samples_leaf=10),\n",
    "            hgbc(max_iter=200,min_samples_leaf=10),\n",
    "            hgbc(learning_rate=0.5, max_iter=200,min_samples_leaf=10),\n",
    "            ]\n",
    "    \n",
    "    num_condition = 10\n",
    "    num_control = 20\n",
    "\n",
    "    indices = dict()\n",
    "    corrs = dict()\n",
    "    indices['all-sims'] = list(range(train.shape[1]-1))\n",
    "    indices['all-mults'] = list()\n",
    "    indices['all-ents'] = list()\n",
    "    indices['all-difs'] = list()\n",
    "\n",
    "    low_corr_3, rand_corr_3 = testUtils.get_least_corr_and_control(train.iloc[:,:-1],\n",
    "                                                                   3, \n",
    "                                                                   num_condition = num_condition,\n",
    "                                                                   num_control = num_control)\n",
    "    \n",
    "    for _ in range(num_condition):\n",
    "        indices[f'low-corr-3-all_{_}'] = low_corr_3[0][_]\n",
    "        corrs[f'low-corr-3-all_{_}'] = low_corr_3[1][_]\n",
    "    for _ in range(num_control):\n",
    "        indices[f'rand-3-all_{_}'] = rand_corr_3[0][_]\n",
    "        corrs[f'rand-3-all_{_}'] = rand_corr_3[1][_]\n",
    "    print('generated 3')\n",
    "\n",
    "    low_corr_5, rand_corr_5 = testUtils.get_least_corr_and_control(train.iloc[:,:-1],\n",
    "                                                                   5, \n",
    "                                                                   num_condition = num_condition,\n",
    "                                                                   num_control = num_control)\n",
    "    \n",
    "    for _ in range(num_condition):\n",
    "        indices[f'low-corr-5-all_{_}'] = low_corr_5[0][_]\n",
    "        corrs[f'low-corr-5-all_{_}'] = low_corr_5[1][_]\n",
    "    for _ in range(num_control):\n",
    "        indices[f'rand-5-all_{_}'] = rand_corr_5[0][_]\n",
    "        corrs[f'rand-5-all_{_}'] = rand_corr_5[1][_]\n",
    "    print('generated 5')\n",
    "\n",
    "    low_corr_10, rand_corr_10 = testUtils.get_least_corr_and_control(train.iloc[:,:-1],\n",
    "                                                                   10, \n",
    "                                                                   num_condition = num_condition,\n",
    "                                                                   num_control = num_control)\n",
    "    \n",
    "    for _ in range(num_condition):\n",
    "        indices[f'low-corr-10-all_{_}'] = low_corr_10[0][_]\n",
    "        corrs[f'low-corr-10-all_{_}'] = low_corr_10[1][_]\n",
    "    for _ in range(num_control):\n",
    "        indices[f'rand-10-all_{_}'] = rand_corr_10[0][_]\n",
    "        corrs[f'rand-10-all_{_}'] = rand_corr_10[1][_]\n",
    "    print('generated 10')\n",
    "\n",
    "    low_corr_15, rand_corr_15 = testUtils.get_least_corr_and_control(train.iloc[:,:-1],\n",
    "                                                                   15, \n",
    "                                                                   num_condition = num_condition,\n",
    "                                                                   num_control = num_control)\n",
    "    \n",
    "    for _ in range(num_condition):\n",
    "        indices[f'low-corr-15-all_{_}'] = low_corr_15[0][_]\n",
    "        corrs[f'low-corr-15-all_{_}'] = low_corr_15[1][_]\n",
    "    for _ in range(num_control):\n",
    "        indices[f'rand-15-all_{_}'] = rand_corr_15[0][_]\n",
    "        corrs[f'rand-15-all_{_}'] = rand_corr_15[1][_]\n",
    "    print('generated 15')\n",
    "\n",
    "    low_corr_20, rand_corr_20 = testUtils.get_least_corr_and_control(train.iloc[:,:-1],\n",
    "                                                                   20, \n",
    "                                                                   num_condition = num_condition,\n",
    "                                                                   num_control = num_control)\n",
    "    for _ in range(num_condition):\n",
    "        indices[f'low-corr-20-all_{_}'] = low_corr_20[0][_]\n",
    "        corrs[f'low-corr-20-all_{_}'] = low_corr_20[1][_]\n",
    "    for _ in range(num_control):\n",
    "        indices[f'rand-20-all_{_}'] = rand_corr_20[0][_]\n",
    "        corrs[f'rand-20-all_{_}'] = rand_corr_20[1][_]\n",
    "    print('generated 20')\n",
    "\n",
    "    num_condition = 5\n",
    "    num_control = 5\n",
    "    for i in range(int((train.shape[1]-1)/len(comparison_metrics))):\n",
    "\n",
    "        low_corr_3,rand_corr_3 = testUtils.get_least_corr_and_control(train.iloc[:,i*len(comparison_metrics):(i+1)*len(comparison_metrics)],3, num_condition=num_condition, num_control=num_control)\n",
    "        for _ in range(num_condition):\n",
    "            indices[f'low-corr-3-{i}_{_}'] = low_corr_3[0][_]+(i*len(comparison_metrics))\n",
    "            corrs[f'low-corr-3-{i}_{_}'] = low_corr_3[1][_]+(i*len(comparison_metrics))\n",
    "        for _ in range(num_control):\n",
    "            indices[f'rand-3-{i}_{_}'] = rand_corr_3[0][_]+(i*len(comparison_metrics))\n",
    "            corrs[f'rand-3-{i}_{_}'] = rand_corr_3[1][_]+(i*len(comparison_metrics))\n",
    "\n",
    "        low_corr_5,rand_corr_5= testUtils.get_least_corr_and_control(train.iloc[:,i*len(comparison_metrics):(i+1)*len(comparison_metrics)],5, num_condition=num_condition, num_control=num_control)\n",
    "        \n",
    "        for _ in range(num_condition):\n",
    "            indices[f'low-corr-5-{i}_{_}'] = low_corr_5[0][_]+(i*len(comparison_metrics))\n",
    "            corrs[f'low-corr-5-{i}_{_}'] = low_corr_5[1][_]+(i*len(comparison_metrics))\n",
    "        for _ in range(num_control):\n",
    "            indices[f'rand-5-{i}_{_}'] = rand_corr_5[0][_]+(i*len(comparison_metrics))\n",
    "            corrs[f'rand-5-{i}_{_}'] = rand_corr_5[1][_]+(i*len(comparison_metrics))\n",
    "\n",
    "        indices[f'all-setting-{i}'] = list(np.array(range(len(comparison_metrics)))+(i*len(comparison_metrics)))\n",
    "        indices[f'mults-{i}'] = list(np.array([3,4,9,11])+(i*len(comparison_metrics)))\n",
    "        indices[f'difs-{i}'] = list(np.array([1,2,5,7,10])+(i*len(comparison_metrics)))\n",
    "\n",
    "        indices[f'all-mults'] = indices['all-mults'] + list(np.array([3,4,9,11])+(i*len(comparison_metrics)))\n",
    "        indices[f'all-ents'] = indices['all-ents'] + list(np.array([0])+(i*len(comparison_metrics)))\n",
    "        indices[f'all-difs'] = indices['all-difs'] + list(np.array([1,2,5,7,10])+(i*len(comparison_metrics)))\n",
    "\n",
    "        for _ in range(len(unique_pairs)):\n",
    "            indices[f'pair_{i}_{_}'] = np.array(unique_pairs[_])+(i*len(comparison_metrics))\n",
    "\n",
    "        for _ in range(len(unique_triplets)):\n",
    "            indices[f'triplet_{i}_{_}'] = np.array(unique_triplets[_])+(i*len(comparison_metrics))\n",
    "\n",
    "    print('finished creating indices')\n",
    "    \n",
    "    #now populate correlation dictionary with anything we don't already have\n",
    "    corr_matrix = train.corr()\n",
    "    for key, value in indices.items():\n",
    "\n",
    "        if key not in corrs:\n",
    "            \n",
    "            corr = 0\n",
    "            for i in value:\n",
    "                for j in value:\n",
    "\n",
    "                    if i>j:\n",
    "                        corr += corr_matrix.iloc[i,j]/math.comb(len(value),2)\n",
    "\n",
    "            corrs[key] = corr\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/gbc_res/custom_indices_{ppm_windows[0]}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "        pickle.dump(indices,handle)\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/gbc_res/mean_correlations_{ppm_windows[0]}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "        pickle.dump(corrs,handle)\n",
    "\n",
    "    print(f' total number of models: {len(models) * len(indices)}')\n",
    "    del(indices)\n",
    "    del(corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "breakpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train all models, collecting input aucs, their correlations, and their combined performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Models and Collect Train Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_name_models(train, models, indices):\n",
    "\n",
    "    trained_models = list()\n",
    "    for key, value in indices.items():\n",
    "\n",
    "        sub = train.iloc[:,value]\n",
    "        models_ = copy.deepcopy(models)\n",
    "\n",
    "        for i in range(len(models_)):\n",
    "\n",
    "            models_[i].fit(sub,train['match'])\n",
    "            trained_models[f'{key}_{i}'] = models_[i]\n",
    "\n",
    "    return trained_models\n",
    "\n",
    "def evaluate_models_by_subset(models, indices, eval_data):\n",
    "\n",
    "    model_aucs = list()\n",
    "    model_names = sorted(list(models.keys()))\n",
    "    for name in model_names:\n",
    "\n",
    "        subset_name = name.split('_')[0]\n",
    "\n",
    "        sub = eval_data.iloc[:,indices[subset_name]]\n",
    "        model = models[name]\n",
    "        pos_ind = np.where(model.classes_==1)[0][0]\n",
    "        model_aucs.append(auc(val['match'],model.predict_proba(sub)[:,pos_ind]))\n",
    "\n",
    "    return model_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in ppm_windows:\n",
    "\n",
    "    chunks=list()\n",
    "    #catch case where we run out of chunks to combine\n",
    "    labels = list()\n",
    "    for j in range(num_chunks):\n",
    "        try:\n",
    "            chunk = pd.read_pickle(f'{outputs_path}/intermediateOutputs/datasets/train/{i}_ppm/chunk_{j+1}.pkl')\n",
    "            chunk = chunk.iloc[:,sim_indices]\n",
    "            labels = labels + chunk[label_field].tolist()\n",
    "            chunks.append(chunk)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    train = pd.concat(chunks)\n",
    "    train['match'] = labels\n",
    "    del(chunk)\n",
    "    del(chunks)\n",
    "     \n",
    "    trained_models = train_and_name_models(train, models, indices)\n",
    "    names = sorted(list(trained_models.keys()))\n",
    "    train_aucs = evaluate_models_by_subset(trained_models, indices, train)\n",
    "    del(train)\n",
    "\n",
    "    chunks=list()\n",
    "    #catch case where we run out of chunks to combine\n",
    "    labels = list()\n",
    "    for j in range(num_chunks):\n",
    "        try:\n",
    "            chunk = pd.read_pickle(f'{outputs_path}/intermediateOutputs/datasets/val/{i}_ppm/chunk_{j+1}.pkl')\n",
    "            chunk = chunk.iloc[:,sim_indices]\n",
    "            labels = labels + chunk[label_field].tolist()\n",
    "            chunks.append(chunk)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    val = pd.concat(chunks)\n",
    "    val['match'] = labels\n",
    "    del(chunk)\n",
    "    del(chunks)\n",
    "\n",
    "    val_aucs = evaluate_models_by_subset(trained_models, indices, val)\n",
    "    del(val)\n",
    "\n",
    "    chunks=list()\n",
    "    #catch case where we run out of chunks to combine\n",
    "    labels = list()\n",
    "    for j in range(num_chunks):\n",
    "        try:\n",
    "            chunk = pd.read_pickle(f'{outputs_path}/intermediateOutputs/datasets/test/{i}_ppm/chunk_{j+1}.pkl')\n",
    "            chunk = chunk.iloc[:,sim_indices]\n",
    "            labels = labels + chunk[label_field].tolist()\n",
    "            chunks.append(chunk)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    test = pd.concat(chunks)\n",
    "    test['match'] = labels\n",
    "    del(chunk)\n",
    "    del(chunks)\n",
    "\n",
    "    test_aucs = evaluate_models_by_subset(trained_models, indices, test)\n",
    "    del(val)\n",
    "\n",
    "    model_aucs = pd.DataFrame([names, train_aucs, val_aucs, test_aucs], columns=['name','train','val','test'])\n",
    "    \n",
    "    pd.to_csv(f'{outputs_path}/gbc_res/model_aucs_{window}_ppm.csv')\n",
    "    del(model_aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Functions to original metrics, evaluate how far off we are on test data with original normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inits = {'a' : 1,\n",
    "        'b': 1,\n",
    "        'c' : 1,\n",
    "        'd' : -1,\n",
    "        'e' : 1,\n",
    "        'f' : 1,\n",
    "        'g' :1,\n",
    "        'h' :0,\n",
    "        'i' : -1,\n",
    "        'j' :1,\n",
    "        'k' : 1,\n",
    "        'l' : 1,\n",
    "        'm' : 1,\n",
    "        'n' : -1,\n",
    "        'o' : 1,\n",
    "        'p' : 1,\n",
    "        'q' : 1,\n",
    "        'r' : 0,\n",
    "        's' : 1,\n",
    "        't' : -1,\n",
    "        'u' : 1,\n",
    "        'v' : -1,\n",
    "        'w' : 1,\n",
    "        'x' : 1,\n",
    "        'y' : 1,\n",
    "        'z' : 1,\n",
    "        'a_' : 1,\n",
    "        'b_' : -1,\n",
    "        'c_' : 1,\n",
    "        'd_' : -1,\n",
    "        'e_' : 1,\n",
    "        'f_' : 1,\n",
    "        'g_' : 1,\n",
    "        'h_' : 1,\n",
    "        'i_' : 1,\n",
    "        'j_' : -1,\n",
    "        'k_' : 1,\n",
    "        'l_': -1,\n",
    "        'm_' : 1,\n",
    "        'n_' : 1,\n",
    "        'o_' : 1,\n",
    "        'p_' : 1,\n",
    "        'q_' : -1,\n",
    "        'r_' : 1,\n",
    "        's_' : -1,\n",
    "        't_' : 1,\n",
    "        'u_' : 1,\n",
    "        'v_' : 1,\n",
    "        'w_' : -1,\n",
    "        'x_' : 1,\n",
    "        'y_' : -1,\n",
    "        'z_':1}\n",
    "\n",
    "\n",
    "fit_funcs = {\n",
    "    'entropy-1':(['f','g','i','n_','p_'],None),\n",
    "    'entropy-2':(['f','g','h','i','j','n_','o_','p_','q_','r_','s_'],None),\n",
    "    'entropy-3':(['f','g','h','i','j','n_','o_','p_','q_','r_','s_','b','l','x','y','z','a_','b_','c_','d_','e_'],None),\n",
    "    'lorentzian-1':(['a','b'],None),\n",
    "    'lorentzian-2':(['a','b','c','d','e'],None),\n",
    "    'lorentzian-3':(['a','b','c','d','e','f','g','h','i','j','k','n_','o_','p_','q_','r_','s_'],None),\n",
    "    'dot_product-1':(['k','l','t_','u_'],None),\n",
    "    'dot_product-2':(['k','l','m','n','o','t_','u_','v_','w_','x_','y_'],None),\n",
    "    'dot_product-3':(['k','l','m','n','o','t_','u_','v_','w_','x_','y_','k','n_','o_','p_','q_','r_','s_'],None),\n",
    "    'harmonic_mean-1':(['x','y','z','a_','b_','c_'],None),\n",
    "    'harmonic_mean-2':(['b','l','x','y','z','a_','b_','c_','d_','e_','b','l'],None),\n",
    "    'harmonic_mean-3':(['b','l','x','y','z','a_','b_','c_','d_','e_','b','l','n_','o_','p_','q_','r_','s_'],None),\n",
    "    'fidelity-1':(['k','l','m'],None),\n",
    "    'fidelity-2':(['k','l','m','n','o',],None),\n",
    "    'fidelity-3':(['k','l','m','n','o','n_','o_','p_','q_','r_','s_'],None),\n",
    "    'squared_chord-1':(['a','b'],None),\n",
    "    'squared_chord-2':(['a','b','c','d','e'],None),\n",
    "    'squared_chord-3':(['a','b','c','d','e','f','g','h','i','j','k','n_','o_','p_','q_','r_','s_'],None),\n",
    "    'bhattacharya_1-1':(['a','b'],None),\n",
    "    'bhattacharya_1-2':(['a','b','c','d','e'],None),\n",
    "    'bhattacharya_1-3':(['a','b','c','d','e','f','g','h','i','j','k','n_','o_','p_','q_','r_','s_'],None),\n",
    "}\n",
    "\n",
    "reload(testUtils)\n",
    "reload(func_ob)\n",
    "reload(TunaSims)\n",
    "\n",
    "train_reses = list()\n",
    "test_reses = list()\n",
    "for i in ppm_windows:\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/datasets/train_unnorm_dist_{i}_ppm.pkl', 'rb') as handle:\n",
    "\n",
    "        train_labels = pickle.load(handle)\n",
    "\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_dif_ce_{i}_ppm.pkl', 'rb') as handle:\n",
    "\n",
    "        train_specs = pickle.load(handle)\n",
    "\n",
    "    #just focus on first setting for now\n",
    "    train_labels = train_labels.iloc[:,:len(comparison_metrics)]\n",
    "    train_specs = train_specs.iloc[:,[0,1,2,-2]]\n",
    "    train_specs.columns=['mzs','query','target','precursor']\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/datasets/test_unnorm_dist_{i}_ppm.pkl', 'rb') as handle:\n",
    "\n",
    "        test_labels = pickle.load(handle)\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/splitMatches/test_cleaned_matches_dif_ce_{i}_ppm.pkl', 'rb') as handle:\n",
    "\n",
    "        test_specs = pickle.load(handle)\n",
    "\n",
    "    test_labels = test_labels.iloc[:,:len(comparison_metrics)]\n",
    "    test_specs = test_specs.iloc[:,[0,1,2,-2]]\n",
    "    test_specs.columns=['mzs','query','target','precursor']\n",
    "\n",
    "    squared_loss = lambda x: (x)**2\n",
    "    lin_loss = lambda x: np.abs(x)\n",
    "    l1_reg = lambda l,x: l*np.sum(np.abs(x))\n",
    "    l2_reg = lambda l,x: l*np.sqrt(np.sum(x**2))\n",
    "    no_reg = lambda x: 0\n",
    "\n",
    "    reg_funcs = [no_reg,partial(l2_reg,0.01),partial(l1_reg,0.01)]\n",
    "    reg_names = ['none','l2_0.01','l1_0.01']\n",
    "    losses = [squared_loss]\n",
    "    loss_names = ['squared']\n",
    "    momentums = ['none','simple']\n",
    "    mom_weights = [[0.2,0.8]]\n",
    "    lambdas = [0.01,0.001]\n",
    "    max_iters = [1e5]\n",
    "\n",
    "    funcs = testUtils.create_all_funcs_stoch(reg_funcs=reg_funcs,\n",
    "                                        reg_names=reg_names,\n",
    "                                        losses=losses,\n",
    "                                        loss_names=loss_names,\n",
    "                                        momentums=momentums,\n",
    "                                        inits = inits,\n",
    "                                        params=fit_funcs,\n",
    "                                        mom_weights=mom_weights,\n",
    "                                        lambdas=lambdas,\n",
    "                                        max_iters=max_iters,\n",
    "                                        func = TunaSims.tuna_combo_distance_demo)\n",
    "    \n",
    "    print(f'total number of functions : {len(funcs)}')\n",
    "    trained=list()\n",
    "    for func in funcs:\n",
    "\n",
    "        name = func.name.split('-')[0]\n",
    "        train_specs['match'] = train_labels[name]\n",
    "\n",
    "        func.fit(train_specs)\n",
    "        trained.append(func)\n",
    "        print(func.name)\n",
    "\n",
    "    #get train and test errors under proper normalization protocol\n",
    "    trained_res=list()\n",
    "    test_res=list()\n",
    "    names=list()\n",
    "reload(testUtils)\n",
    "for i in ppm_windows:\n",
    "    for func in trained:\n",
    "\n",
    "        #generate proper train and test datasets\n",
    "        name = func.name.split('-')[0]\n",
    "        train_specs['match'] = train_labels[name]\n",
    "        test_specs['match'] = test_labels[name]\n",
    "\n",
    "        #get trained_func\n",
    "        pred_func = func.trained_func()\n",
    "\n",
    "        trained_res.append(testUtils.get_func_dist(train_specs, pred_func, name))\n",
    "        test_res.append(testUtils.get_func_dist(test_specs, pred_func, name))\n",
    "        names.append(f'{name}_{func.regularization_name}_{func.momentum_type}_{func.momentum_weights}')\n",
    "        print(f'{name}_{func.regularization_name}_{func.momentum_type}_{func.momentum_weights}')\n",
    "        \n",
    "\n",
    "    trained_res = pd.DataFrame(trained_res).transpose()\n",
    "    trained_res.columns  = names\n",
    "\n",
    "    test_res = pd.DataFrame(test_res).transpose()\n",
    "    test_res.columns  = names\n",
    "\n",
    "    train_reses.append(trained_res)\n",
    "    test_reses.append(test_res)\n",
    "\n",
    "with open(f'{outputs_path}/intermediateOutputs/train_to_func/trained_reses_{i}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "    pickle.dump(train_reses, handle)\n",
    "\n",
    "with open(f'{outputs_path}/intermediateOutputs/train_to_func/test_reses_{i}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "    pickle.dump(test_reses, handle)\n",
    "\n",
    "del(train_reses)\n",
    "del(test_reses)\n",
    "del(train_labels)\n",
    "del(test_labels)\n",
    "del(train_specs)\n",
    "del(test_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{outputs_path}/intermediateOutputs/datasets/train_unnorm_dist_3_ppm.pkl', 'rb') as handle:\n",
    "\n",
    "    train_labels = pickle.load(handle)\n",
    "\n",
    "with open(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_dif_ce_3_ppm.pkl', 'rb') as handle:\n",
    "\n",
    "    train_specs = pickle.load(handle)\n",
    "\n",
    "comparison_metrics = ['entropy',\n",
    "                'manhattan',\n",
    "                'lorentzian',\n",
    "                'dot_product',\n",
    "                'fidelity',\n",
    "                'matusita',\n",
    "                'chi2',\n",
    "                'laplacian',\n",
    "                'harmonic_mean',\n",
    "                'bhattacharya_1',\n",
    "                'squared_chord',\n",
    "                'cross_ent']\n",
    "\n",
    "#just focus on first setting for now\n",
    "train_labels = train_labels.iloc[:,:len(comparison_metrics)]\n",
    "train_specs = train_specs.iloc[:,[0,1,2,-2]]\n",
    "train_specs.columns=['mzs','query','target','precursor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Distance Functions by Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadk = False\n",
    "if quadk:\n",
    "    flats = {\n",
    "            'fdif_quadk':(['a','b','c','d','e'],None),\n",
    "            'fadd_quadk':(['f','g','h','i','j'],None),\n",
    "            'fmult_quadk':(['k','l','m','n','o'],None),\n",
    "    }\n",
    "\n",
    "    exts = {'edif_add':(['b','g','p','q','r','s','t','u','v','w'],None),\n",
    "            'edif_mult':(['b','l','x','y','z','a_','b_','c_','d_','e_'],None),\n",
    "            'emult_add':(['l','g','f_','g_','h_','i_','j_','k_','l_','m_'],None),      \n",
    "    }\n",
    "\n",
    "    params = dict()\n",
    "    seen =set()\n",
    "    for key in flats.keys():\n",
    "        for key_ in flats.keys():\n",
    "\n",
    "            feature_type = key.split('_')[0]\n",
    "            feature_type_ = key_.split('_')[0]\n",
    "\n",
    "            func_type = key.split('_')[1]\n",
    "            func_type_ = key_.split('_')[1]\n",
    "\n",
    "            try:\n",
    "                bounds_type = key.split('_')[2]\n",
    "                bounds_type_ = key_.split('_')[2]\n",
    "            except:\n",
    "                bounds_type = ''\n",
    "                bounds_type_ = ''\n",
    "\n",
    "            if f'{key_}_{key}' in params.keys():\n",
    "                continue\n",
    "            params[f'{key}_{key_}']=(sorted(list(set(flats[key][0]+flats[key_][0]))),testUtils.dict_combine(flats[key][1],flats[key_][1]))\n",
    "            \n",
    "    params_ = dict()\n",
    "    seen =set()\n",
    "    for key in exts.keys():\n",
    "        for key_ in exts.keys():\n",
    "\n",
    "            feature_type = key.split('_')[0]\n",
    "            feature_type_ = key_.split('_')[0]\n",
    "\n",
    "            func_type = key.split('_')[1]\n",
    "            func_type_ = key_.split('_')[1]\n",
    "\n",
    "            try:\n",
    "                bounds_type = key.split('_')[2]\n",
    "                bounds_type_ = key_.split('_')[2]\n",
    "            except:\n",
    "                bounds_type = ''\n",
    "                bounds_type_ = ''\n",
    "\n",
    "            if f'{key_}_{key}' in params_.keys():\n",
    "                continue\n",
    "            params_[f'{key}_{key_}']=(sorted(list(set(exts[key][0]+exts[key_][0]))),testUtils.dict_combine(exts[key][1],exts[key_][1]))\n",
    "\n",
    "    params.update(params_)   \n",
    "\n",
    "    params['all_flat_quadk']= (['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o'],None)\n",
    "    params['all_ext_quadk'] = (['b','l','g','p','q','r','s','t','u','v','w','x','y','z','a_','b_','c_','d_','e_','f_','g_','h_','i_','j_','k_','l_','m_','t_','u_','v_','w_','x_','y_'],None)\n",
    "\n",
    "    for key in list(params_.keys())[:5]:\n",
    "        params[f'{key}_normed_add']=(sorted(list(set(params_[key][0]+['n_','o_','p_','q_','r_','s_']))),testUtils.dict_combine(params_[key][1],None))\n",
    "        params[f'{key}_normed_mult']=(sorted(list(set(params_[key][0]+['t_','u_','v_','w_','x_','y_']))),testUtils.dict_combine(params_[key][1],None))\n",
    "\n",
    "    params['norm_only_add']=(['n_','o_','p_','q_','r_','s_'],None)\n",
    "    params['norm_only_mult']=(['t_','u_','v_','w_','x_','y_'],None)\n",
    "\n",
    "quad=True\n",
    "if quad:\n",
    "    flats = {\n",
    "            'fdif_quad':(['a','b','c'],None),\n",
    "            'fadd_quad':(['f','g','h'],None),\n",
    "            'fmult_quad':(['k','l','m'],None),\n",
    "    }\n",
    "\n",
    "    exts = {'edif_add_quad':(['b','g','p','q','r','s','t','u'],None),\n",
    "            'edif_mult_quad':(['b','l','x','y','z','a_','b_','c_'],None),\n",
    "            'emult_add_quad':(['l','g','f_','g_','h_','i_','j_','k_'],None),      \n",
    "    }\n",
    "\n",
    "    params2 = dict()\n",
    "    seen =set()\n",
    "    for key in flats.keys():\n",
    "        for key_ in flats.keys():\n",
    "\n",
    "            feature_type = key.split('_')[0]\n",
    "            feature_type_ = key_.split('_')[0]\n",
    "\n",
    "            func_type = key.split('_')[1]\n",
    "            func_type_ = key_.split('_')[1]\n",
    "\n",
    "            try:\n",
    "                bounds_type = key.split('_')[2]\n",
    "                bounds_type_ = key_.split('_')[2]\n",
    "            except:\n",
    "                bounds_type = ''\n",
    "                bounds_type_ = ''\n",
    "\n",
    "            if f'{key_}_{key}' in params2.keys():\n",
    "                continue\n",
    "            params2[f'{key}_{key_}']=(sorted(list(set(flats[key][0]+flats[key_][0]))),testUtils.dict_combine(flats[key][1],flats[key_][1]))\n",
    "            \n",
    "    params2_ = dict()\n",
    "    seen =set()\n",
    "    for key in exts.keys():\n",
    "        for key_ in exts.keys():\n",
    "\n",
    "            feature_type = key.split('_')[0]\n",
    "            feature_type_ = key_.split('_')[0]\n",
    "\n",
    "            func_type = key.split('_')[1]\n",
    "            func_type_ = key_.split('_')[1]\n",
    "\n",
    "            try:\n",
    "                bounds_type = key.split('_')[2]\n",
    "                bounds_type_ = key_.split('_')[2]\n",
    "            except:\n",
    "                bounds_type = ''\n",
    "                bounds_type_ = ''\n",
    "\n",
    "            if f'{key_}_{key}' in params2_.keys():\n",
    "                continue\n",
    "            params2_[f'{key}_{key_}']=(sorted(list(set(exts[key][0]+exts[key_][0]))),testUtils.dict_combine(exts[key][1],exts[key_][1]))\n",
    "\n",
    "    params2['all_flat_quad']= (['a','b','c','f','g','h','k','l','m'],None)\n",
    "    params2['all_ext_quad'] = (['b','l','g','p','q','r','s','t','u','x','y','z','a_','b_','c_','f_','g_','h_','i_','j_','k_'],None)\n",
    "\n",
    "\n",
    "    for key in list(params2_.keys())[:5]:\n",
    "        params2[f'{key}_normed_add']=(sorted(list(set(params2_[key][0]+['n_','o_','p_','s_']))),testUtils.dict_combine(params2_[key][1],None))\n",
    "        params2[f'{key}_normed_mult']=(sorted(list(set(params2_[key][0]+['t_','u_','v_','w_','x_','y_']))),testUtils.dict_combine(params2_[key][1],None))\n",
    "\n",
    "    params2.update(params2_) \n",
    "    #params.update(params2)  \n",
    "\n",
    "    for key in list(params2.keys())[:10]:\n",
    "        params[f'{key}_sigtune']=(params[key][0]+['z_'],None)\n",
    "\n",
    "    # for key in list(params2.keys())[:10]:\n",
    "    #     params[f'{key}_with_mz']=(params[key][0]+['z_'],None)\n",
    "\n",
    "    reload(func_ob)\n",
    "    reload(TunaSims)\n",
    "    reload(testUtils)\n",
    "    #helper lambda funcs\n",
    "    squared_loss = lambda x: (1-x)**2\n",
    "    lin_loss = lambda x: np.abs(1-x)\n",
    "    l1_reg = lambda l,x: l*np.sum(np.abs(x))\n",
    "    l2_reg = lambda l,x: l*np.sqrt(np.sum(x**2))\n",
    "    no_reg = lambda x: 0\n",
    "\n",
    "    reg_funcs = [no_reg,partial(l2_reg,0.01),partial(l2_reg,0.1)]\n",
    "    reg_names = ['none_none','l2_0.01','l2_0.1']\n",
    "    losses = [squared_loss]\n",
    "    loss_names = ['squared']\n",
    "    momentums = ['none']\n",
    "    mom_weights = [[0.2,0.8]]\n",
    "    lambdas = [0.01]\n",
    "    max_iters = [1e4]\n",
    "\n",
    "    funcs_same = testUtils.create_all_funcs_stoch(reg_funcs=reg_funcs,\n",
    "                                        reg_names=reg_names,\n",
    "                                        losses=losses,\n",
    "                                        loss_names=loss_names,\n",
    "                                        momentums=momentums,\n",
    "                                        params=params2,\n",
    "                                        inits=inits,\n",
    "                                        mom_weights=mom_weights,\n",
    "                                        lambdas=lambdas,\n",
    "                                        max_iters=max_iters,\n",
    "                                        func = TunaSims.tuna_combo_distance)\n",
    "\n",
    "    funcs_dif = testUtils.create_all_funcs_stoch(reg_funcs=reg_funcs,\n",
    "                                        reg_names=reg_names,\n",
    "                                        losses=losses,\n",
    "                                        loss_names=loss_names,\n",
    "                                        momentums=momentums,\n",
    "                                        params=params2,\n",
    "                                        inits=inits,\n",
    "                                        mom_weights=mom_weights,\n",
    "                                        lambdas=lambdas,\n",
    "                                        max_iters=max_iters,\n",
    "                                        func = TunaSims.tuna_combo_distance)\n",
    "\n",
    "    all_funcs_ = [funcs_same, funcs_dif]\n",
    "\n",
    "    print(f'number of specifications: {len(funcs_same)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in ppm_windows:\n",
    "\n",
    "    trained_dict = dict()\n",
    "    all_funcs = copy.deepcopy(all_funcs_)\n",
    "\n",
    "    sub_train_same_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_same_ce_{window}_ppm.pkl')\n",
    "    sub_train_dif_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_dif_ce_{window}_ppm.pkl')\n",
    "    \n",
    "    train_datasets = [sub_train_same_ce, sub_train_dif_ce]\n",
    "    dataset_names = ['same_ce','dif_ce']\n",
    "\n",
    "    settings = 1\n",
    "\n",
    "    for _ in range(len(train_datasets)):\n",
    "        \n",
    "        for j in range(settings):\n",
    "\n",
    "            sub = train_datasets[_].iloc[:,3*j:3*(j+1)]\n",
    "            sub.columns=['mzs','query','target']\n",
    "            sub['precursor'] = train_datasets[_]['precursor']\n",
    "            sub['match'] = train_datasets[_]['match']\n",
    "        \n",
    "            trained=list()\n",
    "            for i in range(len(all_funcs[_])):\n",
    "                \n",
    "                all_funcs[_][i].fit(sub)\n",
    "                trained.append(all_funcs[_][i])\n",
    "                if (i+1)%10==0:\n",
    "                    print(f'trained {i+1} functions on {dataset_names[_]}_{j}')\n",
    "\n",
    "\n",
    "            trained_dict[f'{dataset_names[_]}_{j}'] = trained\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/train_to_error/trained_dict_{window}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "        pickle.dump(trained_dict, handle)\n",
    "        del(trained_dict)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in ppm_windows:\n",
    "    \n",
    "    sub_train_same_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_same_ce_{window}_ppm.pkl')\n",
    "    sub_train_dif_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_dif_ce_{window}_ppm.pkl')\n",
    "    \n",
    "    train_datasets = [sub_train_same_ce, sub_train_dif_ce]\n",
    "    dataset_names = ['same_ce','dif_ce']\n",
    "\n",
    "    settings=1\n",
    "\n",
    "    trained_res=None\n",
    "    for _ in range(len(train_datasets)):\n",
    "        for j in range(settings):\n",
    "\n",
    "            #grab trained models for this portion of dataframe\n",
    "            sub = train_datasets[_].iloc[:,3*j:3*(j+1)]\n",
    "            sub.columns=['mzs','query','target']\n",
    "            sub['precursor'] = train_datasets[_]['precursor']\n",
    "            sub['match'] = train_datasets[_]['match']\n",
    "\n",
    "            small = testUtils.trained_res_to_df(models,sub)\n",
    "            small.insert(1,'settings', f'{dataset_names[_]}_{j}')\n",
    "            trained_res=pd.concat((trained_res,small))\n",
    "            print(f'completed {dataset_names[_]}_{j}')\n",
    "\n",
    "    print('generated train results')\n",
    "    del(sub_train_dif_ce)\n",
    "    del(sub_train_same_ce)\n",
    "\n",
    "    sub_val_same_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/val_cleaned_matches_same_ce_{window}_ppm.pkl')\n",
    "    sub_val_dif_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/val_cleaned_matches_dif_ce_{window}_ppm.pkl')\n",
    "    val_datasets = [sub_val_same_ce, sub_val_dif_ce]\n",
    "\n",
    "    val_aucs=list()\n",
    "    for _ in range(len(val_datasets)):\n",
    "        for j in range(settings):\n",
    "\n",
    "            #grab trained models for this portion of dataframe\n",
    "            models = trained_dict[f'{dataset_names[_]}_{j}']\n",
    "            sub = val_datasets[_].iloc[:,3*j:3*(j+1)]\n",
    "            sub.columns=['mzs','query','target']\n",
    "            sub['precursor'] = val_datasets[_]['precursor']\n",
    "            sub['match'] = val_datasets[_]['match']\n",
    "            val_aucs = val_aucs + testUtils.trained_res_to_df(models,sub)['auc'].tolist()\n",
    "            print(f'completed {dataset_names[_]}_{j}')\n",
    "\n",
    "    trained_res['val']=val_aucs\n",
    "    print('generated val results')\n",
    "\n",
    "    del(sub_val_dif_ce)\n",
    "    del(sub_val_same_ce)\n",
    "\n",
    "    sub_test_same_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/test_cleaned_matches_same_ce_{window}_ppm.pkl')\n",
    "    sub_test_dif_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/test_cleaned_matches_dif_ce_{window}_ppm.pkl')\n",
    "    test_datasets = [sub_test_same_ce, sub_test_dif_ce]\n",
    "    \n",
    "    test_aucs=list()\n",
    "    for _ in range(len(test_datasets)):\n",
    "        for j in range(settings):\n",
    "\n",
    "            #grab trained models for this portion of dataframe\n",
    "            models = trained_dict[f'{dataset_names[_]}_{j}']\n",
    "            sub = test_datasets[_].iloc[:,3*j:3*(j+1)]\n",
    "            sub.columns=['mzs','query','target']\n",
    "            sub['precursor'] = test_datasets[_]['precursor']\n",
    "            sub['match'] = test_datasets[_]['match'].tolist()\n",
    "            test_aucs = test_aucs + testUtils.trained_res_to_df(models,sub)['auc'].tolist()\n",
    "            print(f'completed {dataset_names[_]}_{j}')\n",
    "\n",
    "    trained_res['test']=test_aucs\n",
    "    print('generated test results')\n",
    "\n",
    "    del(sub_test_dif_ce)\n",
    "    del(sub_test_same_ce)\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/train_to_error/trained_res_{window}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "        pickle.dump(trained_res, handle)\n",
    "        del(trained_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions: \n",
    "\n",
    "add offsets for terms\n",
    "\n",
    "num of params not appearing to change train time much\n",
    "\n",
    "consider replacing knockouts with sigmoids\n",
    "\n",
    "consider tuning final sigmoid\n",
    "\n",
    "should features like length,entropy be included in the similarity, or be used outside as extra feature in learned mod.both? neither?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Ideas:\n",
    "\n",
    "Accuracy (In order of increasing difficulty):\n",
    "\n",
    "-Incorporate as feature how many possible chem structures (can also restrict to NPS) exist within a certain precursor distance. (violating golden rules or not)\n",
    "\n",
    "-include original NIST version or theoretical res as feature\n",
    "\n",
    "-Weight different ranges of spec differently for matches (more diversity/greater accuracy)\n",
    "\n",
    "-smush together top n results over different inchicores and come up with combined model predicting over individual inchicores\n",
    "\n",
    "-diagnostic ion/loss classing as a feature...do they match\n",
    "\n",
    "-kernelized smooth match\n",
    "\n",
    "-3d struct guesses...do they match (cores, but can generalize to 3d)\n",
    "\n",
    "Speed(In order of increasing difficulty):\n",
    "\n",
    "-combine sim metrics and expand(apply func to df)\n",
    "\n",
    "-exclude matches based on non-similarity features to cut down on needed comparisons\n",
    "\n",
    "-ion tables to upper bound similarity\n",
    "\n",
    "-only use one peak consolidation and matching protocol...then only do reweight transformations on already matched peaks for spec and sim features\n",
    "\n",
    "-can missing peaks in lower energy be explained by frags and losses from higher energy? incorporate into model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order to proceed:\n",
    "\n",
    "-recreate databases with coll energy included (standardized format across DBs)\n",
    "\n",
    "-what proportion of matches are the same coll energy?\n",
    "\n",
    "-quantify variability in peak appearance vs peak intensity across collision energies\n",
    "    -does this relate in a predictable way to fragment mass\n",
    "\n",
    "-test sim metrics for same coll energy vs not same col energy (is the same inductive bias useful)\n",
    "\n",
    "-Show that regular funcs are in the space of combo distance\n",
    "\n",
    "-test combining individual metrics that use different components of the 2 vectors (add, mult, dif)\n",
    "\n",
    "-range over individual metrics in combined score in attempt to explain why combining them is successful\n",
    "\n",
    "-train combo metrics with flattened components and individual (should these sims be broken out?)\n",
    "    -should we do this for same coll energy vs dif energies\n",
    "\n",
    "-are different combo metrics put into larger model more successful than the combined individual metrics\n",
    "\n",
    "-can tunasims be fit with nonlinearities between the components (flattened or not?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
