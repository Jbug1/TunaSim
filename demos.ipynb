{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from importlib import reload\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "import copy\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier as hgbc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import TunaSims\n",
    "import func_ob\n",
    "import tools\n",
    "import datasetBuilder\n",
    "import testUtils\n",
    "import spectral_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for Different Ways of Distributing Interspectral Intensity Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs=list()\n",
    "scores_1=list()\n",
    "scores_2=list()\n",
    "\n",
    "total_difference = 0.9\n",
    "len_difference = 10\n",
    "max_len = 25\n",
    "\n",
    "func1 = partial(TunaSims.tuna_dif_distance,f=1,g=1,h=2)\n",
    "func2 = partial(TunaSims.tuna_dif_distance,f=1,g=1,h=2)\n",
    "\n",
    "normalize = False\n",
    "\n",
    "for i in range(1,max_len):\n",
    "\n",
    "    xs.append(i)\n",
    "    dif_1 = np.array([1/(x+1) for x in range(i)])\n",
    "    dif_1 = dif_1/sum(dif_1)*total_difference\n",
    "\n",
    "    dif_2 = np.array([total_difference/i for x in range(i)])\n",
    "\n",
    "    if normalize:\n",
    "        scores_1.append(1- 1/func1(dif_1))\n",
    "        scores_2.append(1 - 1/func2(dif_2))\n",
    "    else:\n",
    "        scores_1.append(func1(dif_1, np.zeros(len(dif_1))))\n",
    "        scores_2.append(func2(dif_2, np.zeros(len(dif_2))))\n",
    "\n",
    "plt.plot(xs, scores_1, label='descending')\n",
    "plt.plot(xs, scores_2, label='unfiorm')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#databases\n",
    "outputs_path='/Users/jonahpoczobutt/projects/TunaRes/testy'\n",
    "nist14='/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist14_highres.pkl'\n",
    "nist20_prot_deprot = '/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist20_prot_deprot.pkl'\n",
    "nist23_hr_prot_deprot_only = '/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist23_prot_deprot_only.pkl'\n",
    "nist23_hr_full ='/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist23_full.pkl'\n",
    "gnps='/Users/jonahpoczobutt/projects/raw_data/db_csvs/gnps_highres.pkl'\n",
    "mona='/Users/jonahpoczobutt/projects/raw_data/db_csvs/mona_highres.pkl'\n",
    "metlin='/Users/jonahpoczobutt/projects/raw_data/db_csvs/metlin_highres_inst.pkl'\n",
    "mona_nist = '/Users/jonahpoczobutt/projects/raw_data/db_csvs/mona_nist_prot_only.pkl'\n",
    "\n",
    "self_search=False\n",
    "query = metlin\n",
    "target = nist23_hr_full\n",
    "if self_search:\n",
    "    target=query\n",
    "    \n",
    "fullRun=True\n",
    "if fullRun:\n",
    "    os.mkdir(outputs_path)\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullRun=True\n",
    "if fullRun:\n",
    "\n",
    "    #This should be replaced with a function to read in all the databases\n",
    "    query_ = pd.read_pickle(query)\n",
    "    all_bases = list(set(query_['inchi_base']))\n",
    "\n",
    "    if self_search:\n",
    "        query_.insert(0,'queryID', [i for i in range(len(query_))])\n",
    "    else:\n",
    "        query_.insert(0,'queryID', [\"_\" for i in range(len(query_))])\n",
    "\n",
    "    #this method is in place\n",
    "    np.random.shuffle(all_bases)\n",
    "\n",
    "    first_bases = all_bases[:int(len(all_bases)*0.5)]\n",
    "    second_bases = all_bases[int(len(all_bases)*0.5):int(len(all_bases)*0.7)]\n",
    "    third_bases = all_bases[int(len(all_bases)*0.7):]\n",
    "\n",
    "    first_query_ = query_[np.isin(query_['inchi_base'],first_bases)]\n",
    "    first_query_.reset_index(inplace=True)\n",
    "    first_query_.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/first_query.pkl')\n",
    "    del(first_query_)\n",
    "\n",
    "    second_query_ = query_[np.isin(query_['inchi_base'],second_bases)]\n",
    "    second_query_.reset_index(inplace=True)\n",
    "    second_query_.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/second_query.pkl')\n",
    "    del(second_query_)\n",
    "\n",
    "    third_query_ = query_[np.isin(query_['inchi_base'],third_bases)]\n",
    "    third_query_.reset_index(inplace=True)\n",
    "    third_query_.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/third_query.pkl')\n",
    "    del(third_query_)\n",
    "    del(query_)\n",
    "\n",
    "    \n",
    "    np.save(f'{outputs_path}/intermediateOutputs/splitMatches/first_bases.npy',first_bases)\n",
    "    np.save(f'{outputs_path}/intermediateOutputs/splitMatches/second_bases.npy',second_bases)\n",
    "    np.save(f'{outputs_path}/intermediateOutputs/splitMatches/third_bases.npy',third_bases)\n",
    "    del(first_bases)\n",
    "    del(second_bases)\n",
    "    del(third_bases)\n",
    "    del(all_bases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarity methods and transformation parameters below. Leave sim methods as None to run all\n",
    "reload(datasetBuilder)\n",
    "reload(tools)\n",
    "\n",
    "comparison_metrics = ['entropy',\n",
    "             'manhattan',\n",
    "             'lorentzian',\n",
    "             'dot_product',\n",
    "             'fidelity',\n",
    "             'matusita',\n",
    "             'chi2',\n",
    "             'laplacian',\n",
    "             'harmonic_mean',\n",
    "             'bhattacharya_1',\n",
    "             'squared_chord',\n",
    "             'cross_ent'\n",
    "    ]\n",
    "\n",
    "ppm_windows = [10]\n",
    "noise_threshes=[0.01,0.0]\n",
    "centroid_tolerance_vals = [0.05]\n",
    "centroid_tolerance_types=['da']\n",
    "powers=['orig',1]\n",
    "sim_methods=comparison_metrics\n",
    "prec_removes=[True]\n",
    "build_dataset=True\n",
    "\n",
    "\n",
    "train_size=3e6\n",
    "test_size=1e6\n",
    "test_size=2e6\n",
    "\n",
    "max_matches=None\n",
    "adduct_match = False\n",
    "\n",
    "target_=pd.read_pickle(target)\n",
    "\n",
    "if self_search:\n",
    "    target_.insert(0,'queryID', [i for i in range(len(target_))])\n",
    "else:\n",
    "    target_.insert(0,'queryID', [\"*\" for i in range(len(target_))])\n",
    "\n",
    "for i in ppm_windows:\n",
    "\n",
    "    if build_dataset:\n",
    "\n",
    "        #read in first bases and shuffle order\n",
    "        query_train = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/first_query.pkl')\n",
    "        query_train=query_train.sample(frac=1)\n",
    "\n",
    "        #create matches for model to train on\n",
    "        matches = datasetBuilder.create_matches_df(query_train,target_,i,max_matches,train_size, adduct_match)\n",
    "        matches.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_matches_{i}_ppm.pkl')\n",
    "        del(query_train)\n",
    "\n",
    "        matches_same_ce = matches[matches['ceratio']==1]\n",
    "        matches_dif_ce = matches[matches['ceratio']!=1]\n",
    "        \n",
    "        sub_train_same_ce = datasetBuilder.create_cleaned_df(\n",
    "                                            matches_same_ce, \n",
    "                                            sim_methods, \n",
    "                                            noise_threshes, \n",
    "                                            centroid_tolerance_vals, \n",
    "                                            centroid_tolerance_types,\n",
    "                                            powers,\n",
    "                                            prec_removes\n",
    "        )\n",
    "\n",
    "        sub_train_dif_ce = datasetBuilder.create_cleaned_df(\n",
    "                                            matches_dif_ce, \n",
    "                                            sim_methods, \n",
    "                                            noise_threshes, \n",
    "                                            centroid_tolerance_vals, \n",
    "                                            centroid_tolerance_types,\n",
    "                                            powers,\n",
    "                                            prec_removes\n",
    "        )\n",
    "\n",
    "        sub_train_same_ce.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_same_ce_{i}_ppm.pkl')\n",
    "        sub_train_dif_ce.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_dif_ce_{i}_ppm.pkl')\n",
    "\n",
    "        #read in first bases and shuffle order\n",
    "        query_val = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/second_query.pkl')\n",
    "        query_query_val = query_val.sample(frac=1)\n",
    "\n",
    "        #create matches for model to train on\n",
    "        matches = datasetBuilder.create_matches_df(query_val,target_,i,max_matches,test_size, adduct_match)\n",
    "        matches.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/val_matches_{i}_ppm.pkl')\n",
    "        del(query_val)\n",
    "\n",
    "        \n",
    "        matches_same_ce = matches[matches['ceratio']==1]\n",
    "        matches_dif_ce = matches[matches['ceratio']!=1]\n",
    "        \n",
    "        sub_val_same_ce = datasetBuilder.create_cleaned_df(\n",
    "                                            matches_same_ce, \n",
    "                                            sim_methods, \n",
    "                                            noise_threshes, \n",
    "                                            centroid_tolerance_vals, \n",
    "                                            centroid_tolerance_types,\n",
    "                                            powers,\n",
    "                                            prec_removes\n",
    "        )\n",
    "\n",
    "        sub_val_dif_ce = datasetBuilder.create_cleaned_df(\n",
    "                                            matches_dif_ce, \n",
    "                                            sim_methods, \n",
    "                                            noise_threshes, \n",
    "                                            centroid_tolerance_vals, \n",
    "                                            centroid_tolerance_types,\n",
    "                                            powers,\n",
    "                                            prec_removes\n",
    "        )\n",
    "\n",
    "        sub_val_same_ce.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/val_cleaned_matches_same_ce_{i}_ppm.pkl')\n",
    "        sub_val_dif_ce.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/val_cleaned_matches_dif_ce_{i}_ppm.pkl')\n",
    "\n",
    "\n",
    "        #read in first bases and shuffle order\n",
    "        query_test = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/third_query.pkl')\n",
    "        query_test=query_test.sample(frac=1)\n",
    "\n",
    "        #create matches for model to train on\n",
    "        matches = datasetBuilder.create_matches_df(query_test,target_,i,max_matches,test_size, adduct_match)\n",
    "        matches.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/test_matches_{i}_ppm.pkl')\n",
    "        del(query_test)\n",
    "\n",
    "        matches_same_ce = matches[matches['ceratio']==1]\n",
    "        matches_dif_ce = matches[matches['ceratio']!=1]\n",
    "        \n",
    "        sub_test_same_ce = datasetBuilder.create_cleaned_df(\n",
    "                                            matches_same_ce, \n",
    "                                            sim_methods, \n",
    "                                            noise_threshes, \n",
    "                                            centroid_tolerance_vals, \n",
    "                                            centroid_tolerance_types,\n",
    "                                            powers,\n",
    "                                            prec_removes\n",
    "        )\n",
    "\n",
    "        sub_test_dif_ce = datasetBuilder.create_cleaned_df(\n",
    "                                            matches_dif_ce, \n",
    "                                            sim_methods, \n",
    "                                            noise_threshes, \n",
    "                                            centroid_tolerance_vals, \n",
    "                                            centroid_tolerance_types,\n",
    "                                            powers,\n",
    "                                            prec_removes\n",
    "        )\n",
    "\n",
    "        sub_test_same_ce.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/test_cleaned_matches_same_ce_{i}_ppm.pkl')\n",
    "        sub_test_dif_ce.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/test_cleaned_matches_dif_ce_{i}_ppm.pkl')\n",
    "\n",
    "    else:\n",
    "        sub_train_same_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_same_ce_{i}_ppm.pkl')\n",
    "        sub_val_same_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/val_cleaned_matches_same_ce_{i}_ppm.pkl')\n",
    "        sub_test_same_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/test_cleaned_matches_same_ce_{i}_ppm.pkl')\n",
    "\n",
    "        sub_train_dif_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_dif_ce_{i}_ppm.pkl')\n",
    "        sub_val_dif_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/val_cleaned_matches_dif_ce_{i}_ppm.pkl')\n",
    "        sub_test_dif_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/test_cleaned_matches_dif_ce_{i}_ppm.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Train/Val/Test Data & get Individual Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(testUtils)\n",
    "\n",
    "train_datasets = [sub_train_same_ce, sub_train_dif_ce]\n",
    "val_datasets = [sub_val_same_ce, sub_val_dif_ce]\n",
    "test_datasets = [sub_test_same_ce, sub_test_dif_ce]\n",
    "dataset_names = ['same_ce','dif_ce']\n",
    "# train_datasets = [pd.concat((sub_train_same_ce, sub_train_dif_ce)).sample(frac=1)]\n",
    "# dataset_names=['combined']\n",
    "\n",
    "gbc_train_datasets = list()\n",
    "ind_aucs_full = list()\n",
    "\n",
    "for metric in comparison_metrics:\n",
    "    for j in range(int(train_datasets[_].shape[1]/2)):\n",
    "\n",
    "        ind_aucs_full.append(f'{metric}_{j}')\n",
    "\n",
    "ind_aucs_full = pd.DataFrame(ind_aucs_full, columns=['metric'])\n",
    "\n",
    "for _ in range(len(train_datasets)):\n",
    "\n",
    "    ind_aucs_=None\n",
    "    train_data_gbcs = None\n",
    "    for j in range(int(train_datasets[_].shape[1]/2)):\n",
    "\n",
    "        sub = train_datasets[_].iloc[:,2*j:2*(j+1)]\n",
    "        old_cols = sub.columns\n",
    "        sub.columns=['query','target']\n",
    "        sub['match'] = train_datasets[_]['match'].tolist()\n",
    "\n",
    "        ind_aucs, inds = testUtils.orig_metric_to_df(comparison_metrics, sub)\n",
    "        ind_aucs_ = pd.concat((ind_aucs_, ind_aucs))\n",
    "        sub = sub.iloc[:,:2]\n",
    "        sub.columns=old_cols\n",
    "        train_data_gbcs = pd.concat((train_data_gbcs,inds), axis=1)\n",
    "\n",
    "    if _ ==0:    \n",
    "        ind_aucs_full['same_train']=ind_aucs_['AUC'].tolist()\n",
    "    else:\n",
    "        ind_aucs_full['dif_train']=ind_aucs_['AUC'].tolist()\n",
    "\n",
    "    train_data_gbcs['match'] = train_datasets[_]['match'].tolist()\n",
    "    gbc_train_datasets.append(train_data_gbcs)\n",
    "\n",
    "with open(f'{outputs_path}/intermediateOutputs/datasets/gbc_train.pkl', 'wb') as handle:\n",
    "\n",
    "    pickle.dump(gbc_train_datasets, handle)\n",
    "\n",
    "del(gbc_train_datasets)\n",
    "print('created train data')\n",
    "\n",
    "gbc_val_datasets = list()\n",
    "for _ in range(len(val_datasets)):\n",
    "\n",
    "    val_data_gbcs = None\n",
    "    ind_aucs_=None\n",
    "    for j in range(int(val_datasets[_].shape[1]/2)):\n",
    "\n",
    "        sub = val_datasets[_].iloc[:,2*j:2*(j+1)]\n",
    "        old_cols = sub.columns\n",
    "        sub.columns=['query','target']\n",
    "        sub['match'] = val_datasets[_]['match'].tolist()\n",
    "\n",
    "        ind_aucs, inds = testUtils.orig_metric_to_df(comparison_metrics, sub)\n",
    "        ind_aucs_ = pd.concat((ind_aucs_, ind_aucs))\n",
    "        sub = sub.iloc[:,:2]\n",
    "        sub.columns=old_cols\n",
    "        val_data_gbcs = pd.concat((val_data_gbcs,inds), axis=1)\n",
    "    \n",
    "    if _ ==0:    \n",
    "        ind_aucs_full['same_val']=ind_aucs_['AUC'].tolist()\n",
    "    else:\n",
    "        ind_aucs_full['dif_val']=ind_aucs_['AUC'].tolist()\n",
    "\n",
    "    val_data_gbcs['match'] = val_datasets[_]['match'].tolist()\n",
    "    gbc_val_datasets.append(val_data_gbcs)\n",
    "\n",
    "with open(f'{outputs_path}/intermediateOutputs/datasets/gbc_val.pkl', 'wb') as handle:\n",
    "\n",
    "    pickle.dump(gbc_val_datasets, handle)\n",
    "\n",
    "del(gbc_val_datasets)\n",
    "print('created val data')\n",
    "\n",
    "gbc_test_datasets = list()\n",
    "for _ in range(len(test_datasets)):\n",
    "\n",
    "    test_data_gbcs = None\n",
    "    ind_aucs_ = None\n",
    "    for j in range(int(test_datasets[_].shape[1]/2)):\n",
    "\n",
    "        sub = test_datasets[_].iloc[:,2*j:2*(j+1)]\n",
    "        old_cols = sub.columns\n",
    "        sub.columns=['query','target']\n",
    "        sub['match'] = test_datasets[_]['match'].tolist()\n",
    "\n",
    "        ind_aucs, inds = testUtils.orig_metric_to_df(comparison_metrics, sub)\n",
    "        ind_aucs['metric'] = [f'{x}_{j}' for x in ind_aucs['metric']]\n",
    "        ind_aucs_ = pd.concat((ind_aucs_, ind_aucs))\n",
    "        sub = sub.iloc[:,:2]\n",
    "        sub.columns=old_cols\n",
    "        test_data_gbcs = pd.concat((test_data_gbcs,inds), axis=1)\n",
    "    \n",
    "    if _ ==0:    \n",
    "        ind_aucs_full['same_test']=ind_aucs_['AUC'].tolist()\n",
    "    else:\n",
    "        ind_aucs_full['dif_test']=ind_aucs_['AUC'].tolist()\n",
    "\n",
    "    test_data_gbcs['match'] = test_datasets[_]['match'].tolist()\n",
    "    gbc_test_datasets.append(test_data_gbcs)\n",
    "\n",
    "with open(f'{outputs_path}/intermediateOutputs/datasets/gbc_test.pkl', 'wb') as handle:\n",
    "\n",
    "    pickle.dump(gbc_test_datasets, handle)\n",
    "\n",
    "del(gbc_test_datasets)\n",
    "print('created test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create indices to pull for each metric and those with same components, specify GBC models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "                hgbc(),\n",
    "                hgbc(learning_rate=0.5),\n",
    "                hgbc(max_iter=200),\n",
    "                hgbc(learning_rate=0.01,min_samples_leaf=10),\n",
    "                hgbc(max_iter=200,min_samples_leaf=10),\n",
    "                hgbc(learning_rate=0.5, max_iter=200,min_samples_leaf=10),\n",
    "                ]\n",
    "\n",
    "\n",
    "indices = dict()\n",
    "indices['all-sims'] = list(range(gbc_train_datasets[0].shape[1]-1))\n",
    "indices['all-mults'] = list()\n",
    "indices['all-ents'] = list()\n",
    "indices['all-difs'] = list()\n",
    "\n",
    "for i in range(int((gbc_train_datasets[0].shape[1]-1)/len(comparison_metrics))):\n",
    "\n",
    "    indices[f'setting-{i}-all'] = list(np.array(range(len(comparison_metrics)))+(i*len(comparison_metrics)))\n",
    "    indices[f'mults-{i}'] = list(np.array([3,4,9])+(i*len(comparison_metrics)))\n",
    "    indices[f'ents-{i}'] = list(np.array([0,11])+(i*len(comparison_metrics)))\n",
    "    indices[f'difs-{i}'] = list(np.array([1,2,5,7,10])+(i*len(comparison_metrics)))\n",
    "\n",
    "    indices[f'all-mults'] = indices['all-mults'] +list(np.array([3,4,9])+(i*len(comparison_metrics)))\n",
    "    indices[f'all-ents'] =  indices['all-ents'] + list(np.array([0,11])+(i*len(comparison_metrics)))\n",
    "    indices[f'all-difs'] =  indices['all-difs'] + list(np.array([1,2,5,7,10])+(i*len(comparison_metrics)))\n",
    "\n",
    "\n",
    "print(f' total number of models for each: {len(models) * len(indices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Models and Collect Train Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_train_model_aucs = list()\n",
    "model_names = list()\n",
    "dif_train_model_aucs = list()\n",
    "trained_models = dict()\n",
    "\n",
    "with open(f'{outputs_path}/intermediateOutputs/datasets/gbc_train.pkl', 'rb') as handle:\n",
    "\n",
    "    gbc_train_datasets = pickle.load(gbc_test_datasets, handle)\n",
    "\n",
    "for key, value in indices.items():\n",
    "\n",
    "    sub = gbc_train_datasets[0].iloc[:,value]\n",
    "    models_ = copy.deepcopy(models)\n",
    "\n",
    "    for i in range(len(models_)):\n",
    "\n",
    "        models_[i].fit(sub,gbc_train_datasets[0]['match'])\n",
    "        pos_ind = np.where(models_[i].classes_==1)[0][0]\n",
    "        same_train_model_aucs.append(auc(gbc_train_datasets[0]['match'],models_[i].predict_proba(sub)[:,pos_ind]))\n",
    "        model_names.append(f'{key}_{i}')\n",
    "        trained_models[f'same_{key}_{i}'] = models_[i]\n",
    "\n",
    "    sub = gbc_train_datasets[1].iloc[:,value]\n",
    "    models_ = copy.deepcopy(models)\n",
    "\n",
    "    for i in range(len(models_)):\n",
    "\n",
    "        models_[i].fit(sub,gbc_train_datasets[1]['match'])\n",
    "        pos_ind = np.where(models_[i].classes_==1)[0][0]\n",
    "        dif_train_model_aucs.append(auc(gbc_train_datasets[1]['match'],models_[i].predict_proba(sub)[:,pos_ind]))\n",
    "        trained_models[f'dif_{key}_{i}'] = models_[i]\n",
    "\n",
    "model_aucs = pd.DataFrame([model_names, same_train_model_aucs, dif_train_model_aucs]).transpose()\n",
    "model_aucs.columns = ['name','same_train','dif_train']\n",
    "\n",
    "del(gbc_train_datasets)\n",
    "\n",
    "with open(f'{outputs_path}/intermediateOutputs/datasets/gbc_val.pkl', 'rb') as handle:\n",
    "\n",
    "    gbc_val_datasets = pickle.load(gbc_test_datasets, handle)\n",
    "\n",
    "same_val = list()\n",
    "dif_val = list()\n",
    "same_test = list()\n",
    "dif_test = list()\n",
    "\n",
    "for name in model_aucs['name'].tolist():\n",
    "\n",
    "    subset_name = name.split('_')[0]\n",
    "\n",
    "    sub = gbc_val_datasets[0].iloc[:,indices[subset_name]]\n",
    "    model = trained_models[f'same_{name}']\n",
    "    pos_ind = np.where(model.classes_==1)[0][0]\n",
    "    same_val.append(auc(gbc_val_datasets[0]['match'],model.predict_proba(sub)[:,pos_ind]))\n",
    "\n",
    "    sub = gbc_val_datasets[1].iloc[:,indices[subset_name]]\n",
    "    model = trained_models[f'dif_{name}']\n",
    "    pos_ind = np.where(model.classes_==1)[0][0]\n",
    "    dif_val.append(auc(gbc_val_datasets[1]['match'],model.predict_proba(sub)[:,pos_ind]))\n",
    "\n",
    "del(gbc_val_datasets)\n",
    "\n",
    "with open(f'{outputs_path}/intermediateOutputs/datasets/gbc_test.pkl', 'rb') as handle:\n",
    "\n",
    "    gbc_test_datasets = pickle.load(gbc_test_datasets, handle)\n",
    "\n",
    "for name in model_aucs['name'].tolist():\n",
    "\n",
    "    subset_name = name.split('_')[0]\n",
    "\n",
    "    sub = gbc_test_datasets[0].iloc[:,indices[subset_name]]\n",
    "    model = trained_models[f'same_{name}']\n",
    "    pos_ind = np.where(model.classes_==1)[0][0]\n",
    "    same_test.append(auc(gbc_test_datasets[0]['match'],model.predict_proba(sub)[:,pos_ind]))\n",
    "\n",
    "    sub = gbc_test_datasets[1].iloc[:,indices[subset_name]]\n",
    "    model = trained_models[f'dif_{name}']\n",
    "    pos_ind = np.where(model.classes_==1)[0][0]\n",
    "    dif_test.append(auc(gbc_test_datasets[1]['match'],model.predict_proba(sub)[:,pos_ind]))\n",
    "\n",
    "del(gbc_test_datasets)\n",
    "\n",
    "model_aucs['same_val'] = same_val\n",
    "model_aucs['dif_val'] = dif_val\n",
    "model_aucs['same_test'] = same_test\n",
    "model_aucs['dif_test'] = dif_test\n",
    "\n",
    "model_aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Distance Functions by Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flats = {\n",
    "          'fdif_quadk':(['a','b','c','d','e'],None),\n",
    "          'fadd_quadk':(['f','g','h','i','j'],None),\n",
    "          'fmult_quadk':(['k','l','m','n','o'],None),\n",
    "}\n",
    "\n",
    "exts = {'edif_add':(['b','g','p','q','r','s','t','u','v','w'],None),\n",
    "        'edif_mult':(['b','l','x','y','z','a_','b_','c_','d_','e_'],None),\n",
    "        'emult_add':(['l','g','f_','g_','h_','i_','j_','k_','l_','m_'],None),      \n",
    "}\n",
    "\n",
    "params = dict()\n",
    "seen =set()\n",
    "for key in flats.keys():\n",
    "    for key_ in flats.keys():\n",
    "\n",
    "        feature_type = key.split('_')[0]\n",
    "        feature_type_ = key_.split('_')[0]\n",
    "\n",
    "        func_type = key.split('_')[1]\n",
    "        func_type_ = key_.split('_')[1]\n",
    "\n",
    "        try:\n",
    "            bounds_type = key.split('_')[2]\n",
    "            bounds_type_ = key_.split('_')[2]\n",
    "        except:\n",
    "            bounds_type = ''\n",
    "            bounds_type_ = ''\n",
    "\n",
    "\n",
    "        params[f'{key}_{key_}']=(sorted(list(set(flats[key][0]+flats[key_][0]))),testUtils.dict_combine(flats[key][1],flats[key_][1]))\n",
    "        \n",
    "params_ = dict()\n",
    "seen =set()\n",
    "for key in exts.keys():\n",
    "    for key_ in exts.keys():\n",
    "\n",
    "        feature_type = key.split('_')[0]\n",
    "        feature_type_ = key_.split('_')[0]\n",
    "\n",
    "        func_type = key.split('_')[1]\n",
    "        func_type_ = key_.split('_')[1]\n",
    "\n",
    "        try:\n",
    "            bounds_type = key.split('_')[2]\n",
    "            bounds_type_ = key_.split('_')[2]\n",
    "        except:\n",
    "            bounds_type = ''\n",
    "            bounds_type_ = ''\n",
    "\n",
    "\n",
    "        params_[f'{key}_{key_}']=(sorted(list(set(exts[key][0]+exts[key_][0]))),testUtils.dict_combine(exts[key][1],exts[key_][1]))\n",
    "\n",
    "params.update(params_)   \n",
    "\n",
    "params['all_flat']= (['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o'],None)\n",
    "params['all_ext'] = (['b','l','g','p','q','r','s','t','u','v','w','x','y','z','a_','b_','c_','d_','e_','f_','g_','h_','i_','j_','k_','l_','m_'],None)\n",
    "\n",
    "for key in params.keys():\n",
    "    params_[f'{key}_normed']=(sorted(list(set(params[key][0]+['n_','o_','p_','q_','r_','s_']))),testUtils.dict_combine(params[key][1],None))\n",
    "\n",
    "params['norm_only']=(['n_','o_','p_','q_','r_','s_'],None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create all Func Obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(func_ob)\n",
    "reload(TunaSims)\n",
    "reload(testUtils)\n",
    "#helper lambda funcs\n",
    "squared_loss = lambda x: (1-x)**2\n",
    "lin_loss = lambda x: abs(1-x)\n",
    "l1_reg = lambda l,x: l*np.sum(np.abs(x))\n",
    "l2_reg = lambda l,x: l*np.sqrt(np.sum(x**2))\n",
    "no_reg = lambda x: 0\n",
    "\n",
    "reg_funcs = [partial(l2_reg,.1),no_reg]\n",
    "reg_names = ['l2_0.1','none']\n",
    "losses = [squared_loss]\n",
    "loss_names = ['squared']\n",
    "momentums = ['none','simple','jonie']\n",
    "mom_weights = [[0.2,0.8]]\n",
    "lambdas = [0.01]\n",
    "max_iters = [1e4]\n",
    "\n",
    "funcs_same = testUtils.create_all_funcs_stoch(reg_funcs=reg_funcs,\n",
    "                                       reg_names=reg_names,\n",
    "                                       losses=losses,\n",
    "                                       loss_names=loss_names,\n",
    "                                       momentums=momentums,\n",
    "                                       params=params,\n",
    "                                       mom_weights=mom_weights,\n",
    "                                       lambdas=lambdas,\n",
    "                                       max_iters=max_iters,\n",
    "                                       func = TunaSims.tuna_combo_distance)\n",
    "\n",
    "funcs_dif = testUtils.create_all_funcs_stoch(reg_funcs=reg_funcs,\n",
    "                                       reg_names=reg_names,\n",
    "                                       losses=losses,\n",
    "                                       loss_names=loss_names,\n",
    "                                       momentums=momentums,\n",
    "                                       params=params,\n",
    "                                       mom_weights=mom_weights,\n",
    "                                       lambdas=lambdas,\n",
    "                                       max_iters=max_iters,\n",
    "                                       func = TunaSims.tuna_combo_distance)\n",
    "\n",
    "all_funcs = [funcs_same[:2], funcs_dif[:2]]\n",
    "\n",
    "print(f'number of specifications: {len(funcs_same)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_dict = dict()\n",
    "\n",
    "for _ in range(len(train_datasets)):\n",
    "    train_datasets[_]= train_datasets[_].sample(frac=1)\n",
    "    for j in range(int(train_datasets[_].shape[1]/2)):\n",
    "\n",
    "        sub = train_datasets[_].iloc[:,2*j:2*(j+1)]\n",
    "        sub.columns=['query','target']\n",
    "        sub['match'] = train_datasets[_]['match']\n",
    "    \n",
    "        trained=list()\n",
    "        for i in range(len(all_funcs[_])):\n",
    "            \n",
    "            all_funcs[_][i].fit(sub)\n",
    "            trained.append(all_funcs[_][i])\n",
    "            if i+1%10==0:\n",
    "                print(f'trained {i} functions on {dataset_names[_]}_{j}')\n",
    "\n",
    "        trained_dict[f'{dataset_names[_]}_{j}'] = trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datasets = [sub_test_same_ce, sub_test_dif_ce]\n",
    "#test_datasets = [pd.concat((sub_train_same_ce, sub_train_dif_ce)).sample(frac=1)]\n",
    "reload(func_ob)\n",
    "reload(TunaSims)\n",
    "reload(testUtils)\n",
    "\n",
    "trained_res=None\n",
    "for _ in range(len(train_datasets)):\n",
    "    for j in range(int(train_datasets[_].shape[1]/2)):\n",
    "\n",
    "        #grab trained models for this portion of dataframe\n",
    "        models = trained_dict[f'{dataset_names[_]}_{j}']\n",
    "        sub = test_datasets[_].iloc[:,2*j:2*(j+1)]\n",
    "        sub.columns=['query','target']\n",
    "        sub['match'] = test_datasets[_]['match'].tolist()\n",
    "\n",
    "        small = testUtils.trained_res_to_df(models,sub)\n",
    "        small.insert(1,'settings', f'{dataset_names[_]}_{j}')\n",
    "        trained_res=pd.concat((trained_res,small))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while sub.iloc[i]['match']==False:\n",
    "    i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0].trained_func()(sub.iloc[50]['query'],sub.iloc[50]['target'])==models[0].trained_func()(sub.iloc[500]['query'],sub.iloc[500]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = models[0].trained_func()(sub.iloc[50]['query'],sub.iloc[50]['target'])\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "a/sigmoid(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0].momentum_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0].trained_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions: \n",
    "\n",
    "add offsets for terms\n",
    "\n",
    "num of params not appearing to change train time much\n",
    "\n",
    "consider replacing knockouts with sigmoids\n",
    "\n",
    "consider tuning final sigmoid\n",
    "\n",
    "should features like length,entropy be included in the similarity, or be used outside as extra feature in learned mod.both? neither?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Ideas:\n",
    "\n",
    "Accuracy (In order of increasing difficulty):\n",
    "\n",
    "-Incorporate as feature how many possible chem structures (can also restrict to NPS) exist within a certain precursor distance. (violating golden rules or not)\n",
    "\n",
    "-include original NIST version or theoretical res as feature\n",
    "\n",
    "-Weight different ranges of spec differently for matches (more diversity/greater accuracy)\n",
    "\n",
    "-smush together top n results over different inchicores and come up with combined model predicting over individual inchicores\n",
    "\n",
    "-diagnostic ion/loss classing as a feature...do they match\n",
    "\n",
    "-kernelized smooth match\n",
    "\n",
    "-3d struct guesses...do they match (cores, but can generalize to 3d)\n",
    "\n",
    "Speed(In order of increasing difficulty):\n",
    "\n",
    "-combine sim metrics and expand(apply func to df)\n",
    "\n",
    "-exclude matches based on non-similarity features to cut down on needed comparisons\n",
    "\n",
    "-ion tables to upper bound similarity\n",
    "\n",
    "-only use one peak consolidation and matching protocol...then only do reweight transformations on already matched peaks for spec and sim features\n",
    "\n",
    "-can missing peaks in lower energy be explained by frags and losses from higher energy? incorporate into model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order to proceed:\n",
    "\n",
    "-recreate databases with coll energy included (standardized format across DBs)\n",
    "\n",
    "-what proportion of matches are the same coll energy?\n",
    "\n",
    "-quantify variability in peak appearance vs peak intensity across collision energies\n",
    "    -does this relate in a predictable way to fragment mass\n",
    "\n",
    "-test sim metrics for same coll energy vs not same col energy (is the same inductive bias useful)\n",
    "\n",
    "-Show that regular funcs are in the space of combo distance\n",
    "\n",
    "-test combining individual metrics that use different components of the 2 vectors (add, mult, dif)\n",
    "\n",
    "-range over individual metrics in combined score in attempt to explain why combining them is successful\n",
    "\n",
    "-train combo metrics with flattened components and individual (should these sims be broken out?)\n",
    "    -should we do this for same coll energy vs dif energies\n",
    "\n",
    "-are different combo metrics put into larger model more successful than the combined individual metrics\n",
    "\n",
    "-can tunasims be fit with nonlinearities between the components (flattened or not?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
