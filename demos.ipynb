{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from importlib import reload\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "import copy\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier as hgbc\n",
    "import pickle\n",
    "import warnings\n",
    "import math\n",
    "import copy\n",
    "import tests\n",
    "import figures\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.linear_model import Ridge\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import TunaSims\n",
    "import func_ob\n",
    "import tools\n",
    "import datasetBuilder\n",
    "import testUtils\n",
    "import spectral_similarity\n",
    "import itertools\n",
    "import reweightFuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nist14='/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist14_highres.pkl'\n",
    "nist20_prot = '/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist20_prot_fiehn_.pkl'\n",
    "nist20 = '/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist20.pkl'\n",
    "nist23_prot = '/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist23_prot_deprot_only.pkl'\n",
    "nist23='/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist23_full.pkl'\n",
    "gnps='/Users/jonahpoczobutt/projects/raw_data/db_csvs/gnps_highres.pkl'\n",
    "mona='/Users/jonahpoczobutt/projects/raw_data/db_csvs/mona_highres.pkl'\n",
    "metlin='/Users/jonahpoczobutt/projects/raw_data/db_csvs/metlin.pkl'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion Points\n",
    "\n",
    "Should we look at inchiKey for match rather than inchiCore\n",
    "\n",
    "does spectral entropy relate to precursor m/z - yes\n",
    "\n",
    "does spectral entropy relate to CE - yes\n",
    "\n",
    "does peak intensity relate to m/z - meh\n",
    "\n",
    "does peak intensity relate to CE\n",
    "\n",
    "What factors should play into reweighting\n",
    "\n",
    "    -for quality measure: precursor mz\n",
    "    -for reducing corr: fragment mz\n",
    "\n",
    "does having lower correlated similarity measures produce better results\n",
    "\n",
    "can we obtain lower correlation with the same cleaning procedure in order to be memory efficient (ie thru sim measures)\n",
    "\n",
    "Should we try sequential fitting on residuals?\n",
    "\n",
    "incorporate into model as feature the cleanliness of spectra by comparing the entropy of spectra from entire experiment vs their expectation from combined database. Higher entropy -> lower quality when adjusted properly. Inputs could be m/z as well as CE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create all Necessary Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#databases\n",
    "outputs_path='/Users/jonahpoczobutt/projects/TunaRes/metlin_metlin'\n",
    "\n",
    "self_search=True\n",
    "query = metlin\n",
    "target = metlin\n",
    "\n",
    "if query == target:\n",
    "    self_search = True\n",
    "    \n",
    "fullRun=True\n",
    "if fullRun:\n",
    "    os.mkdir(outputs_path)\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/gbcIndices')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches/train')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches/val')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches/test')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches/port')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/datasets')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/datasets/train')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/datasets/val')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/datasets/test')\n",
    "    os.mkdir(f'{outputs_path}/gbc_res')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/train_to_func')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/train_to_error')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splt Queries into Train, Val, Test by Core or Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullRun=True\n",
    "match_category = 'inchi_base'\n",
    "if fullRun:\n",
    "\n",
    "    #This should be replaced with a function to read in all the databases\n",
    "    query_ = pd.read_pickle(query)\n",
    "\n",
    "    #jonah edit here\n",
    "    all_bases = list(set(query_[match_category]))\n",
    "\n",
    "    if self_search:\n",
    "        query_.insert(0,'queryID', [i for i in range(len(query_))])\n",
    "    else:\n",
    "        query_.insert(0,'queryID', [\"_\" for i in range(len(query_))])\n",
    "\n",
    "    #this method is in place\n",
    "    np.random.shuffle(all_bases)\n",
    "\n",
    "    first_bases = all_bases[:int(len(all_bases))]\n",
    "    second_bases = all_bases[int(len(all_bases)*0.5):int(len(all_bases)*0.7)]\n",
    "    third_bases = all_bases[int(len(all_bases)*0.7):]\n",
    "\n",
    "    first_query_ = query_[np.isin(query_[match_category],first_bases)]\n",
    "    first_query_.reset_index(inplace=True)\n",
    "    first_query_.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/first_query.pkl')\n",
    "    del(first_query_)\n",
    "\n",
    "    second_query_ = query_[np.isin(query_[match_category],second_bases)]\n",
    "    second_query_.reset_index(inplace=True)\n",
    "    second_query_.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/second_query.pkl')\n",
    "    del(second_query_)\n",
    "\n",
    "    third_query_ = query_[np.isin(query_[match_category],third_bases)]\n",
    "    third_query_.reset_index(inplace=True)\n",
    "    third_query_.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/third_query.pkl')\n",
    "    del(third_query_)\n",
    "    del(query_)\n",
    "\n",
    "    np.save(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/first_bases.npy',first_bases)\n",
    "    np.save(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/second_bases.npy',second_bases)\n",
    "    np.save(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/third_bases.npy',third_bases)\n",
    "    del(first_bases)\n",
    "    del(second_bases)\n",
    "    del(third_bases)\n",
    "    del(all_bases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Parameters Here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1e5\n",
    "adduct_match = False\n",
    "strong_self_separation = True\n",
    "\n",
    "num_chunks=int(1e6) #number of chunks to be combined for calculating correlations and collecting testable indices\n",
    "\n",
    "label_field = 'InchiCoreMatch' # should be either inchicore or inchi\n",
    "\n",
    "comparison_metrics = ['entropy',\n",
    "                'manhattan',\n",
    "                'lorentzian',\n",
    "                'dot_product',\n",
    "                'fidelity',\n",
    "                'matusita',\n",
    "                'chi2',\n",
    "                'laplacian',\n",
    "                'harmonic_mean',\n",
    "                'bhattacharya_2',\n",
    "                'squared_chord',\n",
    "                'cross_ent'\n",
    "                ]\n",
    "\n",
    "ppm_windows = [10]\n",
    "noise_threshes=[partial(reweightFuncs.noise_clip, perc_thresh = 0.0),\n",
    "                partial(reweightFuncs.noise_clip, perc_thresh = 0.05),\n",
    "                partial(reweightFuncs.noise_clip, fixed_thresh = 10)]\n",
    "\n",
    "noise_names = ['None','5%','10']\n",
    "centroid_tolerance_vals = [0.05,0.01]\n",
    "centroid_tolerance_types=['da','da']\n",
    "reweight_methods = [partial(reweightFuncs.logent,intercept = 0.25), reweightFuncs.weight_intensity_by_entropy, partial(reweightFuncs.fixed_power,power=1)]\n",
    "reweight_names = ['logent','fiehn','1']\n",
    "sim_methods=comparison_metrics\n",
    "prec_removes=[lambda x: x-1.6, lambda x: None]\n",
    "prec_remove_names = ['fiehn', 'none']\n",
    "train_size=3e6\n",
    "val_size=1e6\n",
    "test_size=2e6\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1e5\n",
    "adduct_match = False\n",
    "strong_self_separation = True\n",
    "exclude_failed_cleaning = True\n",
    "\n",
    "num_chunks=int(1e6) #number of chunks to be combined for calculating correlations and collecting testable indices\n",
    "\n",
    "label_field = 'InchiCoreMatch' # should be either inchicore or inchi\n",
    "\n",
    "comparison_metrics = ['entropy',               \n",
    "                'dot_product'\n",
    "                ]\n",
    "\n",
    "ppm_windows = [10]\n",
    "noise_threshes=[partial(reweightFuncs.noise_clip, perc_thresh = 0.01)]\n",
    "\n",
    "noise_names = ['mer']\n",
    "centroid_tolerance_vals = [0.05]\n",
    "centroid_tolerance_types=['da']\n",
    "reweight_methods = [reweightFuncs.weight_intensity_by_entropy]\n",
    "reweight_names = ['fiehn']\n",
    "sim_methods=comparison_metrics\n",
    "prec_removes=[lambda x: x-1.6]\n",
    "prec_remove_names = ['fiehn']\n",
    "train_size=1e9\n",
    "val_size=1e6\n",
    "test_size=2e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create all Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ppm_windows:\n",
    "    try:\n",
    "        os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches/train/{i}_ppm')\n",
    "        os.mkdir(f'{outputs_path}/intermediateOutputs/datasets/train/{i}_ppm')\n",
    "        os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches/val/{i}_ppm')\n",
    "        os.mkdir(f'{outputs_path}/intermediateOutputs/datasets/val/{i}_ppm')\n",
    "        os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches/test/{i}_ppm')\n",
    "        os.mkdir(f'{outputs_path}/intermediateOutputs/datasets/test/{i}_ppm')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#read in first bases and shuffle order\n",
    "query_ = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/first_query.pkl')\n",
    "query_ = query_.sample(frac=1)\n",
    " \n",
    "if self_search:\n",
    "\n",
    "    if strong_self_separation:\n",
    "        target_ = query_\n",
    "    else:\n",
    "        target_=pd.read_pickle(target)\n",
    "        target_.insert(0,'queryID', [i for i in range(len(target_))])\n",
    "\n",
    "else:\n",
    "    target_=pd.read_pickle(target)\n",
    "    target_.insert(0,'queryID', [\"*\" for i in range(len(target_))])\n",
    "    \n",
    "\n",
    "datasetBuilder.create_matches_and_model_data(query_,\n",
    "                              target_,\n",
    "                            matchesOutputPath = f'{outputs_path}/intermediateOutputs/splitMatches/train',\n",
    "                            modelDataOutputPath = f'{outputs_path}/intermediateOutputs/datasets/train',\n",
    "                            chunk_size = chunk_size,\n",
    "                            max_size = train_size,\n",
    "                            ppm_windows = ppm_windows,\n",
    "                            noise_threshes = noise_threshes,\n",
    "                            noise_names = noise_names,\n",
    "                            centroid_tolerance_vals = centroid_tolerance_vals,\n",
    "                            centroid_tolerance_types = centroid_tolerance_types,\n",
    "                            reweight_methods = reweight_methods,\n",
    "                            reweight_names = reweight_names,\n",
    "                            sim_methods = comparison_metrics,\n",
    "                            prec_removes = prec_removes,\n",
    "                            prec_remove_names = prec_remove_names\n",
    "                            )\n",
    "\n",
    "del(query_)\n",
    "del(target_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual Scores Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created train\n"
     ]
    }
   ],
   "source": [
    "top_hit_sanity_check = True\n",
    "if top_hit_sanity_check:\n",
    "\n",
    "    for window in ppm_windows:\n",
    "        try:\n",
    "            os.mkdir(f'{outputs_path}/intermediateOutputs/datasets/checks')\n",
    "            os.mkdir(f'{outputs_path}/intermediateOutputs/datasets/checks/{window}_ppm')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "sim_indices = datasetBuilder.generate_keep_indices(noise_threshes=[True for i in range(len(noise_threshes))],\n",
    "                                                centroid_tolerance_vals = [True for i in range(len(centroid_tolerance_vals))],\n",
    "                                                reweight_methods = [True for i in range(len(reweight_methods))],\n",
    "                                                sim_methods = [True for i in range(len(sim_methods))],\n",
    "                                                prec_removes = [True for i in range(len(prec_removes))],\n",
    "                                                spec_features=[False for i in range(6)])\n",
    "for window in ppm_windows:\n",
    "\n",
    "    chunks=list()\n",
    "    match_chunks = list()\n",
    "    #catch case where we run out of chunks to combine\n",
    "    labels = list()\n",
    "    for j in range(num_chunks):\n",
    "        try:\n",
    "            chunk = pd.read_pickle(f'{outputs_path}/intermediateOutputs/datasets/train/{window}_ppm/chunk_{j+1}.pkl')\n",
    "            labels = labels + chunk[label_field].tolist()\n",
    "            chunk = chunk.iloc[:,sim_indices]\n",
    "            chunks.append(chunk)\n",
    "\n",
    "            matches_chunk = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train/{window}_ppm/chunk_{j+1}.pkl')\n",
    "            match_chunk = matches_chunk[['queryID','target_base']]\n",
    "            match_chunks.append(match_chunk)\n",
    "\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    print('created train')\n",
    "    train = pd.concat(chunks)\n",
    "    train_match = pd.concat(match_chunks)\n",
    "    train['queryID'] = train_match['queryID']\n",
    "    train['target_base'] = train_match['target_base']\n",
    "    train['match'] = labels\n",
    "    train.reset_index(inplace=True)\n",
    "    del(chunk)\n",
    "    del(chunks)\n",
    "    del(match_chunk)\n",
    "    del(match_chunks)\n",
    "    del(labels)\n",
    "\n",
    "    individual_scores = list()\n",
    "    if exclude_failed_cleaning:\n",
    "        train = train.replace(-1,np.nan).dropna(axis = 0, how = 'any')\n",
    "    for i in range(train.shape[1]-3):\n",
    "        temp = train.loc[train.groupby(by=['queryID','target_base'])[train.columns[i]].idxmax()]\n",
    "        individual_scores.append(auc(temp['match'], temp.iloc[:,i].tolist()))\n",
    "\n",
    "    individual_results = pd.DataFrame(train.columns[:-3], individual_scores)\n",
    "    individual_results.reset_index(inplace=True, drop=False)\n",
    "    individual_results.columns = ['auc','name']\n",
    "    individual_results.sort_values(by='auc', inplace=True, ascending=False)\n",
    "\n",
    "    individual_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.801056</td>\n",
       "      <td>entropy_n:mer c:0.05da p:fiehn, pr:fiehn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.792064</td>\n",
       "      <td>dot_product_n:mer c:0.05da p:fiehn, pr:fiehn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.615040</td>\n",
       "      <td>index</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        auc                                          name\n",
       "1  0.801056      entropy_n:mer c:0.05da p:fiehn, pr:fiehn\n",
       "2  0.792064  dot_product_n:mer c:0.05da p:fiehn, pr:fiehn\n",
       "0  0.615040                                         index"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "individual_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create all Val Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = False\n",
    "if val:\n",
    "    #read in second bases and shuffle order\n",
    "    query_ = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/second_query.pkl')\n",
    "    query_ = query_.sample(frac=1)\n",
    "\n",
    "    if self_search:\n",
    "\n",
    "        if strong_self_separation:\n",
    "            target_ = query_\n",
    "        else:\n",
    "            target_=pd.read_pickle(target)\n",
    "            target_.insert(0,'queryID', [i for i in range(len(target_))])\n",
    "\n",
    "    else:\n",
    "        target_=pd.read_pickle(target)\n",
    "        target_.insert(0,'queryID', [\"*\" for i in range(len(target_))])\n",
    "\n",
    "    datasetBuilder.create_matches_and_model_data(query_,\n",
    "                                target_,\n",
    "                                matchesOutputPath = f'{outputs_path}/intermediateOutputs/splitMatches/val',\n",
    "                                modelDataOutputPath = f'{outputs_path}/intermediateOutputs/datasets/val',\n",
    "                                chunk_size = chunk_size,\n",
    "                                max_size = val_size,\n",
    "                                ppm_windows = ppm_windows,\n",
    "                                noise_threshes = noise_threshes,\n",
    "                                noise_names = noise_names,\n",
    "                                centroid_tolerance_vals = centroid_tolerance_vals,\n",
    "                                centroid_tolerance_types = centroid_tolerance_types,\n",
    "                                reweight_methods = reweight_methods,\n",
    "                                reweight_names = reweight_names,\n",
    "                                sim_methods = comparison_metrics,\n",
    "                                prec_removes = prec_removes,\n",
    "                                prec_remove_names = prec_remove_names\n",
    "                                )\n",
    "\n",
    "    del(query_)\n",
    "    del(target_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in second bases and shuffle order\n",
    "query_ = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/inchisBySet/third_query.pkl')\n",
    "query_ = query_.sample(frac=1)\n",
    "\n",
    "if self_search:\n",
    "\n",
    "    if strong_self_separation:\n",
    "        target_ = query_\n",
    "    else:\n",
    "        target_=pd.read_pickle(target)\n",
    "        target_.insert(0,'queryID', [i for i in range(len(target_))])\n",
    "\n",
    "else:\n",
    "    target_=pd.read_pickle(target)\n",
    "    target_.insert(0,'queryID', [\"*\" for i in range(len(target_))])\n",
    "\n",
    "datasetBuilder.create_matches_and_model_data(query_,\n",
    "                              target_,\n",
    "                            matchesOutputPath = f'{outputs_path}/intermediateOutputs/splitMatches/test',\n",
    "                            modelDataOutputPath = f'{outputs_path}/intermediateOutputs/datasets/test',\n",
    "                            chunk_size = chunk_size,\n",
    "                            max_size = test_size,\n",
    "                            ppm_windows = ppm_windows,\n",
    "                            noise_threshes = noise_threshes,\n",
    "                            noise_names = noise_names,\n",
    "                            centroid_tolerance_vals = centroid_tolerance_vals,\n",
    "                            centroid_tolerance_types = centroid_tolerance_types,\n",
    "                            reweight_methods = reweight_methods,\n",
    "                            reweight_names = reweight_names,\n",
    "                            sim_methods = comparison_metrics,\n",
    "                            prec_removes = prec_removes,\n",
    "                            prec_remove_names = prec_remove_names\n",
    "                            )\n",
    "\n",
    "del(query_)\n",
    "del(target_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create indices to pull for interesting metric combos, instantiate GBC models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_indices = datasetBuilder.generate_keep_indices(noise_threshes=[True for i in range(len(noise_threshes))],\n",
    "                                                centroid_tolerance_vals = [True for i in range(len(centroid_tolerance_vals))],\n",
    "                                                reweight_methods = [True for i in range(len(reweight_methods))],\n",
    "                                                sim_methods = [True for i in range(len(sim_methods))],\n",
    "                                                prec_removes = [True for i in range(len(prec_removes))],\n",
    "                                                spec_features=[False for i in range(6)])\n",
    "\n",
    "\n",
    "#get n unique pairs and unique triplets by metric\n",
    "lim_3_metrics = 10\n",
    "combos_added = 0\n",
    "threes = list()\n",
    "for combo in itertools.combinations(range(len(sim_methods)),r=3):\n",
    "\n",
    "    threes.append(list(combo))\n",
    "    combos_added+=1\n",
    "\n",
    "    if combos_added == lim_3_metrics:\n",
    "        break\n",
    "\n",
    "\n",
    "lim_5_metrics = 10\n",
    "combos_added = 0\n",
    "fives = list()\n",
    "for combo in itertools.combinations(range(len(sim_methods)),r=5):\n",
    "\n",
    "    fives.append(list(combo))\n",
    "    combos_added+=1\n",
    "\n",
    "    if combos_added == lim_5_metrics:\n",
    "        break\n",
    "\n",
    "for i in ppm_windows:\n",
    "\n",
    "    chunks=list()\n",
    "    #catch case where we run out of chunks to combine\n",
    "    for j in range(num_chunks):\n",
    "        try:\n",
    "            chunk = pd.read_pickle(f'{outputs_path}/intermediateOutputs/datasets/train/{i}_ppm/chunk_{j+1}.pkl')\n",
    "            chunk = chunk.iloc[:,sim_indices]\n",
    "            chunks.append(chunk)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    train = pd.concat(chunks)\n",
    "    del(chunk)\n",
    "    del(chunks)\n",
    "\n",
    "    train = train.corr()\n",
    "    train.to_csv(f'{outputs_path}/intermediateOutputs/datasets/train/{i}_ppm/corrs.csv')\n",
    "\n",
    "    models = [\n",
    "            hgbc(),\n",
    "            # hgbc(learning_rate=0.5),\n",
    "            # hgbc(max_iter=200),\n",
    "            # hgbc(learning_rate=0.01,min_samples_leaf=10),\n",
    "            # hgbc(max_iter=200,min_samples_leaf=10),\n",
    "            # hgbc(learning_rate=0.5, max_iter=200,min_samples_leaf=10),\n",
    "            ]\n",
    "    \n",
    "    num_condition = 10\n",
    "    num_control = 10\n",
    "\n",
    "    indices = dict()\n",
    "    corrs = dict()\n",
    "    indices['all-sims'] = list(range(train.shape[1]))\n",
    "\n",
    "    low_corr_3, rand_corr_3, high_corr_3 = testUtils.get_corr_and_control(train,\n",
    "                                                                   3, \n",
    "                                                                   num_condition = num_condition,\n",
    "                                                                   num_control = num_control)\n",
    "    \n",
    "    for _ in range(num_condition):\n",
    "        indices[f'low-corr-3-all_{_}'] = low_corr_3[0][_]\n",
    "        corrs[f'low-corr-3-all_{_}'] = low_corr_3[1][_]\n",
    "    for _ in range(num_control):\n",
    "        indices[f'rand-3-all_{_}'] = rand_corr_3[0][_]\n",
    "        corrs[f'rand-3-all_{_}'] = rand_corr_3[1][_]\n",
    "    for _ in range(num_condition):\n",
    "        indices[f'high-3-all_{_}'] = high_corr_3[0][_]\n",
    "        corrs[f'high-3-all_{_}'] = high_corr_3[1][_]\n",
    "    \n",
    "    print('generated 3')\n",
    "\n",
    "    low_corr_5, rand_corr_5, high_corr_5 = testUtils.get_corr_and_control(train,\n",
    "                                                                   5, \n",
    "                                                                   num_condition = num_condition,\n",
    "                                                                   num_control = num_control)\n",
    "    \n",
    "    for _ in range(num_condition):\n",
    "        indices[f'low-corr-5-all_{_}'] = low_corr_5[0][_]\n",
    "        corrs[f'low-corr-5-all_{_}'] = low_corr_5[1][_]\n",
    "    for _ in range(num_control):\n",
    "        indices[f'rand-5-all_{_}'] = rand_corr_5[0][_]\n",
    "        corrs[f'rand-5-all_{_}'] = rand_corr_5[1][_]\n",
    "    for _ in range(num_condition):\n",
    "        indices[f'high-5-all_{_}'] = high_corr_5[0][_]\n",
    "        corrs[f'high-5-all_{_}'] = high_corr_5[1][_]\n",
    "    print('generated 5')\n",
    "\n",
    "    low_corr_10, rand_corr_10, high_corr_10 = testUtils.get_corr_and_control(train,\n",
    "                                                                   10, \n",
    "                                                                   num_condition = num_condition,\n",
    "                                                                   num_control = num_control)\n",
    "    \n",
    "    for _ in range(num_condition):\n",
    "        indices[f'low-corr-10-all_{_}'] = low_corr_10[0][_]\n",
    "        corrs[f'low-corr-10-all_{_}'] = low_corr_10[1][_]\n",
    "    for _ in range(num_control):\n",
    "        indices[f'rand-10-all_{_}'] = rand_corr_10[0][_]\n",
    "        corrs[f'rand-10-all_{_}'] = rand_corr_10[1][_]\n",
    "    for _ in range(num_condition):\n",
    "        indices[f'high-10-all_{_}'] = high_corr_10[0][_]\n",
    "        corrs[f'high-10-all_{_}'] = high_corr_10[1][_]\n",
    "    print('generated 10')\n",
    "\n",
    "    low_corr_15, rand_corr_15, high_corr_15 = testUtils.get_corr_and_control(train,\n",
    "                                                                   15, \n",
    "                                                                   num_condition = num_condition,\n",
    "                                                                   num_control = num_control)\n",
    "    \n",
    "    for _ in range(num_condition):\n",
    "        indices[f'low-corr-15-all_{_}'] = low_corr_15[0][_]\n",
    "        corrs[f'low-corr-15-all_{_}'] = low_corr_15[1][_]\n",
    "    for _ in range(num_control):\n",
    "        indices[f'rand-15-all_{_}'] = rand_corr_15[0][_]\n",
    "        corrs[f'rand-15-all_{_}'] = rand_corr_15[1][_]\n",
    "    for _ in range(num_condition):\n",
    "        indices[f'high-15-all_{_}'] = high_corr_15[0][_]\n",
    "        corrs[f'high-15-all_{_}'] = high_corr_15[1][_]\n",
    "    print('generated 15')\n",
    "\n",
    "    low_corr_20, rand_corr_20, high_corr_20 = testUtils.get_corr_and_control(train,\n",
    "                                                                   20, \n",
    "                                                                   num_condition = num_condition,\n",
    "                                                                   num_control = num_control)\n",
    "    for _ in range(num_condition):\n",
    "        indices[f'low-corr-20-all_{_}'] = low_corr_20[0][_]\n",
    "        corrs[f'low-corr-20-all_{_}'] = low_corr_20[1][_]\n",
    "    for _ in range(num_control):\n",
    "        indices[f'rand-20-all_{_}'] = rand_corr_20[0][_]\n",
    "        corrs[f'rand-20-all_{_}'] = rand_corr_20[1][_]\n",
    "    for _ in range(num_condition):\n",
    "        indices[f'high-20-all_{_}'] = high_corr_20[0][_]\n",
    "        corrs[f'high-20-all_{_}'] = high_corr_20[1][_]\n",
    "    print('generated 20')\n",
    "\n",
    "    num_condition = 3\n",
    "    num_control = 3\n",
    "    for i in range(int((train.shape[1])/len(comparison_metrics))):\n",
    "\n",
    "        low_corr_3,rand_corr_3, high_corr_3 = testUtils.get_corr_and_control(train.iloc[i*len(comparison_metrics):(i+1)*len(comparison_metrics),i*len(comparison_metrics):(i+1)*len(comparison_metrics)],3, num_condition=num_condition, num_control=num_control)\n",
    "        for _ in range(num_condition):\n",
    "            indices[f'low-corr-3-{i}_{_}'] = low_corr_3[0][_]+(i*len(comparison_metrics))\n",
    "            corrs[f'low-corr-3-{i}_{_}'] = low_corr_3[1][_]+(i*len(comparison_metrics))\n",
    "        for _ in range(num_control):\n",
    "            indices[f'rand-3-{i}_{_}'] = rand_corr_3[0][_]+(i*len(comparison_metrics))\n",
    "            corrs[f'rand-3-{i}_{_}'] = rand_corr_3[1][_]+(i*len(comparison_metrics))\n",
    "        for _ in range(num_control):\n",
    "            indices[f'high-3-{i}_{_}'] = high_corr_3[0][_]+(i*len(comparison_metrics))\n",
    "            corrs[f'high-3-{i}_{_}'] = high_corr_3[1][_]+(i*len(comparison_metrics))\n",
    "\n",
    "        low_corr_5,rand_corr_5, high_corr_5 = testUtils.get_corr_and_control(train.iloc[i*len(comparison_metrics):(i+1)*len(comparison_metrics),i*len(comparison_metrics):(i+1)*len(comparison_metrics)],5, num_condition=num_condition, num_control=num_control)\n",
    "        \n",
    "        for _ in range(num_condition):\n",
    "            indices[f'low-corr-5-{i}_{_}'] = low_corr_5[0][_]+(i*len(comparison_metrics))\n",
    "            corrs[f'low-corr-5-{i}_{_}'] = low_corr_5[1][_]+(i*len(comparison_metrics))\n",
    "        for _ in range(num_control):\n",
    "            indices[f'rand-5-{i}_{_}'] = rand_corr_5[0][_]+(i*len(comparison_metrics))\n",
    "            corrs[f'rand-5-{i}_{_}'] = rand_corr_5[1][_]+(i*len(comparison_metrics))\n",
    "        for _ in range(num_control):\n",
    "            indices[f'high-5-{i}_{_}'] = high_corr_5[0][_]+(i*len(comparison_metrics))\n",
    "            corrs[f'high-5-{i}_{_}'] = high_corr_5[1][_]+(i*len(comparison_metrics))\n",
    "\n",
    "        for _ in range(len(threes)):\n",
    "            indices[f'three_metric_{i}_{_}'] = np.array(threes[_])+(i*len(comparison_metrics))\n",
    "\n",
    "        for _ in range(len(fives)):\n",
    "            indices[f'five_metrics_{i}_{_}'] = np.array(fives[_])+(i*len(comparison_metrics))\n",
    "\n",
    "    print('finished creating indices')\n",
    "    \n",
    "    #now populate correlation dictionary with anything we don't already have\n",
    "    corr_matrix = train.corr()\n",
    "    for key, value in indices.items():\n",
    "\n",
    "        if key not in corrs:\n",
    "            \n",
    "            corr = 0\n",
    "            for i in value:\n",
    "                for j in value:\n",
    "\n",
    "                    if i>j:\n",
    "                        corr += corr_matrix.iloc[i,j]/math.comb(len(value),2)\n",
    "\n",
    "            corrs[key] = corr\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/gbcIndices/custom_indices_{ppm_windows[0]}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "        pickle.dump(indices,handle)\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/gbcIndices/mean_correlations_{ppm_windows[0]}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "        pickle.dump(corrs,handle)\n",
    "\n",
    "    print(f' total number of models: {len(models) * len(indices)}')\n",
    "    del(indices)\n",
    "    del(corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train all models, collecting input aucs, their correlations, and their train,val,test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{outputs_path}/intermediateOutputs/gbcIndices/custom_indices_{ppm_windows[0]}_ppm.pkl', 'rb') as handle:\n",
    "\n",
    "       indices = pickle.load(handle)\n",
    "   \n",
    "for window in ppm_windows:\n",
    "\n",
    "    chunks=list()\n",
    "    #catch case where we run out of chunks to combine\n",
    "    labels = list()\n",
    "    match_chunks = list()\n",
    "    for j in range(num_chunks):\n",
    "        try:\n",
    "            chunk = pd.read_pickle(f'{outputs_path}/intermediateOutputs/datasets/train/{window}_ppm/chunk_{j+1}.pkl')\n",
    "            labels = labels + chunk[label_field].tolist()\n",
    "            chunk = chunk.iloc[:,sim_indices]\n",
    "            chunks.append(chunk)\n",
    "\n",
    "            matches_chunk = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train/{window}_ppm/chunk_{j+1}.pkl')\n",
    "            match_chunk = matches_chunk[['queryID','target_base']]\n",
    "            match_chunks.append(match_chunk)\n",
    "\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    print('created train')\n",
    "    train = pd.concat(chunks)\n",
    "    train_match = pd.concat(match_chunks)\n",
    "    train['queryID'] = train_match['queryID']\n",
    "    train['target_base'] = train_match['target_base']\n",
    "    train['match'] = labels\n",
    "    train.reset_index(inplace=True)\n",
    "    del(chunk)\n",
    "    del(chunks)\n",
    "    del(match_chunk)\n",
    "    del(match_chunks)\n",
    "    del(labels)\n",
    "    \n",
    "    logpath = f'{outputs_path}/gbc_res/trainlog.txt'\n",
    "    trained_models = testUtils.train_and_name_models(train, models, indices, logpath = logpath)\n",
    "    names = sorted(list(trained_models.keys()))\n",
    "\n",
    "    logpath = f'{outputs_path}/gbc_res/evallog.txt'\n",
    "    train_aucs = testUtils.evaluate_models_by_subset(trained_models, indices, train, logpath = logpath)\n",
    "    del(train)\n",
    "\n",
    "    print('finished train')\n",
    "\n",
    "    # chunks=list()\n",
    "    # #catch case where we run out of chunks to combine\n",
    "    # labels = list()\n",
    "    # for j in range(num_chunks):\n",
    "    #     try:\n",
    "    #         chunk = pd.read_pickle(f'{outputs_path}/intermediateOutputs/datasets/val/{window}_ppm/chunk_{j+1}.pkl')\n",
    "    #         labels = labels + chunk[label_field].tolist()\n",
    "    #         chunk = chunk.iloc[:,sim_indices]\n",
    "    #         chunks.append(chunk)\n",
    "    #     except:\n",
    "    #         break\n",
    "\n",
    "    # val = pd.concat(chunks)\n",
    "    # val['match'] = labels\n",
    "    # del(chunk)\n",
    "    # del(chunks)\n",
    "\n",
    "    # val_aucs = testUtils.evaluate_models_by_subset(trained_models, indices, val, logpath = logpath)\n",
    "    # del(val)\n",
    "\n",
    "    print('finished val')\n",
    "\n",
    "    chunks=list()\n",
    "    match_chunks = list()\n",
    "    #catch case where we run out of chunks to combine\n",
    "    labels = list()\n",
    "    for j in range(num_chunks):\n",
    "        try:\n",
    "            chunk = pd.read_pickle(f'{outputs_path}/intermediateOutputs/datasets/test/{window}_ppm/chunk_{j+1}.pkl')\n",
    "            matches_chunk = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/test/{window}_ppm/chunk_{j+1}.pkl')\n",
    "            labels = labels + chunk[label_field].tolist()\n",
    "            chunk = chunk.iloc[:,sim_indices]\n",
    "            match_chunk = matches_chunk[['queryID','target_base']]\n",
    "            chunks.append(chunk)\n",
    "            match_chunks.append(match_chunk)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    test = pd.concat(chunks)\n",
    "    test_match = pd.concat(match_chunks)\n",
    "    test['queryID'] = test_match['queryID']\n",
    "    test['target_base'] = test_match['target_base']\n",
    "    test['match'] = labels\n",
    "    test.reset_index(inplace=True)\n",
    "    del(chunk)\n",
    "    del(chunks)\n",
    "    del(match_chunk)\n",
    "    del(match_chunks)\n",
    "    del(labels)\n",
    "    \n",
    "\n",
    "    test_aucs = testUtils.evaluate_models_by_subset(trained_models, indices, test, logpath = logpath)\n",
    "    del(test)\n",
    "    del(trained_models)\n",
    "\n",
    "    print('finished test')\n",
    "\n",
    "    model_aucs = pd.DataFrame([names, train_aucs, test_aucs]).transpose()\n",
    "    model_aucs.columns=['name','train','test']\n",
    "\n",
    "    model_aucs.to_csv(f'{outputs_path}/gbc_res/model_aucs_{window}_ppm.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recreate training data\n",
    "sim_indices = datasetBuilder.generate_keep_indices(noise_threshes=[True for i in range(len(noise_threshes))],\n",
    "                                                centroid_tolerance_vals = [True for i in range(len(centroid_tolerance_vals))],\n",
    "                                                reweight_methods = [True for i in range(len(reweight_methods))],\n",
    "                                                sim_methods = [True for i in range(len(sim_methods))],\n",
    "                                                prec_removes = [True for i in range(len(prec_removes))],\n",
    "                                                spec_features=[False for i in range(6)])\n",
    "\n",
    "for window in ppm_windows:\n",
    "\n",
    "    model_aucs = pd.read_csv(f'{outputs_path}/gbc_res/model_aucs_{window}_ppm.csv')\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/gbcIndices/custom_indices_{window}_ppm.pkl', 'rb') as handle:\n",
    "\n",
    "       indices = pickle.load(handle)\n",
    "\n",
    "    chunks=list()\n",
    "    #catch case where we run out of chunks to combine\n",
    "    labels = list()\n",
    "    match_chunks = list()\n",
    "    for j in range(num_chunks):\n",
    "        try:\n",
    "            chunk = pd.read_pickle(f'{outputs_path}/intermediateOutputs/datasets/train/{window}_ppm/chunk_{j+1}.pkl')\n",
    "            labels = labels + chunk[label_field].tolist()\n",
    "            chunk = chunk.iloc[:,sim_indices]\n",
    "            chunks.append(chunk)\n",
    "\n",
    "            matches_chunk = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train/{window}_ppm/chunk_{j+1}.pkl')\n",
    "            match_chunk = matches_chunk[['queryID','target_base']]\n",
    "            match_chunks.append(match_chunk)\n",
    "\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    print('created train')\n",
    "    train = pd.concat(chunks)\n",
    "    train_match = pd.concat(match_chunks)\n",
    "    train['queryID'] = train_match['queryID']\n",
    "    train['target_base'] = train_match['target_base']\n",
    "    train['match'] = labels\n",
    "    train.reset_index(inplace=True)\n",
    "    del(chunk)\n",
    "    del(chunks)\n",
    "    del(match_chunk)\n",
    "    del(match_chunks)\n",
    "    del(labels)\n",
    "\n",
    "    #create df for correlation and aucs on training data\n",
    "    individual_aucs = np.zeros(len(train.columns)-3)\n",
    "    individual_names = train.columns\n",
    "\n",
    "    for i in range(train.shape[1]-3):\n",
    "\n",
    "        temp = train.loc[train.groupby(by=['queryID','target_base'])[individual_names[i]].idxmax()]\n",
    "        individual_aucs[i] = auc(temp['match'],temp[individual_names[i]])\n",
    "\n",
    "    train = pd.read_csv(f'{outputs_path}/intermediateOutputs/datasets/train/{window}_ppm/corrs.csv').iloc[:,1:]\n",
    "\n",
    "    max_aucs, mean_aucs = testUtils.auc_data(model_aucs['name'].tolist(), indices, individual_aucs)\n",
    "    min_corrs, mean_corrs = testUtils.corr_data(model_aucs['name'].tolist(), indices, train)\n",
    "\n",
    "    num_features=list()\n",
    "    for name in model_aucs['name'].tolist():\n",
    "\n",
    "        num_features.append(len(indices[name.split('__')[0]]))\n",
    "\n",
    "    feature_attributes = pd.DataFrame([model_aucs['name'].tolist(), max_aucs, mean_aucs, min_corrs, mean_corrs, num_features]).transpose()\n",
    "    feature_attributes.columns = ['name', 'max_auc', 'mean_auc', 'min_corr', 'mean_corr', 'num_features']\n",
    "\n",
    "    feature_attributes.to_csv(f'{outputs_path}/gbc_res/model_attributes_{window}_ppm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Linear Models of Performance by Feature Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in ppm_windows:\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/gbcIndices/custom_indices_{window}_ppm.pkl', 'rb') as handle:\n",
    "\n",
    "       indices = pickle.load(handle)\n",
    "\n",
    "    chunks = list()\n",
    "    match_chunks = list()\n",
    "    #catch case where we run out of chunks to combine\n",
    "    labels = list()\n",
    "    for j in range(num_chunks):\n",
    "        try:\n",
    "            chunk = pd.read_pickle(f'{outputs_path}/intermediateOutputs/datasets/test/{window}_ppm/chunk_{j+1}.pkl')\n",
    "            labels = labels + chunk[label_field].tolist()\n",
    "            chunk = chunk.iloc[:,sim_indices]\n",
    "            chunks.append(chunk)\n",
    "\n",
    "            matches_chunk = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train/{window}_ppm/chunk_{j+1}.pkl')\n",
    "            match_chunk = matches_chunk[['queryID','target_base']]\n",
    "            match_chunks.append(match_chunk)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    train = pd.concat(chunks)\n",
    "    del(chunk)\n",
    "    del(chunks)\n",
    "    train_match = pd.concat(match_chunks)\n",
    "    train['queryID'] = train_match['queryID']\n",
    "    train['target_base'] = train_match['target_base']\n",
    "    train['match'] = labels\n",
    "    del(chunk)\n",
    "    del(chunks)\n",
    "    del(match_chunk)\n",
    "    del(match_chunks)\n",
    "    del(labels)\n",
    "\n",
    "    #create df for correlation and aucs on training data\n",
    "    individual_aucs_test = np.zeros(len(train.columns))\n",
    "\n",
    "    for i in range(train.shape[1]-1):\n",
    "        temp = train.loc[train.groupby(by=['queryID','target_base'])[individual_names[i]].idxmax()]\n",
    "        individual_aucs_test[i] = auc(labels, train.iloc[:,i].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 3\n",
    "model_aucs = pd.read_csv(f'{outputs_path}/gbc_res/model_aucs_{window}_ppm.csv')\n",
    "feature_attributes = pd.read_csv(f'{outputs_path}/gbc_res/model_attributes_{window}_ppm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all Models, Generate Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No regularization\n",
    "reload(figures)\n",
    "all_features = np.array(list(range(5)))\n",
    "figures.performance_attribution(model_aucs, feature_attributes, [['_']], ['all'], model = lr(), feature_indices=all_features, figure=True)\n",
    "\n",
    "#regularization\n",
    "figures.performance_attribution(model_aucs, feature_attributes, [['_']], ['all'], model = Ridge(alpha=5), feature_indices=all_features, figure=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance as we vary the number of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures.performance_attribution(model_aucs.iloc[1:], feature_attributes.iloc[1:], [['all']], ['all'], feature_indices = [4], model = lr(), figure = True, title = 'Performance by Feature Number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 'All' Indices specifically, does Reducing Correlation Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(figures)\n",
    "figures.performance_attribution(model_aucs.iloc[1:], feature_attributes.iloc[1:], [['3-all'],['5-all'],['10-all'],['15-all'],['20-all']], [3,5,10,15,20], feature_indices = [2], model = lr(), figure = True, title = 'Performance by Corr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within one Cleaning Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(figures)\n",
    "yool = figures.performance_attribution(model_aucs.iloc[1:], feature_attributes.iloc[1:], [[f'corr-3-{i}' for i in range(40)],[f'corr-5-{i}' for i in range(40)]], ['3 per setting','5 per setting'], feature_indices = [0,2], model = lr(), figure = True, title = 'Performance by Corr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairs and Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(figures)\n",
    "yool = figures.performance_attribution(model_aucs.iloc[1:], feature_attributes.iloc[1:], [['pair'],['triplet']], ['Pairs','Triplets'], feature_indices = [2], model = lr(), figure = True, title = 'Performance by Corr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(0,1,100)\n",
    "b= -0.2 + 2*a**2\n",
    "plt.plot(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Functions to original metrics, evaluate how far off we are on test data with original normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Ideas:\n",
    "\n",
    "Accuracy (In order of increasing difficulty):\n",
    "\n",
    "-Incorporate as feature how many possible chem structures (can also restrict to NPS) exist within a certain precursor distance. (violating golden rules or not)\n",
    "\n",
    "-include original NIST version or theoretical res as feature\n",
    "\n",
    "-Weight different ranges of spec differently for matches (more diversity/greater accuracy)\n",
    "\n",
    "-smush together top n results over different inchicores and come up with combined model predicting over individual inchicores\n",
    "\n",
    "-diagnostic ion/loss classing as a feature...do they match\n",
    "\n",
    "-kernelized smooth match\n",
    "\n",
    "-3d struct guesses...do they match (cores, but can generalize to 3d)\n",
    "\n",
    "Speed(In order of increasing difficulty):\n",
    "\n",
    "-combine sim metrics and expand(apply func to df)\n",
    "\n",
    "-exclude matches based on non-similarity features to cut down on needed comparisons\n",
    "\n",
    "-ion tables to upper bound similarity\n",
    "\n",
    "-only use one peak consolidation and matching protocol...then only do reweight transformations on already matched peaks for spec and sim features\n",
    "\n",
    "-can missing peaks in lower energy be explained by frags and losses from higher energy? incorporate into model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order to proceed:\n",
    "\n",
    "-recreate databases with coll energy included (standardized format across DBs)\n",
    "\n",
    "-what proportion of matches are the same coll energy?\n",
    "\n",
    "-quantify variability in peak appearance vs peak intensity across collision energies\n",
    "    -does this relate in a predictable way to fragment mass\n",
    "\n",
    "-test sim metrics for same coll energy vs not same col energy (is the same inductive bias useful)\n",
    "\n",
    "-Show that regular funcs are in the space of combo distance\n",
    "\n",
    "-test combining individual metrics that use different components of the 2 vectors (add, mult, dif)\n",
    "\n",
    "-range over individual metrics in combined score in attempt to explain why combining them is successful\n",
    "\n",
    "-train combo metrics with flattened components and individual (should these sims be broken out?)\n",
    "    -should we do this for same coll energy vs dif energies\n",
    "\n",
    "-are different combo metrics put into larger model more successful than the combined individual metrics\n",
    "\n",
    "-can tunasims be fit with nonlinearities between the components (flattened or not?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
