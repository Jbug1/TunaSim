{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from importlib import reload\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "import copy\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier as hgbc\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import TunaSims\n",
    "import func_ob\n",
    "import tools\n",
    "import datasetBuilder\n",
    "import testUtils\n",
    "import spectral_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for Different Ways of Distributing Interspectral Intensity Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#databases\n",
    "outputs_path='/Users/jonahpoczobutt/projects/TunaRes/test'\n",
    "nist14='/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist14_highres.pkl'\n",
    "nist20_prot_deprot = '/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist20_prot_deprot.pkl'\n",
    "nist23_hr_prot_deprot_only = '/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist23_prot_deprot_only.pkl'\n",
    "nist23_hr_full ='/Users/jonahpoczobutt/projects/raw_data/db_csvs/nist23_full.pkl'\n",
    "gnps='/Users/jonahpoczobutt/projects/raw_data/db_csvs/gnps_highres.pkl'\n",
    "mona='/Users/jonahpoczobutt/projects/raw_data/db_csvs/mona_highres.pkl'\n",
    "metlin='/Users/jonahpoczobutt/projects/raw_data/db_csvs/metlin_highres_inst.pkl'\n",
    "mona_nist = '/Users/jonahpoczobutt/projects/raw_data/db_csvs/mona_nist_prot_only.pkl'\n",
    "\n",
    "self_search=False\n",
    "query = metlin\n",
    "target = nist23_hr_full\n",
    "if self_search:\n",
    "    target=query\n",
    "    \n",
    "fullRun=True\n",
    "if fullRun:\n",
    "    os.mkdir(outputs_path)\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/splitMatches')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/datasets')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/gbc_res')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/train_to_func')\n",
    "    os.mkdir(f'{outputs_path}/intermediateOutputs/train_to_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullRun=True\n",
    "if fullRun:\n",
    "\n",
    "    #This should be replaced with a function to read in all the databases\n",
    "    query_ = pd.read_pickle(query)\n",
    "    all_bases = list(set(query_['inchi_base']))\n",
    "\n",
    "    if self_search:\n",
    "        query_.insert(0,'queryID', [i for i in range(len(query_))])\n",
    "    else:\n",
    "        query_.insert(0,'queryID', [\"_\" for i in range(len(query_))])\n",
    "\n",
    "    #this method is in place\n",
    "    np.random.shuffle(all_bases)\n",
    "\n",
    "    first_bases = all_bases[:int(len(all_bases)*0.5)]\n",
    "    second_bases = all_bases[int(len(all_bases)*0.5):int(len(all_bases)*0.7)]\n",
    "    third_bases = all_bases[int(len(all_bases)*0.7):]\n",
    "\n",
    "    first_query_ = query_[np.isin(query_['inchi_base'],first_bases)]\n",
    "    first_query_.reset_index(inplace=True)\n",
    "    first_query_.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/first_query.pkl')\n",
    "    del(first_query_)\n",
    "\n",
    "    second_query_ = query_[np.isin(query_['inchi_base'],second_bases)]\n",
    "    second_query_.reset_index(inplace=True)\n",
    "    second_query_.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/second_query.pkl')\n",
    "    del(second_query_)\n",
    "\n",
    "    third_query_ = query_[np.isin(query_['inchi_base'],third_bases)]\n",
    "    third_query_.reset_index(inplace=True)\n",
    "    third_query_.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/third_query.pkl')\n",
    "    del(third_query_)\n",
    "    del(query_)\n",
    "\n",
    "    \n",
    "    np.save(f'{outputs_path}/intermediateOutputs/splitMatches/first_bases.npy',first_bases)\n",
    "    np.save(f'{outputs_path}/intermediateOutputs/splitMatches/second_bases.npy',second_bases)\n",
    "    np.save(f'{outputs_path}/intermediateOutputs/splitMatches/third_bases.npy',third_bases)\n",
    "    del(first_bases)\n",
    "    del(second_bases)\n",
    "    del(third_bases)\n",
    "    del(all_bases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100013 rows created\n"
     ]
    }
   ],
   "source": [
    "#Similarity methods and transformation parameters below. Leave sim methods as None to run all\n",
    "reload(datasetBuilder)\n",
    "reload(tools)\n",
    "\n",
    "comparison_metrics = ['entropy',\n",
    "             'manhattan',\n",
    "             'lorentzian',\n",
    "             'dot_product',\n",
    "             'fidelity',\n",
    "             'matusita',\n",
    "             'chi2',\n",
    "             'laplacian',\n",
    "             'harmonic_mean',\n",
    "             'bhattacharya_1',\n",
    "             'squared_chord',\n",
    "             'cross_ent'\n",
    "    ]\n",
    "\n",
    "ppm_windows = [10]\n",
    "noise_threshes=[0.01,0.0]\n",
    "centroid_tolerance_vals = [0.05]\n",
    "centroid_tolerance_types=['da']\n",
    "powers=['orig',1]\n",
    "sim_methods=comparison_metrics\n",
    "prec_removes=[True]\n",
    "build_dataset=True\n",
    "\n",
    "\n",
    "train_size=3e6\n",
    "test_size=1e6\n",
    "test_size=2e6\n",
    "\n",
    "max_matches=None\n",
    "adduct_match = False\n",
    "\n",
    "target_=pd.read_pickle(target)\n",
    "\n",
    "if self_search:\n",
    "    target_.insert(0,'queryID', [i for i in range(len(target_))])\n",
    "else:\n",
    "    target_.insert(0,'queryID', [\"*\" for i in range(len(target_))])\n",
    "\n",
    "for i in ppm_windows:\n",
    "\n",
    "    if build_dataset:\n",
    "\n",
    "        #read in first bases and shuffle order\n",
    "        query_train = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/first_query.pkl')\n",
    "        query_train=query_train.sample(frac=1)\n",
    "\n",
    "        #create matches for model to train on\n",
    "        matches = datasetBuilder.create_matches_df(query_train,target_,i,max_matches,train_size, adduct_match)\n",
    "        matches.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_matches_{i}_ppm.pkl')\n",
    "        del(query_train)\n",
    "\n",
    "        matches_same_ce = matches[matches['ceratio']==1]\n",
    "        matches_dif_ce = matches[matches['ceratio']!=1]\n",
    "        \n",
    "        sub_train_same_ce = datasetBuilder.create_cleaned_df(\n",
    "                                            matches_same_ce, \n",
    "                                            sim_methods, \n",
    "                                            noise_threshes, \n",
    "                                            centroid_tolerance_vals, \n",
    "                                            centroid_tolerance_types,\n",
    "                                            powers,\n",
    "                                            prec_removes\n",
    "        )\n",
    "\n",
    "        sub_train_dif_ce = datasetBuilder.create_cleaned_df(\n",
    "                                            matches_dif_ce, \n",
    "                                            sim_methods, \n",
    "                                            noise_threshes, \n",
    "                                            centroid_tolerance_vals, \n",
    "                                            centroid_tolerance_types,\n",
    "                                            powers,\n",
    "                                            prec_removes\n",
    "        )\n",
    "\n",
    "\n",
    "        sub_train_same_ce.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_same_ce_{i}_ppm.pkl')\n",
    "        sub_train_dif_ce.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_dif_ce_{i}_ppm.pkl')\n",
    "\n",
    "        del(sub_train_same_ce)\n",
    "        del(sub_train_dif_ce)\n",
    "        #read in first bases and shuffle order\n",
    "        query_val = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/second_query.pkl')\n",
    "        query_query_val = query_val.sample(frac=1)\n",
    "\n",
    "        #create matches for model to train on\n",
    "        matches = datasetBuilder.create_matches_df(query_val,target_,i,max_matches,test_size, adduct_match)\n",
    "        matches.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/val_matches_{i}_ppm.pkl')\n",
    "        del(query_val)\n",
    "\n",
    "        \n",
    "        matches_same_ce = matches[matches['ceratio']==1]\n",
    "        matches_dif_ce = matches[matches['ceratio']!=1]\n",
    "        \n",
    "        sub_val_same_ce = datasetBuilder.create_cleaned_df(\n",
    "                                            matches_same_ce, \n",
    "                                            sim_methods, \n",
    "                                            noise_threshes, \n",
    "                                            centroid_tolerance_vals, \n",
    "                                            centroid_tolerance_types,\n",
    "                                            powers,\n",
    "                                            prec_removes\n",
    "        )\n",
    "\n",
    "        sub_val_dif_ce = datasetBuilder.create_cleaned_df(\n",
    "                                            matches_dif_ce, \n",
    "                                            sim_methods, \n",
    "                                            noise_threshes, \n",
    "                                            centroid_tolerance_vals, \n",
    "                                            centroid_tolerance_types,\n",
    "                                            powers,\n",
    "                                            prec_removes\n",
    "        )\n",
    "\n",
    "        sub_val_same_ce.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/val_cleaned_matches_same_ce_{i}_ppm.pkl')\n",
    "        sub_val_dif_ce.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/val_cleaned_matches_dif_ce_{i}_ppm.pkl')\n",
    "\n",
    "\n",
    "        del(sub_val_same_ce)\n",
    "        del(sub_val_dif_ce)\n",
    "\n",
    "        #read in first bases and shuffle order\n",
    "        query_test = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/third_query.pkl')\n",
    "        query_test=query_test.sample(frac=1)\n",
    "\n",
    "        #create matches for model to train on\n",
    "        matches = datasetBuilder.create_matches_df(query_test,target_,i,max_matches,test_size, adduct_match)\n",
    "        matches.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/test_matches_{i}_ppm.pkl')\n",
    "        del(query_test)\n",
    "\n",
    "        matches_same_ce = matches[matches['ceratio']==1]\n",
    "        matches_dif_ce = matches[matches['ceratio']!=1]\n",
    "        \n",
    "        sub_test_same_ce = datasetBuilder.create_cleaned_df(\n",
    "                                            matches_same_ce, \n",
    "                                            sim_methods, \n",
    "                                            noise_threshes, \n",
    "                                            centroid_tolerance_vals, \n",
    "                                            centroid_tolerance_types,\n",
    "                                            powers,\n",
    "                                            prec_removes\n",
    "        )\n",
    "\n",
    "        sub_test_dif_ce = datasetBuilder.create_cleaned_df(\n",
    "                                            matches_dif_ce, \n",
    "                                            sim_methods, \n",
    "                                            noise_threshes, \n",
    "                                            centroid_tolerance_vals, \n",
    "                                            centroid_tolerance_types,\n",
    "                                            powers,\n",
    "                                            prec_removes\n",
    "        )\n",
    "\n",
    "\n",
    "        sub_test_same_ce.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/test_cleaned_matches_same_ce_{i}_ppm.pkl')\n",
    "        sub_test_dif_ce.to_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/test_cleaned_matches_dif_ce_{i}_ppm.pkl')\n",
    "\n",
    "        del(sub_test_same_ce)\n",
    "        del(sub_test_dif_ce)\n",
    "    # else:\n",
    "    #     sub_train_same_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_same_ce_{i}_ppm.pkl')\n",
    "    #     sub_val_same_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/val_cleaned_matches_same_ce_{i}_ppm.pkl')\n",
    "    #     sub_test_same_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/test_cleaned_matches_same_ce_{i}_ppm.pkl')\n",
    "\n",
    "    #     sub_train_dif_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_dif_ce_{i}_ppm.pkl')\n",
    "    #     sub_val_dif_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/val_cleaned_matches_dif_ce_{i}_ppm.pkl')\n",
    "    #     sub_test_dif_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/test_cleaned_matches_dif_ce_{i}_ppm.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Train/Val/Test Data & get Individual Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in ppm_windows:\n",
    "\n",
    "    sub_train_same_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_same_ce_{i}_ppm.pkl')\n",
    "    sub_train_dif_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_dif_ce_{i}_ppm.pkl')\n",
    "    \n",
    "    train_datasets = [sub_train_same_ce, sub_train_dif_ce]\n",
    "    dataset_names = ['same_ce','dif_ce']\n",
    "\n",
    "    gbc_train_datasets = list()\n",
    "    train_unnorm_dists_=list()\n",
    "    ind_aucs_full = list()\n",
    "\n",
    "    #create init df\n",
    "    for metric in comparison_metrics:\n",
    "        for j in range(int(train_datasets[0].shape[1]/2)):\n",
    "\n",
    "            ind_aucs_full.append(f'{metric}_{j}')\n",
    "\n",
    "    ind_aucs_full = pd.DataFrame(ind_aucs_full, columns=['metric'])\n",
    "\n",
    "    for _ in range(len(train_datasets)):\n",
    "\n",
    "        ind_aucs_=None\n",
    "        train_data_gbcs = None\n",
    "        train_unnorm_dists = None\n",
    "        for j in range(int(train_datasets[_].shape[1]/2)):\n",
    "\n",
    "            sub = train_datasets[_].iloc[:,2*j:2*(j+1)]\n",
    "            old_cols = sub.columns\n",
    "            sub.columns=['query','target']\n",
    "            sub['match'] = train_datasets[_]['match'].tolist()\n",
    "\n",
    "            ind_aucs, inds, inds_unnorm = testUtils.orig_metric_to_df(comparison_metrics, sub, unnnormalized=True)\n",
    "            ind_aucs_ = pd.concat((ind_aucs_, ind_aucs))\n",
    "            sub = sub.iloc[:,:2]\n",
    "            sub.columns=old_cols\n",
    "            train_data_gbcs = pd.concat((train_data_gbcs,inds), axis=1)\n",
    "            train_unnorm_dists = pd.concat((train_unnorm_dists,inds_unnorm), axis=1)\n",
    "\n",
    "        if _ ==0:    \n",
    "            ind_aucs_full['same_train']=ind_aucs_['AUC'].tolist()\n",
    "        else:\n",
    "            ind_aucs_full['dif_train']=ind_aucs_['AUC'].tolist()\n",
    "\n",
    "        train_data_gbcs['match'] = train_datasets[_]['match'].tolist()\n",
    "        gbc_train_datasets.append(train_data_gbcs)\n",
    "        train_unnorm_dists_.append(train_unnorm_dists)\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/datasets/gbc_train_{i}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "        pickle.dump(gbc_train_datasets, handle)\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/datasets/train_unnorm_dist_{i}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "        pickle.dump(train_unnorm_dists, handle)\n",
    "\n",
    "    del(train_unnorm_dists)\n",
    "    del(gbc_train_datasets)\n",
    "    del(sub_train_same_ce)\n",
    "    del(sub_train_dif_ce)\n",
    "\n",
    "    print('created train data')\n",
    "\n",
    "    sub_val_same_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/val_cleaned_matches_same_ce_{i}_ppm.pkl')\n",
    "    sub_val_dif_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/val_cleaned_matches_dif_ce_{i}_ppm.pkl')\n",
    "    val_datasets = [sub_val_same_ce, sub_val_dif_ce]\n",
    "    \n",
    "    gbc_val_datasets = list()\n",
    "    for _ in range(len(val_datasets)):\n",
    "\n",
    "        val_data_gbcs = None\n",
    "        ind_aucs_=None\n",
    "        for j in range(int(val_datasets[_].shape[1]/2)):\n",
    "\n",
    "            sub = val_datasets[_].iloc[:,2*j:2*(j+1)]\n",
    "            old_cols = sub.columns\n",
    "            sub.columns=['query','target']\n",
    "            sub['match'] = val_datasets[_]['match'].tolist()\n",
    "\n",
    "            ind_aucs, inds = testUtils.orig_metric_to_df(comparison_metrics, sub)\n",
    "            ind_aucs_ = pd.concat((ind_aucs_, ind_aucs))\n",
    "            sub = sub.iloc[:,:2]\n",
    "            sub.columns=old_cols\n",
    "            val_data_gbcs = pd.concat((val_data_gbcs,inds), axis=1)\n",
    "        \n",
    "        if _ ==0:    \n",
    "            ind_aucs_full['same_val']=ind_aucs_['AUC'].tolist()\n",
    "        else:\n",
    "            ind_aucs_full['dif_val']=ind_aucs_['AUC'].tolist()\n",
    "\n",
    "        val_data_gbcs['match'] = val_datasets[_]['match'].tolist()\n",
    "        gbc_val_datasets.append(val_data_gbcs)\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/datasets/gbc_val_{i}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "        pickle.dump(gbc_val_datasets, handle)\n",
    "\n",
    "    del(gbc_val_datasets)\n",
    "    del(sub_val_same_ce)\n",
    "    del(sub_val_dif_ce)\n",
    "    print('created val data')\n",
    "\n",
    "    sub_test_same_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/test_cleaned_matches_same_ce_{i}_ppm.pkl')\n",
    "    sub_test_dif_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/test_cleaned_matches_dif_ce_{i}_ppm.pkl')\n",
    "    test_datasets = [sub_test_same_ce, sub_test_dif_ce]\n",
    "\n",
    "    gbc_test_datasets = list()\n",
    "    test_unnorm_dists_ = list()\n",
    "    for _ in range(len(test_datasets)):\n",
    "\n",
    "        test_data_gbcs = None\n",
    "        ind_aucs_ = None\n",
    "        test_unnorm_dists=None\n",
    "        for j in range(int(test_datasets[_].shape[1]/2)):\n",
    "\n",
    "            sub = test_datasets[_].iloc[:,2*j:2*(j+1)]\n",
    "            old_cols = sub.columns\n",
    "            sub.columns=['query','target']\n",
    "            sub['match'] = test_datasets[_]['match'].tolist()\n",
    "\n",
    "            ind_aucs, inds, inds_unnorm = testUtils.orig_metric_to_df(comparison_metrics, sub, unnnormalized=True)\n",
    "            ind_aucs['metric'] = [f'{x}_{j}' for x in ind_aucs['metric']]\n",
    "            ind_aucs_ = pd.concat((ind_aucs_, ind_aucs))\n",
    "            sub = sub.iloc[:,:2]\n",
    "            sub.columns=old_cols\n",
    "            test_data_gbcs = pd.concat((test_data_gbcs,inds), axis=1)\n",
    "            test_unnorm_dists = pd.concat((test_unnorm_dists,inds_unnorm), axis=1)\n",
    "        \n",
    "        if _ ==0:    \n",
    "            ind_aucs_full['same_test']=ind_aucs_['AUC'].tolist()\n",
    "        else:\n",
    "            ind_aucs_full['dif_test']=ind_aucs_['AUC'].tolist()\n",
    "\n",
    "        test_data_gbcs['match'] = test_datasets[_]['match'].tolist()\n",
    "        gbc_test_datasets.append(test_data_gbcs)\n",
    "        test_unnorm_dists_.append(test_unnorm_dists)\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/datasets/gbc_test_{i}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "        pickle.dump(gbc_test_datasets, handle)\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/datasets/test_unnorm_dist_{i}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "        pickle.dump(test_unnorm_dists, handle)\n",
    "\n",
    "    del(test_unnorm_dists)\n",
    "    del(gbc_test_datasets)\n",
    "    del(sub_test_same_ce)\n",
    "    del(sub_test_dif_ce)\n",
    "    print('created test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create indices to pull for each metric and those with same components, specify GBC models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{outputs_path}/intermediateOutputs/datasets/gbc_train_{ppm_windows[0]}_ppm.pkl', 'rb') as handle:\n",
    "\n",
    "    gbc_train_datasets = pickle.load(handle)\n",
    "\n",
    "    models = [\n",
    "                    hgbc(),\n",
    "                    hgbc(learning_rate=0.5),\n",
    "                    hgbc(max_iter=200),\n",
    "                    hgbc(learning_rate=0.01,min_samples_leaf=10),\n",
    "                    hgbc(max_iter=200,min_samples_leaf=10),\n",
    "                    hgbc(learning_rate=0.5, max_iter=200,min_samples_leaf=10),\n",
    "                    ]\n",
    "\n",
    "\n",
    "    indices = dict()\n",
    "    indices['all-sims'] = list(range(gbc_train_datasets[0].shape[1]-1))\n",
    "    indices['all-mults'] = list()\n",
    "    indices['all-ents'] = list()\n",
    "    indices['all-difs'] = list()\n",
    "\n",
    "    for i in range(int((gbc_train_datasets[0].shape[1]-1)/len(comparison_metrics))):\n",
    "\n",
    "        indices[f'setting-{i}-all'] = list(np.array(range(len(comparison_metrics)))+(i*len(comparison_metrics)))\n",
    "        indices[f'mults-{i}'] = list(np.array([3,4,9])+(i*len(comparison_metrics)))\n",
    "        indices[f'ents-{i}'] = list(np.array([0,11])+(i*len(comparison_metrics)))\n",
    "        indices[f'difs-{i}'] = list(np.array([1,2,5,7,10])+(i*len(comparison_metrics)))\n",
    "\n",
    "        indices[f'all-mults'] = indices['all-mults'] +list(np.array([3,4,9])+(i*len(comparison_metrics)))\n",
    "        indices[f'all-ents'] =  indices['all-ents'] + list(np.array([0,11])+(i*len(comparison_metrics)))\n",
    "        indices[f'all-difs'] =  indices['all-difs'] + list(np.array([1,2,5,7,10])+(i*len(comparison_metrics)))\n",
    "\n",
    "\n",
    "    print(f' total number of models for each: {len(models) * len(indices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Models and Collect Train Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in ppm_windows:\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/datasets/gbc_train_{window}_ppm.pkl', 'rb') as handle:\n",
    "\n",
    "        gbc_train_datasets = pickle.load(handle)\n",
    "     \n",
    "    same_train_model_aucs = list()\n",
    "    model_names = list()\n",
    "    dif_train_model_aucs = list()\n",
    "    trained_models = dict()\n",
    "\n",
    "    for key, value in indices.items():\n",
    "\n",
    "        sub = gbc_train_datasets[0].iloc[:,value]\n",
    "        models_ = copy.deepcopy(models)\n",
    "\n",
    "        for i in range(len(models_)):\n",
    "\n",
    "            models_[i].fit(sub,gbc_train_datasets[0]['match'])\n",
    "            pos_ind = np.where(models_[i].classes_==1)[0][0]\n",
    "            same_train_model_aucs.append(auc(gbc_train_datasets[0]['match'],models_[i].predict_proba(sub)[:,pos_ind]))\n",
    "            model_names.append(f'{key}_{i}')\n",
    "            trained_models[f'same_{key}_{i}'] = models_[i]\n",
    "\n",
    "        sub = gbc_train_datasets[1].iloc[:,value]\n",
    "        models_ = copy.deepcopy(models)\n",
    "\n",
    "        for i in range(len(models_)):\n",
    "\n",
    "            models_[i].fit(sub,gbc_train_datasets[1]['match'])\n",
    "            pos_ind = np.where(models_[i].classes_==1)[0][0]\n",
    "            dif_train_model_aucs.append(auc(gbc_train_datasets[1]['match'],models_[i].predict_proba(sub)[:,pos_ind]))\n",
    "            trained_models[f'dif_{key}_{i}'] = models_[i]\n",
    "\n",
    "    model_aucs = pd.DataFrame([model_names, same_train_model_aucs, dif_train_model_aucs]).transpose()\n",
    "    model_aucs.columns = ['name','same_train','dif_train']\n",
    "\n",
    "    del(gbc_train_datasets)\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/datasets/gbc_val_{window}_ppm.pkl', 'rb') as handle:\n",
    "\n",
    "        gbc_val_datasets = pickle.load(handle)\n",
    "\n",
    "    same_val = list()\n",
    "    dif_val = list()\n",
    "    same_test = list()\n",
    "    dif_test = list()\n",
    "\n",
    "    for name in model_aucs['name'].tolist():\n",
    "\n",
    "        subset_name = name.split('_')[0]\n",
    "\n",
    "        sub = gbc_val_datasets[0].iloc[:,indices[subset_name]]\n",
    "        model = trained_models[f'same_{name}']\n",
    "        pos_ind = np.where(model.classes_==1)[0][0]\n",
    "        same_val.append(auc(gbc_val_datasets[0]['match'],model.predict_proba(sub)[:,pos_ind]))\n",
    "\n",
    "        sub = gbc_val_datasets[1].iloc[:,indices[subset_name]]\n",
    "        model = trained_models[f'dif_{name}']\n",
    "        pos_ind = np.where(model.classes_==1)[0][0]\n",
    "        dif_val.append(auc(gbc_val_datasets[1]['match'],model.predict_proba(sub)[:,pos_ind]))\n",
    "\n",
    "    del(gbc_val_datasets)\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/datasets/gbc_test_{window}_ppm.pkl', 'rb') as handle:\n",
    "\n",
    "        gbc_test_datasets = pickle.load(handle)\n",
    "\n",
    "    for name in model_aucs['name'].tolist():\n",
    "\n",
    "        subset_name = name.split('_')[0]\n",
    "\n",
    "        sub = gbc_test_datasets[0].iloc[:,indices[subset_name]]\n",
    "        model = trained_models[f'same_{name}']\n",
    "        pos_ind = np.where(model.classes_==1)[0][0]\n",
    "        same_test.append(auc(gbc_test_datasets[0]['match'],model.predict_proba(sub)[:,pos_ind]))\n",
    "\n",
    "        sub = gbc_test_datasets[1].iloc[:,indices[subset_name]]\n",
    "        model = trained_models[f'dif_{name}']\n",
    "        pos_ind = np.where(model.classes_==1)[0][0]\n",
    "        dif_test.append(auc(gbc_test_datasets[1]['match'],model.predict_proba(sub)[:,pos_ind]))\n",
    "\n",
    "    del(gbc_test_datasets)\n",
    "\n",
    "    model_aucs['same_val'] = same_val\n",
    "    model_aucs['dif_val'] = dif_val\n",
    "    model_aucs['same_test'] = same_test\n",
    "    model_aucs['dif_test'] = dif_test\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/gbc_res/model_aucs_{window}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "        pickle.dump(model_aucs,handle)\n",
    "        del(model_aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Functions to original metrics, evaluate how far off we are on test data with original normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inits = {'a' : 1,\n",
    "        'b': 1,\n",
    "        'c' : 1,\n",
    "        'd' : -1,\n",
    "        'e' : 1,\n",
    "        'f' : 1,\n",
    "        'g' :1,\n",
    "        'h' :0,\n",
    "        'i' : -1,\n",
    "        'j' :1,\n",
    "        'k' : 1,\n",
    "        'l' : 1,\n",
    "        'm' : 1,\n",
    "        'n' : -1,\n",
    "        'o' : 1,\n",
    "        'p' : 1,\n",
    "        'q' : 1,\n",
    "        'r' : 1,\n",
    "        's' : 0,\n",
    "        't' : -1,\n",
    "        'u' : 1,\n",
    "        'v' : -1,\n",
    "        'w' : 1,\n",
    "        'x' : 1,\n",
    "        'y' : 1,\n",
    "        'z' : 1,\n",
    "        'a_' : 1,\n",
    "        'b_' : -1,\n",
    "        'c_' : 1,\n",
    "        'd_' : -1,\n",
    "        'e_' : 1,\n",
    "        'f_' : 1,\n",
    "        'g_' : 1,\n",
    "        'h_' : 1,\n",
    "        'i_' : 1,\n",
    "        'j_' : -1,\n",
    "        'k_' : 1,\n",
    "        'l_': -1,\n",
    "        'm_' : 1,\n",
    "        'n_' : 1,\n",
    "        'o_' : 1,\n",
    "        'p_' : 1,\n",
    "        'q_' : -1,\n",
    "        'r_' : 1,\n",
    "        's_' : -1,\n",
    "        't_' : 1,\n",
    "        'u_' : 1,\n",
    "        'v_' : 1,\n",
    "        'w_' : -1,\n",
    "        'x_' : 1,\n",
    "        'y_' : -1,\n",
    "        'z_':1}\n",
    "\n",
    "\n",
    "fit_funcs = {\n",
    "    'entropy-1':(['f','g','i','n_','p_'],None),\n",
    "    'entropy-2':(['f','g','h','i','j','n_','o_','p_','q_','r_','s_'],None),\n",
    "    'entropy-3':(['f','g','h','i','j','n_','o_','p_','q_','r_','s_','b','l','x','y','z','a_','b_','c_','d_','e_'],None),\n",
    "    'lorentzian-1':(['a','b'],None),\n",
    "    'lorentzian-2':(['a','b','c','d','e'],None),\n",
    "    'lorentzian-3':(['a','b','c','d','e','f','g','h','i','j','k','n_','o_','p_','q_','r_','s_'],None),\n",
    "    'dot_product-1':(['k','l','t_','u_'],None),\n",
    "    'dot_product-2':(['k','l','m','n','o','t_','u_','v_','w_','x_','y_'],None),\n",
    "    'dot_product-3':(['k','l','m','n','o','t_','u_','v_','w_','x_','y_','k','n_','o_','p_','q_','r_','s_'],None),\n",
    "    'harmonic_mean-1':(['x','y','z','a_','b_','c_'],None),\n",
    "    'harmonic_mean-2':(['b','l','x','y','z','a_','b_','c_','d_','e_','b','l'],None),\n",
    "    'harmonic_mean-3':(['b','l','x','y','z','a_','b_','c_','d_','e_','b','l','n_','o_','p_','q_','r_','s_'],None),\n",
    "    'fidelity-1':(['k','l','m'],None),\n",
    "    'fidelity-2':(['k','l','m','n','o',],None),\n",
    "    'fidelity-2':(['k','l','m','n','o','n_','o_','p_','q_','r_','s_'],None),\n",
    "    'squared_chord-1':(['a','b'],None),\n",
    "    'squared_chord-2':(['a','b','c','d','e'],None),\n",
    "    'squared_chord-3':(['a','b','c','d','e','f','g','h','i','j','k','n_','o_','p_','q_','r_','s_'],None),\n",
    "    'bhattacharya_1-1':(['a','b'],None),\n",
    "    'bhattacharya_1-2':(['a','b','c','d','e'],None),\n",
    "    'bhattacharya_1-3':(['a','b','c','d','e','f','g','h','i','j','k','n_','o_','p_','q_','r_','s_'],None),\n",
    "}\n",
    "\n",
    "reload(testUtils)\n",
    "reload(func_ob)\n",
    "reload(TunaSims)\n",
    "\n",
    "train_reses = list()\n",
    "test_reses = list()\n",
    "for i in ppm_windows:\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/datasets/train_unnorm_dist_{i}_ppm.pkl', 'rb') as handle:\n",
    "\n",
    "        train_labels = pickle.load(handle)\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/datasets/test_unnorm_dist_{i}_ppm.pkl', 'rb') as handle:\n",
    "\n",
    "        test_labels = pickle.load(handle)\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_dif_ce_{i}_ppm.pkl', 'rb') as handle:\n",
    "\n",
    "        train_specs = pickle.load(handle)\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/splitMatches/test_cleaned_matches_dif_ce_{i}_ppm.pkl', 'rb') as handle:\n",
    "\n",
    "        test_specs = pickle.load(handle)\n",
    "\n",
    "    #just focus on first setting for now\n",
    "    train_labels = train_labels.iloc[:,:len(comparison_metrics)]\n",
    "    train_specs = train_specs.iloc[:,:2]\n",
    "    train_specs.columns=['query','target']\n",
    "\n",
    "    test_labels = test_labels.iloc[:,:len(comparison_metrics)]\n",
    "    test_specs = test_specs.iloc[:,:2]\n",
    "    test_specs.columns=['query','target']\n",
    "\n",
    "    squared_loss = lambda x: (x)**2\n",
    "    lin_loss = lambda x: np.abs(x)\n",
    "    l1_reg = lambda l,x: l*np.sum(np.abs(x))\n",
    "    l2_reg = lambda l,x: l*np.sqrt(np.sum(x**2))\n",
    "    no_reg = lambda x: 0\n",
    "\n",
    "    reg_funcs = [no_reg,partial(l2_reg,0.01),partial(l2_reg,0.1)]\n",
    "    reg_names = ['none','l2_0.01','l2_0.1']\n",
    "    losses = [squared_loss]\n",
    "    loss_names = ['squared']\n",
    "    momentums = ['none']\n",
    "    mom_weights = [[0.2,0.8]]\n",
    "    lambdas = [0.01]\n",
    "    max_iters = [1e4]\n",
    "\n",
    "    funcs = testUtils.create_all_funcs_stoch(reg_funcs=reg_funcs,\n",
    "                                        reg_names=reg_names,\n",
    "                                        losses=losses,\n",
    "                                        loss_names=loss_names,\n",
    "                                        momentums=momentums,\n",
    "                                        inits = inits,\n",
    "                                        params=fit_funcs,\n",
    "                                        mom_weights=mom_weights,\n",
    "                                        lambdas=lambdas,\n",
    "                                        max_iters=max_iters,\n",
    "                                        func = TunaSims.tuna_combo_distance_demo)\n",
    "    \n",
    "    print(f'total number of functions : {len(funcs)}')\n",
    "    trained=list()\n",
    "    for func in funcs:\n",
    "\n",
    "        name = func.name.split('-')[0]\n",
    "        train_specs['match'] = train_labels[name]\n",
    "\n",
    "        func.fit(train_specs)\n",
    "        trained.append(func)\n",
    "        print(func.name)\n",
    "\n",
    "    #get train and test errors under proper normalization protocol\n",
    "    trained_res=list()\n",
    "    test_res=list()\n",
    "    names=list()\n",
    "    for func in trained:\n",
    "\n",
    "        #generate proper train and test datasets\n",
    "        name = func.name.split('-')[0]\n",
    "        train_specs['match'] = train_labels[name]\n",
    "        test_specs['match'] = test_labels[name]\n",
    "\n",
    "        #get trained_func\n",
    "        pred_func = func.trained_func()\n",
    "\n",
    "        trained_res.append(testUtils.get_func_dist(train_specs, pred_func, name))\n",
    "        test_res.append(testUtils.get_func_dist(test_specs, pred_func, name))\n",
    "        names.append(func.name)\n",
    "        \n",
    "\n",
    "    trained_res = pd.DataFrame(trained_res).transpose()\n",
    "    trained_res.columns  = names\n",
    "\n",
    "    test_res = pd.DataFrame(test_res).transpose()\n",
    "    test_res.columns  = names\n",
    "\n",
    "    train_reses.append(trained_res)\n",
    "    test_reses.append(test_res)\n",
    "\n",
    "with open(f'{outputs_path}/intermediateOutputs/train_to_func/trained_reses_{i}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "    pickle.dump(train_reses, handle)\n",
    "\n",
    "with open(f'{outputs_path}/intermediateOutputs/train_to_func/test_reses_{i}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "    pickle.dump(test_reses, handle)\n",
    "\n",
    "del(train_reses)\n",
    "del(test_reses)\n",
    "del(train_labels)\n",
    "del(test_labels)\n",
    "del(train_specs)\n",
    "del(test_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Distance Functions by Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flats = {\n",
    "          'fdif_quadk':(['a','b','c','d','e'],None),\n",
    "          'fadd_quadk':(['f','g','h','i','j'],None),\n",
    "          'fmult_quadk':(['k','l','m','n','o'],None),\n",
    "}\n",
    "\n",
    "exts = {'edif_add':(['b','g','p','q','r','s','t','u','v','w'],None),\n",
    "        'edif_mult':(['b','l','x','y','z','a_','b_','c_','d_','e_'],None),\n",
    "        'emult_add':(['l','g','f_','g_','h_','i_','j_','k_','l_','m_'],None),      \n",
    "}\n",
    "\n",
    "params = dict()\n",
    "seen =set()\n",
    "for key in flats.keys():\n",
    "    for key_ in flats.keys():\n",
    "\n",
    "        feature_type = key.split('_')[0]\n",
    "        feature_type_ = key_.split('_')[0]\n",
    "\n",
    "        func_type = key.split('_')[1]\n",
    "        func_type_ = key_.split('_')[1]\n",
    "\n",
    "        try:\n",
    "            bounds_type = key.split('_')[2]\n",
    "            bounds_type_ = key_.split('_')[2]\n",
    "        except:\n",
    "            bounds_type = ''\n",
    "            bounds_type_ = ''\n",
    "\n",
    "        if f'{key_}_{key}' in params.keys():\n",
    "            continue\n",
    "        params[f'{key}_{key_}']=(sorted(list(set(flats[key][0]+flats[key_][0]))),testUtils.dict_combine(flats[key][1],flats[key_][1]))\n",
    "        \n",
    "params_ = dict()\n",
    "seen =set()\n",
    "for key in exts.keys():\n",
    "    for key_ in exts.keys():\n",
    "\n",
    "        feature_type = key.split('_')[0]\n",
    "        feature_type_ = key_.split('_')[0]\n",
    "\n",
    "        func_type = key.split('_')[1]\n",
    "        func_type_ = key_.split('_')[1]\n",
    "\n",
    "        try:\n",
    "            bounds_type = key.split('_')[2]\n",
    "            bounds_type_ = key_.split('_')[2]\n",
    "        except:\n",
    "            bounds_type = ''\n",
    "            bounds_type_ = ''\n",
    "\n",
    "        if f'{key_}_{key}' in params_.keys():\n",
    "            continue\n",
    "        params_[f'{key}_{key_}']=(sorted(list(set(exts[key][0]+exts[key_][0]))),testUtils.dict_combine(exts[key][1],exts[key_][1]))\n",
    "\n",
    "params.update(params_)   \n",
    "\n",
    "params['all_flat_quadk']= (['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o'],None)\n",
    "params['all_ext_quadk'] = (['b','l','g','p','q','r','s','t','u','v','w','x','y','z','a_','b_','c_','d_','e_','f_','g_','h_','i_','j_','k_','l_','m_','t_','u_','v_','w_','x_','y_'],None)\n",
    "\n",
    "for key in list(params_.keys())[:5]:\n",
    "    params[f'{key}_normed_add']=(sorted(list(set(params_[key][0]+['n_','o_','p_','q_','r_','s_']))),testUtils.dict_combine(params_[key][1],None))\n",
    "    params[f'{key}_normed_mult']=(sorted(list(set(params_[key][0]+['t_','u_','v_','w_','x_','y_']))),testUtils.dict_combine(params_[key][1],None))\n",
    "\n",
    "params['norm_only_add']=(['n_','o_','p_','q_','r_','s_'],None)\n",
    "params['norm_only_mult']=(['t_','u_','v_','w_','x_','y_'],None)\n",
    "\n",
    "\n",
    "flats = {\n",
    "          'fdif_quad':(['a','b','c'],None),\n",
    "          'fadd_quad':(['f','g','h'],None),\n",
    "          'fmult_quad':(['k','l','m'],None),\n",
    "}\n",
    "\n",
    "exts = {'edif_add_quad':(['b','g','p','q','r','s','t','u'],None),\n",
    "        'edif_mult_quad':(['b','l','x','y','z','a_','b_','c_'],None),\n",
    "        'emult_add_quad':(['l','g','f_','g_','h_','i_','j_','k_'],None),      \n",
    "}\n",
    "\n",
    "params2 = dict()\n",
    "seen =set()\n",
    "for key in flats.keys():\n",
    "    for key_ in flats.keys():\n",
    "\n",
    "        feature_type = key.split('_')[0]\n",
    "        feature_type_ = key_.split('_')[0]\n",
    "\n",
    "        func_type = key.split('_')[1]\n",
    "        func_type_ = key_.split('_')[1]\n",
    "\n",
    "        try:\n",
    "            bounds_type = key.split('_')[2]\n",
    "            bounds_type_ = key_.split('_')[2]\n",
    "        except:\n",
    "            bounds_type = ''\n",
    "            bounds_type_ = ''\n",
    "\n",
    "        if f'{key_}_{key}' in params2.keys():\n",
    "            continue\n",
    "        params2[f'{key}_{key_}']=(sorted(list(set(flats[key][0]+flats[key_][0]))),testUtils.dict_combine(flats[key][1],flats[key_][1]))\n",
    "        \n",
    "params2_ = dict()\n",
    "seen =set()\n",
    "for key in exts.keys():\n",
    "    for key_ in exts.keys():\n",
    "\n",
    "        feature_type = key.split('_')[0]\n",
    "        feature_type_ = key_.split('_')[0]\n",
    "\n",
    "        func_type = key.split('_')[1]\n",
    "        func_type_ = key_.split('_')[1]\n",
    "\n",
    "        try:\n",
    "            bounds_type = key.split('_')[2]\n",
    "            bounds_type_ = key_.split('_')[2]\n",
    "        except:\n",
    "            bounds_type = ''\n",
    "            bounds_type_ = ''\n",
    "\n",
    "        if f'{key_}_{key}' in params2_.keys():\n",
    "            continue\n",
    "        params2_[f'{key}_{key_}']=(sorted(list(set(exts[key][0]+exts[key_][0]))),testUtils.dict_combine(exts[key][1],exts[key_][1]))\n",
    "\n",
    "params2['all_flat_quad']= (['a','b','c','f','g','h','k','l','m'],None)\n",
    "params2['all_ext_quad'] = (['b','l','g','p','q','r','s','t','u','x','y','z','a_','b_','c_','f_','g_','h_','i_','j_','k_'],None)\n",
    "\n",
    "\n",
    "for key in list(params2_.keys())[:5]:\n",
    "    params2[f'{key}_normed_add']=(sorted(list(set(params2_[key][0]+['n_','o_','p_','s_']))),testUtils.dict_combine(params2_[key][1],None))\n",
    "    params2[f'{key}_normed_mult']=(sorted(list(set(params2_[key][0]+['t_','u_','v_','w_','x_','y_']))),testUtils.dict_combine(params2_[key][1],None))\n",
    "\n",
    "params2.update(params2_) \n",
    "#params.update(params2)  \n",
    "\n",
    "for key in list(params.keys())[:10]:\n",
    "    params[f'{key}_sigtune']=(params[key][0]+['z_'],None)\n",
    "\n",
    "reload(func_ob)\n",
    "reload(TunaSims)\n",
    "reload(testUtils)\n",
    "#helper lambda funcs\n",
    "squared_loss = lambda x: (1-x)**2\n",
    "lin_loss = lambda x: np.abs(1-x)\n",
    "l1_reg = lambda l,x: l*np.sum(np.abs(x))\n",
    "l2_reg = lambda l,x: l*np.sqrt(np.sum(x**2))\n",
    "no_reg = lambda x: 0\n",
    "\n",
    "reg_funcs = [no_reg,partial(l2_reg,0.01),partial(l2_reg,0.1)]\n",
    "reg_names = ['none_none','l2_0.01','l2_0.1']\n",
    "losses = [squared_loss]\n",
    "loss_names = ['squared']\n",
    "momentums = ['none']\n",
    "mom_weights = [[0.2,0.8]]\n",
    "lambdas = [0.01]\n",
    "max_iters = [1e4]\n",
    "\n",
    "funcs_same = testUtils.create_all_funcs_stoch(reg_funcs=reg_funcs,\n",
    "                                       reg_names=reg_names,\n",
    "                                       losses=losses,\n",
    "                                       loss_names=loss_names,\n",
    "                                       momentums=momentums,\n",
    "                                       params=params,\n",
    "                                       inits=inits,\n",
    "                                       mom_weights=mom_weights,\n",
    "                                       lambdas=lambdas,\n",
    "                                       max_iters=max_iters,\n",
    "                                       func = TunaSims.tuna_combo_distance)\n",
    "\n",
    "funcs_dif = testUtils.create_all_funcs_stoch(reg_funcs=reg_funcs,\n",
    "                                       reg_names=reg_names,\n",
    "                                       losses=losses,\n",
    "                                       loss_names=loss_names,\n",
    "                                       momentums=momentums,\n",
    "                                       params=params,\n",
    "                                       inits=inits,\n",
    "                                       mom_weights=mom_weights,\n",
    "                                       lambdas=lambdas,\n",
    "                                       max_iters=max_iters,\n",
    "                                       func = TunaSims.tuna_combo_distance)\n",
    "\n",
    "all_funcs_ = [funcs_same, funcs_dif]\n",
    "\n",
    "print(f'number of specifications: {len(funcs_same)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(TunaSims)\n",
    "for window in ppm_windows:\n",
    "\n",
    "    trained_dict = dict()\n",
    "    all_funcs = copy.deepcopy(all_funcs_)\n",
    "\n",
    "    sub_train_same_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_same_ce_{window}_ppm.pkl')\n",
    "    sub_train_dif_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_dif_ce_{window}_ppm.pkl')\n",
    "    \n",
    "    train_datasets = [sub_train_same_ce, sub_train_dif_ce]\n",
    "    dataset_names = ['same_ce','dif_ce']\n",
    "\n",
    "    for _ in range(len(train_datasets)):\n",
    "        \n",
    "        for j in range(int(train_datasets[_].shape[1]/2)):\n",
    "\n",
    "            sub = train_datasets[_].iloc[:,2*j:2*(j+1)]\n",
    "            sub.columns=['query','target']\n",
    "            sub['match'] = train_datasets[_]['match']\n",
    "        \n",
    "            trained=list()\n",
    "            for i in range(len(all_funcs[_])):\n",
    "                \n",
    "                all_funcs[_][i].fit(sub)\n",
    "                trained.append(all_funcs[_][i])\n",
    "                if (i+1)%10==0:\n",
    "                    print(f'trained {i+1} functions on {dataset_names[_]}_{j}')\n",
    "\n",
    "\n",
    "            trained_dict[f'{dataset_names[_]}_{j}'] = trained\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/train_to_error/trained_dict_{window}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "        pickle.dump(trained_dict, handle)\n",
    "        del(trained_dict)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datasets = [sub_test_same_ce, sub_test_dif_ce]\n",
    "\n",
    "for window in ppm_windows:\n",
    "    \n",
    "    sub_train_same_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_same_ce_{window}_ppm.pkl')\n",
    "    sub_train_dif_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/train_cleaned_matches_dif_ce_{window}_ppm.pkl')\n",
    "    \n",
    "    train_datasets = [sub_train_same_ce, sub_train_dif_ce]\n",
    "    dataset_names = ['same_ce','dif_ce']\n",
    "\n",
    "    trained_res=None\n",
    "    for _ in range(len(train_datasets)):\n",
    "        for j in range(int(train_datasets[_].shape[1]/2)):\n",
    "\n",
    "            #grab trained models for this portion of dataframe\n",
    "            models = trained_dict[f'{dataset_names[_]}_{j}']\n",
    "            sub = train_datasets[_].iloc[:,2*j:2*(j+1)]\n",
    "            sub.columns=['query','target']\n",
    "            sub['match'] = train_datasets[_]['match'].tolist()\n",
    "\n",
    "            small = testUtils.trained_res_to_df(models,sub)\n",
    "            small.insert(1,'settings', f'{dataset_names[_]}_{j}')\n",
    "            trained_res=pd.concat((trained_res,small))\n",
    "            print(f'completed {dataset_names[_]}_{j}')\n",
    "\n",
    "    print('generated train results')\n",
    "    del(sub_train_dif_ce)\n",
    "    del(sub_train_same_ce)\n",
    "\n",
    "    sub_val_same_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/val_cleaned_matches_same_ce_{window}_ppm.pkl')\n",
    "    sub_val_dif_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/val_cleaned_matches_dif_ce_{window}_ppm.pkl')\n",
    "    val_datasets = [sub_val_same_ce, sub_val_dif_ce]\n",
    "\n",
    "    val_aucs=list()\n",
    "    for _ in range(len(val_datasets)):\n",
    "        for j in range(int(val_datasets[_].shape[1]/2)):\n",
    "\n",
    "            #grab trained models for this portion of dataframe\n",
    "            models = trained_dict[f'{dataset_names[_]}_{j}']\n",
    "            sub = val_datasets[_].iloc[:,2*j:2*(j+1)]\n",
    "            sub.columns=['query','target']\n",
    "            sub['match'] = val_datasets[_]['match'].tolist()\n",
    "            val_aucs = val_aucs + testUtils.trained_res_to_df(models,sub)['auc'].tolist()\n",
    "            print(f'completed {dataset_names[_]}_{j}')\n",
    "\n",
    "    trained_res['val']=val_aucs\n",
    "    print('generated val results')\n",
    "\n",
    "    del(sub_val_dif_ce)\n",
    "    del(sub_val_same_ce)\n",
    "\n",
    "    sub_test_same_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/test_cleaned_matches_same_ce_{window}_ppm.pkl')\n",
    "    sub_test_dif_ce = pd.read_pickle(f'{outputs_path}/intermediateOutputs/splitMatches/test_cleaned_matches_dif_ce_{window}_ppm.pkl')\n",
    "    test_datasets = [sub_test_same_ce, sub_test_dif_ce]\n",
    "    \n",
    "    test_aucs=list()\n",
    "    for _ in range(len(test_datasets)):\n",
    "        for j in range(int(test_datasets[_].shape[1]/2)):\n",
    "\n",
    "            #grab trained models for this portion of dataframe\n",
    "            models = trained_dict[f'{dataset_names[_]}_{j}']\n",
    "            sub = test_datasets[_].iloc[:,2*j:2*(j+1)]\n",
    "            sub.columns=['query','target']\n",
    "            sub['match'] = test_datasets[_]['match'].tolist()\n",
    "            test_aucs = test_aucs + testUtils.trained_res_to_df(models,sub)['auc'].tolist()\n",
    "            print(f'completed {dataset_names[_]}_{j}')\n",
    "\n",
    "    trained_res['test']=test_aucs\n",
    "    print('generated test results')\n",
    "\n",
    "    del(sub_test_dif_ce)\n",
    "    del(sub_test_same_ce)\n",
    "\n",
    "    with open(f'{outputs_path}/intermediateOutputs/train_to_error/trained_res_{window}_ppm.pkl', 'wb') as handle:\n",
    "\n",
    "        pickle.dump(trained_res, handle)\n",
    "        del(trained_res)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions: \n",
    "\n",
    "add offsets for terms\n",
    "\n",
    "num of params not appearing to change train time much\n",
    "\n",
    "consider replacing knockouts with sigmoids\n",
    "\n",
    "consider tuning final sigmoid\n",
    "\n",
    "should features like length,entropy be included in the similarity, or be used outside as extra feature in learned mod.both? neither?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Ideas:\n",
    "\n",
    "Accuracy (In order of increasing difficulty):\n",
    "\n",
    "-Incorporate as feature how many possible chem structures (can also restrict to NPS) exist within a certain precursor distance. (violating golden rules or not)\n",
    "\n",
    "-include original NIST version or theoretical res as feature\n",
    "\n",
    "-Weight different ranges of spec differently for matches (more diversity/greater accuracy)\n",
    "\n",
    "-smush together top n results over different inchicores and come up with combined model predicting over individual inchicores\n",
    "\n",
    "-diagnostic ion/loss classing as a feature...do they match\n",
    "\n",
    "-kernelized smooth match\n",
    "\n",
    "-3d struct guesses...do they match (cores, but can generalize to 3d)\n",
    "\n",
    "Speed(In order of increasing difficulty):\n",
    "\n",
    "-combine sim metrics and expand(apply func to df)\n",
    "\n",
    "-exclude matches based on non-similarity features to cut down on needed comparisons\n",
    "\n",
    "-ion tables to upper bound similarity\n",
    "\n",
    "-only use one peak consolidation and matching protocol...then only do reweight transformations on already matched peaks for spec and sim features\n",
    "\n",
    "-can missing peaks in lower energy be explained by frags and losses from higher energy? incorporate into model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order to proceed:\n",
    "\n",
    "-recreate databases with coll energy included (standardized format across DBs)\n",
    "\n",
    "-what proportion of matches are the same coll energy?\n",
    "\n",
    "-quantify variability in peak appearance vs peak intensity across collision energies\n",
    "    -does this relate in a predictable way to fragment mass\n",
    "\n",
    "-test sim metrics for same coll energy vs not same col energy (is the same inductive bias useful)\n",
    "\n",
    "-Show that regular funcs are in the space of combo distance\n",
    "\n",
    "-test combining individual metrics that use different components of the 2 vectors (add, mult, dif)\n",
    "\n",
    "-range over individual metrics in combined score in attempt to explain why combining them is successful\n",
    "\n",
    "-train combo metrics with flattened components and individual (should these sims be broken out?)\n",
    "    -should we do this for same coll energy vs dif energies\n",
    "\n",
    "-are different combo metrics put into larger model more successful than the combined individual metrics\n",
    "\n",
    "-can tunasims be fit with nonlinearities between the components (flattened or not?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
